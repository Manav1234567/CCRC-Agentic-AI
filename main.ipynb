{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c7a35a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/anaconda3/lib/python3.12/site-packages (1.1.3)\n",
      "Requirement already satisfied: langchain-core in /opt/anaconda3/lib/python3.12/site-packages (1.1.3)\n",
      "Requirement already satisfied: langchain-core in /opt/anaconda3/lib/python3.12/site-packages (1.1.3)\n",
      "Collecting langchain-core\n",
      "  Downloading langchain_core-1.2.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: langchain-openai in /opt/anaconda3/lib/python3.12/site-packages (1.1.2)\n",
      "Collecting langchain-core\n",
      "  Downloading langchain_core-1.2.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: langchain-openai in /opt/anaconda3/lib/python3.12/site-packages (1.1.2)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-1.1.3-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain-community in /opt/anaconda3/lib/python3.12/site-packages (0.4.1)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-1.1.3-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain-community in /opt/anaconda3/lib/python3.12/site-packages (0.4.1)\n",
      "Requirement already satisfied: pydantic in /opt/anaconda3/lib/python3.12/site-packages (2.12.5)\n",
      "Requirement already satisfied: pydantic in /opt/anaconda3/lib/python3.12/site-packages (2.12.5)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (1.0.4)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (0.4.59)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (24.1)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (6.0.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (0.12.0)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-openai) (2.11.0)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.0.34)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic) (0.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (2.1)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (1.0.4)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (0.4.59)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (24.1)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (6.0.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (0.12.0)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-openai) (2.11.0)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.0.34)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic) (0.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (2.1)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.15)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.27.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.66.5)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.15)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.27.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.66.5)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.9.11)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.14.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.9.11)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.14.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.0.0)\n",
      "Downloading langchain_core-1.2.0-py3-none-any.whl (475 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.0.0)\n",
      "Downloading langchain_core-1.2.0-py3-none-any.whl (475 kB)\n",
      "Downloading langchain_openai-1.1.3-py3-none-any.whl (84 kB)\n",
      "Downloading langchain_openai-1.1.3-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: langchain-core, langchain-openai\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 1.1.3\n",
      "    Uninstalling langchain-core-1.1.3:\n",
      "      Successfully uninstalled langchain-core-1.1.3\n",
      "Installing collected packages: langchain-core, langchain-openai\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 1.1.3\n",
      "    Uninstalling langchain-core-1.1.3:\n",
      "      Successfully uninstalled langchain-core-1.1.3\n",
      "  Attempting uninstall: langchain-openai\n",
      "  Attempting uninstall: langchain-openai\n",
      "    Found existing installation: langchain-openai 1.1.2\n",
      "    Uninstalling langchain-openai-1.1.2:\n",
      "      Successfully uninstalled langchain-openai-1.1.2\n",
      "    Found existing installation: langchain-openai 1.1.2\n",
      "    Uninstalling langchain-openai-1.1.2:\n",
      "      Successfully uninstalled langchain-openai-1.1.2\n",
      "Successfully installed langchain-core-1.2.0 langchain-openai-1.1.3\n",
      "Successfully installed langchain-core-1.2.0 langchain-openai-1.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 1. Force upgrade the critical libraries\n",
    "%pip install -U langchain langchain-core langchain-openai langchain-community pydantic\n",
    "\n",
    "# 2. IMPORTANT: You must restart the kernel after running this!\n",
    "# In VS Code/Jupyter: Click \"Restart\" or \"Restart Kernel\" in the top toolbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "20d1243a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/lib/python3.12/site-packages (5.2.0)\n",
      "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.12/site-packages (4.3.3)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (4.4.1)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers gensim datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a9f93c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting filelock (from datasets)\n",
      "Collecting filelock (from datasets)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting numpy>=1.17 (from datasets)\n",
      "  Downloading numpy-2.3.5-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting numpy>=1.17 (from datasets)\n",
      "  Downloading numpy-2.3.5-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Using cached pyarrow-22.0.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.2 kB)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Using cached pyarrow-22.0.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting httpx<1.0.0 (from datasets)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx<1.0.0 (from datasets)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.6.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Using cached multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.6.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Using cached multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
      "Collecting fsspec<=2025.10.0,>=2023.1.0 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting fsspec<=2025.10.0,>=2023.1.0 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Using cached huggingface_hub-1.2.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Using cached huggingface_hub-1.2.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting packaging (from datasets)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting packaging (from datasets)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Downloading pyyaml-6.0.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Downloading pyyaml-6.0.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting transformers<6.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting transformers<6.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.9.1-cp312-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.9.1-cp312-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.8.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.8.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting typing_extensions>=4.5.0 (from sentence-transformers)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing_extensions>=4.5.0 (from sentence-transformers)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting anyio (from httpx<1.0.0->datasets)\n",
      "  Downloading anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting anyio (from httpx<1.0.0->datasets)\n",
      "  Downloading anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting certifi (from httpx<1.0.0->datasets)\n",
      "  Downloading certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting certifi (from httpx<1.0.0->datasets)\n",
      "  Downloading certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<1.0.0->datasets)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<1.0.0->datasets)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Using cached typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting typer-slim (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Using cached typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.32.2->datasets)\n",
      "  Downloading charset_normalizer-3.4.4-cp312-cp312-macosx_10_13_universal2.whl.metadata (37 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.32.2->datasets)\n",
      "  Downloading charset_normalizer-3.4.4-cp312-cp312-macosx_10_13_universal2.whl.metadata (37 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets)\n",
      "  Downloading urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets)\n",
      "  Downloading urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting setuptools (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting setuptools (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch>=1.11.0->sentence-transformers)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas->datasets)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas->datasets)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (75 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->datasets)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (75 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->datasets)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading markupsafe-3.0.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading markupsafe-3.0.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Collecting click>=8.0.0 (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting click>=8.0.0 (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Using cached datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Using cached sentence_transformers-5.2.0-py3-none-any.whl (493 kB)\n",
      "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
      "Downloading numpy-2.3.5-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[?25lUsing cached datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Using cached sentence_transformers-5.2.0-py3-none-any.whl (493 kB)\n",
      "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
      "Downloading numpy-2.3.5-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m\u001b[0m \u001b[32m0.0/5.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[?25hUsing cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached pyarrow-22.0.0-cp312-cp312-macosx_12_0_arm64.whl (34.2 MB)\n",
      "Downloading pyyaml-6.0.3-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading torch-2.9.1-cp312-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "Using cached pyarrow-22.0.0-cp312-cp312-macosx_12_0_arm64.whl (34.2 MB)\n",
      "Downloading pyyaml-6.0.3-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading torch-2.9.1-cp312-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\u001b[0m \u001b[32m0.0/74.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached pandas-2.3.3-cp312-cp312-macosx_11_0_arm64.whl (10.7 MB)\n",
      "Downloading scikit_learn-1.8.0-cp312-cp312-macosx_12_0_arm64.whl (8.1 MB)\n",
      "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/8.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mUsing cached transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached pandas-2.3.3-cp312-cp312-macosx_11_0_arm64.whl (10.7 MB)\n",
      "Downloading scikit_learn-1.8.0-cp312-cp312-macosx_12_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached xxhash-3.6.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading aiohttp-3.13.2-cp312-cp312-macosx_11_0_arm64.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached xxhash-3.6.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading aiohttp-3.13.2-cp312-cp312-macosx_11_0_arm64.whl (491 kB)\n",
      "Downloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp312-cp312-macosx_10_13_universal2.whl (208 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "Downloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp312-cp312-macosx_10_13_universal2.whl (208 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading regex-2025.11.3-cp312-cp312-macosx_11_0_arm64.whl (288 kB)\n",
      "Using cached safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl (447 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading regex-2025.11.3-cp312-cp312-macosx_11_0_arm64.whl (288 kB)\n",
      "Using cached safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl (447 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Downloading urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
      "Downloading anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
      "Downloading anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-macosx_11_0_arm64.whl (50 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading markupsafe-3.0.3-cp312-cp312-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading multidict-6.7.0-cp312-cp312-macosx_11_0_arm64.whl (43 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-macosx_11_0_arm64.whl (50 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading markupsafe-3.0.3-cp312-cp312-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading multidict-6.7.0-cp312-cp312-macosx_11_0_arm64.whl (43 kB)\n",
      "Downloading propcache-0.4.1-cp312-cp312-macosx_11_0_arm64.whl (47 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-macosx_11_0_arm64.whl (94 kB)\n",
      "Downloading propcache-0.4.1-cp312-cp312-macosx_11_0_arm64.whl (47 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-macosx_11_0_arm64.whl (94 kB)\n",
      "Installing collected packages: pytz, mpmath, xxhash, urllib3, tzdata, typing_extensions, tqdm, threadpoolctl, sympy, six, setuptools, safetensors, regex, pyyaml, pyarrow, propcache, packaging, numpy, networkx, multidict, MarkupSafe, joblib, idna, hf-xet, h11, fsspec, frozenlist, filelock, dill, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, scipy, requests, python-dateutil, multiprocess, jinja2, httpcore, anyio, aiosignal, torch, scikit-learn, pandas, huggingface-hub, httpx, aiohttp, tokenizers, transformers, datasets, sentence-transformers\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2024.1\n",
      "Installing collected packages: pytz, mpmath, xxhash, urllib3, tzdata, typing_extensions, tqdm, threadpoolctl, sympy, six, setuptools, safetensors, regex, pyyaml, pyarrow, propcache, packaging, numpy, networkx, multidict, MarkupSafe, joblib, idna, hf-xet, h11, fsspec, frozenlist, filelock, dill, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, scipy, requests, python-dateutil, multiprocess, jinja2, httpcore, anyio, aiosignal, torch, scikit-learn, pandas, huggingface-hub, httpx, aiohttp, tokenizers, transformers, datasets, sentence-transformers\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2024.1\n",
      "    Uninstalling pytz-2024.1:\n",
      "      Successfully uninstalled pytz-2024.1\n",
      "    Uninstalling pytz-2024.1:\n",
      "      Successfully uninstalled pytz-2024.1\n",
      "  Attempting uninstall: mpmath\n",
      "    Found existing installation: mpmath 1.3.0\n",
      "    Uninstalling mpmath-1.3.0:\n",
      "      Successfully uninstalled mpmath-1.3.0\n",
      "  Attempting uninstall: mpmath\n",
      "    Found existing installation: mpmath 1.3.0\n",
      "    Uninstalling mpmath-1.3.0:\n",
      "      Successfully uninstalled mpmath-1.3.0\n",
      "  Attempting uninstall: xxhash\n",
      "    Found existing installation: xxhash 3.6.0\n",
      "    Uninstalling xxhash-3.6.0:\n",
      "      Successfully uninstalled xxhash-3.6.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "  Attempting uninstall: xxhash\n",
      "    Found existing installation: xxhash 3.6.0\n",
      "    Uninstalling xxhash-3.6.0:\n",
      "      Successfully uninstalled xxhash-3.6.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2023.3\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2023.3\n",
      "    Uninstalling tzdata-2023.3:\n",
      "      Successfully uninstalled tzdata-2023.3\n",
      "    Uninstalling tzdata-2023.3:\n",
      "      Successfully uninstalled tzdata-2023.3\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.15.0\n",
      "    Uninstalling typing_extensions-4.15.0:\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.15.0\n",
      "    Uninstalling typing_extensions-4.15.0:\n",
      "      Successfully uninstalled typing_extensions-4.15.0\n",
      "      Successfully uninstalled typing_extensions-4.15.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.5\n",
      "    Uninstalling tqdm-4.66.5:\n",
      "      Successfully uninstalled tqdm-4.66.5\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.5\n",
      "    Uninstalling tqdm-4.66.5:\n",
      "      Successfully uninstalled tqdm-4.66.5\n",
      "  Attempting uninstall: threadpoolctl\n",
      "    Found existing installation: threadpoolctl 3.5.0\n",
      "    Uninstalling threadpoolctl-3.5.0:\n",
      "  Attempting uninstall: threadpoolctl\n",
      "    Found existing installation: threadpoolctl 3.5.0\n",
      "    Uninstalling threadpoolctl-3.5.0:\n",
      "      Successfully uninstalled threadpoolctl-3.5.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.14.0\n",
      "      Successfully uninstalled threadpoolctl-3.5.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.14.0\n",
      "    Uninstalling sympy-1.14.0:\n",
      "    Uninstalling sympy-1.14.0:\n",
      "      Successfully uninstalled sympy-1.14.0\n",
      "      Successfully uninstalled sympy-1.14.0\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.1.0\n",
      "    Uninstalling setuptools-75.1.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.1.0\n",
      "    Uninstalling setuptools-75.1.0:\n",
      "      Successfully uninstalled setuptools-75.1.0\n",
      "      Successfully uninstalled setuptools-75.1.0\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.7.0\n",
      "    Uninstalling safetensors-0.7.0:\n",
      "      Successfully uninstalled safetensors-0.7.0\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2024.9.11\n",
      "    Uninstalling regex-2024.9.11:\n",
      "      Successfully uninstalled regex-2024.9.11\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.7.0\n",
      "    Uninstalling safetensors-0.7.0:\n",
      "      Successfully uninstalled safetensors-0.7.0\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2024.9.11\n",
      "    Uninstalling regex-2024.9.11:\n",
      "      Successfully uninstalled regex-2024.9.11\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 22.0.0\n",
      "    Uninstalling pyarrow-22.0.0:\n",
      "      Successfully uninstalled pyarrow-22.0.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 22.0.0\n",
      "    Uninstalling pyarrow-22.0.0:\n",
      "      Successfully uninstalled pyarrow-22.0.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.1\n",
      "    Uninstalling packaging-24.1:\n",
      "      Successfully uninstalled packaging-24.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.1\n",
      "    Uninstalling packaging-24.1:\n",
      "      Successfully uninstalled packaging-24.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.3\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.3\n",
      "    Uninstalling networkx-3.3:\n",
      "      Successfully uninstalled networkx-3.3\n",
      "    Uninstalling networkx-3.3:\n",
      "      Successfully uninstalled networkx-3.3\n",
      "  Attempting uninstall: multidict\n",
      "    Found existing installation: multidict 6.0.4\n",
      "    Uninstalling multidict-6.0.4:\n",
      "      Successfully uninstalled multidict-6.0.4\n",
      "  Attempting uninstall: multidict\n",
      "    Found existing installation: multidict 6.0.4\n",
      "    Uninstalling multidict-6.0.4:\n",
      "      Successfully uninstalled multidict-6.0.4\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.3\n",
      "    Uninstalling MarkupSafe-2.1.3:\n",
      "      Successfully uninstalled MarkupSafe-2.1.3\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.4.2\n",
      "    Uninstalling joblib-1.4.2:\n",
      "      Successfully uninstalled joblib-1.4.2\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.3\n",
      "    Uninstalling MarkupSafe-2.1.3:\n",
      "      Successfully uninstalled MarkupSafe-2.1.3\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.4.2\n",
      "    Uninstalling joblib-1.4.2:\n",
      "      Successfully uninstalled joblib-1.4.2\n",
      "  Attempting uninstall: idna\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.7\n",
      "    Uninstalling idna-3.7:\n",
      "      Successfully uninstalled idna-3.7\n",
      "    Found existing installation: idna 3.7\n",
      "    Uninstalling idna-3.7:\n",
      "      Successfully uninstalled idna-3.7\n",
      "  Attempting uninstall: hf-xet\n",
      "    Found existing installation: hf-xet 1.2.0\n",
      "    Uninstalling hf-xet-1.2.0:\n",
      "      Successfully uninstalled hf-xet-1.2.0\n",
      "  Attempting uninstall: hf-xet\n",
      "    Found existing installation: hf-xet 1.2.0\n",
      "    Uninstalling hf-xet-1.2.0:\n",
      "      Successfully uninstalled hf-xet-1.2.0\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: fsspec\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "  Attempting uninstall: frozenlist\n",
      "    Found existing installation: frozenlist 1.4.0\n",
      "    Uninstalling frozenlist-1.4.0:\n",
      "      Successfully uninstalled frozenlist-1.4.0\n",
      "  Attempting uninstall: frozenlist\n",
      "    Found existing installation: frozenlist 1.4.0\n",
      "    Uninstalling frozenlist-1.4.0:\n",
      "      Successfully uninstalled frozenlist-1.4.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.13.1\n",
      "    Uninstalling filelock-3.13.1:\n",
      "      Successfully uninstalled filelock-3.13.1\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.4.0\n",
      "    Uninstalling dill-0.4.0:\n",
      "      Successfully uninstalled dill-0.4.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.13.1\n",
      "    Uninstalling filelock-3.13.1:\n",
      "      Successfully uninstalled filelock-3.13.1\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.4.0\n",
      "    Uninstalling dill-0.4.0:\n",
      "      Successfully uninstalled dill-0.4.0\n",
      "  Attempting uninstall: charset_normalizer\n",
      "    Found existing installation: charset-normalizer 3.3.2\n",
      "    Uninstalling charset-normalizer-3.3.2:\n",
      "      Successfully uninstalled charset-normalizer-3.3.2\n",
      "  Attempting uninstall: certifi\n",
      "  Attempting uninstall: charset_normalizer\n",
      "    Found existing installation: charset-normalizer 3.3.2\n",
      "    Uninstalling charset-normalizer-3.3.2:\n",
      "      Successfully uninstalled charset-normalizer-3.3.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2025.11.12\n",
      "    Uninstalling certifi-2025.11.12:\n",
      "      Successfully uninstalled certifi-2025.11.12\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "    Found existing installation: certifi 2025.11.12\n",
      "    Uninstalling certifi-2025.11.12:\n",
      "      Successfully uninstalled certifi-2025.11.12\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "  Attempting uninstall: aiohappyeyeballs\n",
      "    Found existing installation: aiohappyeyeballs 2.4.0\n",
      "    Uninstalling aiohappyeyeballs-2.4.0:\n",
      "      Successfully uninstalled aiohappyeyeballs-2.4.0\n",
      "  Attempting uninstall: yarl\n",
      "    Found existing installation: yarl 1.11.0\n",
      "    Uninstalling yarl-1.11.0:\n",
      "      Successfully uninstalled yarl-1.11.0\n",
      "  Attempting uninstall: aiohappyeyeballs\n",
      "    Found existing installation: aiohappyeyeballs 2.4.0\n",
      "    Uninstalling aiohappyeyeballs-2.4.0:\n",
      "      Successfully uninstalled aiohappyeyeballs-2.4.0\n",
      "  Attempting uninstall: yarl\n",
      "    Found existing installation: yarl 1.11.0\n",
      "    Uninstalling yarl-1.11.0:\n",
      "      Successfully uninstalled yarl-1.11.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.13.1\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.13.1\n",
      "    Uninstalling scipy-1.13.1:\n",
      "    Uninstalling scipy-1.13.1:\n",
      "      Successfully uninstalled scipy-1.13.1\n",
      "      Successfully uninstalled scipy-1.13.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.5\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.5\n",
      "    Uninstalling requests-2.32.5:\n",
      "      Successfully uninstalled requests-2.32.5\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "    Uninstalling requests-2.32.5:\n",
      "      Successfully uninstalled requests-2.32.5\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: multiprocess\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.18\n",
      "    Uninstalling multiprocess-0.70.18:\n",
      "      Successfully uninstalled multiprocess-0.70.18\n",
      "    Found existing installation: multiprocess 0.70.18\n",
      "    Uninstalling multiprocess-0.70.18:\n",
      "      Successfully uninstalled multiprocess-0.70.18\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.4\n",
      "    Uninstalling Jinja2-3.1.4:\n",
      "      Successfully uninstalled Jinja2-3.1.4\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.4\n",
      "    Uninstalling Jinja2-3.1.4:\n",
      "      Successfully uninstalled Jinja2-3.1.4\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.2\n",
      "    Uninstalling httpcore-1.0.2:\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.2\n",
      "    Uninstalling httpcore-1.0.2:\n",
      "      Successfully uninstalled httpcore-1.0.2\n",
      "  Attempting uninstall: anyio\n",
      "      Successfully uninstalled httpcore-1.0.2\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.2.0\n",
      "    Uninstalling anyio-4.2.0:\n",
      "      Successfully uninstalled anyio-4.2.0\n",
      "    Found existing installation: anyio 4.2.0\n",
      "    Uninstalling anyio-4.2.0:\n",
      "      Successfully uninstalled anyio-4.2.0\n",
      "  Attempting uninstall: aiosignal\n",
      "    Found existing installation: aiosignal 1.2.0\n",
      "    Uninstalling aiosignal-1.2.0:\n",
      "      Successfully uninstalled aiosignal-1.2.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.7.1\n",
      "  Attempting uninstall: aiosignal\n",
      "    Found existing installation: aiosignal 1.2.0\n",
      "    Uninstalling aiosignal-1.2.0:\n",
      "      Successfully uninstalled aiosignal-1.2.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.7.1\n",
      "    Uninstalling torch-2.7.1:\n",
      "    Uninstalling torch-2.7.1:\n",
      "      Successfully uninstalled torch-2.7.1\n",
      "      Successfully uninstalled torch-2.7.1\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.5.1\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.5.1\n",
      "    Uninstalling scikit-learn-1.5.1:\n",
      "    Uninstalling scikit-learn-1.5.1:\n",
      "      Successfully uninstalled scikit-learn-1.5.1\n",
      "      Successfully uninstalled scikit-learn-1.5.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.3.3\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.3.3\n",
      "    Uninstalling pandas-2.3.3:\n",
      "      Successfully uninstalled pandas-2.3.3\n",
      "    Uninstalling pandas-2.3.3:\n",
      "      Successfully uninstalled pandas-2.3.3\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.36.0\n",
      "    Uninstalling huggingface-hub-0.36.0:\n",
      "      Successfully uninstalled huggingface-hub-0.36.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.36.0\n",
      "    Uninstalling huggingface-hub-0.36.0:\n",
      "      Successfully uninstalled huggingface-hub-0.36.0\n",
      "  Attempting uninstall: httpx\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.27.0\n",
      "    Uninstalling httpx-0.27.0:\n",
      "      Successfully uninstalled httpx-0.27.0\n",
      "    Found existing installation: httpx 0.27.0\n",
      "    Uninstalling httpx-0.27.0:\n",
      "      Successfully uninstalled httpx-0.27.0\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.10.5\n",
      "    Uninstalling aiohttp-3.10.5:\n",
      "      Successfully uninstalled aiohttp-3.10.5\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.10.5\n",
      "    Uninstalling aiohttp-3.10.5:\n",
      "      Successfully uninstalled aiohttp-3.10.5\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.22.1\n",
      "    Uninstalling tokenizers-0.22.1:\n",
      "      Successfully uninstalled tokenizers-0.22.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.22.1\n",
      "    Uninstalling tokenizers-0.22.1:\n",
      "      Successfully uninstalled tokenizers-0.22.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.57.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.57.3\n",
      "    Uninstalling transformers-4.57.3:\n",
      "    Uninstalling transformers-4.57.3:\n",
      "      Successfully uninstalled transformers-4.57.3\n",
      "      Successfully uninstalled transformers-4.57.3\n",
      "  Attempting uninstall: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.4.1\n",
      "    Uninstalling datasets-4.4.1:\n",
      "      Successfully uninstalled datasets-4.4.1\n",
      "    Found existing installation: datasets 4.4.1\n",
      "    Uninstalling datasets-4.4.1:\n",
      "      Successfully uninstalled datasets-4.4.1\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 5.2.0\n",
      "    Uninstalling sentence-transformers-5.2.0:\n",
      "      Successfully uninstalled sentence-transformers-5.2.0\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 5.2.0\n",
      "    Uninstalling sentence-transformers-5.2.0:\n",
      "      Successfully uninstalled sentence-transformers-5.2.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aext-panels 4.1.0 requires anaconda-cloud-auth>=0.7.1, which is not installed.\n",
      "aext-core 4.1.0 requires anaconda-cloud-auth>=0.7.1, which is not installed.\n",
      "aext-shared 4.1.0 requires anaconda-cloud-auth>=0.7.1, which is not installed.\n",
      "aext-panels-server 4.1.0 requires anaconda-cloud-auth>=0.7.1, which is not installed.\n",
      "aext-assistant-server 4.1.0 requires anaconda-cloud-auth>=0.7.1, which is not installed.\n",
      "aext-share-notebook-server 4.1.0 requires anaconda-cloud-auth>=0.7.1, which is not installed.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.5 which is incompatible.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.16.3 which is incompatible.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.3.5 which is incompatible.\n",
      "s3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2025.10.0 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-3.0.3 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 anyio-4.12.0 attrs-25.4.0 certifi-2025.11.12 charset_normalizer-3.4.4 datasets-4.4.1 dill-0.4.0 filelock-3.20.0 frozenlist-1.8.0 fsspec-2025.10.0 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.36.0 idna-3.11 jinja2-3.1.6 joblib-1.5.2 mpmath-1.3.0 multidict-6.7.0 multiprocess-0.70.18 networkx-3.6.1 numpy-2.3.5 packaging-25.0 pandas-2.3.3 propcache-0.4.1 pyarrow-22.0.0 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.3 regex-2025.11.3 requests-2.32.5 safetensors-0.7.0 scikit-learn-1.8.0 scipy-1.16.3 sentence-transformers-5.2.0 setuptools-80.9.0 six-1.17.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.22.1 torch-2.9.1 tqdm-4.67.1 transformers-4.57.3 typing_extensions-4.15.0 tzdata-2025.3 urllib3-2.6.2 xxhash-3.6.0 yarl-1.22.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aext-panels 4.1.0 requires anaconda-cloud-auth>=0.7.1, which is not installed.\n",
      "aext-core 4.1.0 requires anaconda-cloud-auth>=0.7.1, which is not installed.\n",
      "aext-shared 4.1.0 requires anaconda-cloud-auth>=0.7.1, which is not installed.\n",
      "aext-panels-server 4.1.0 requires anaconda-cloud-auth>=0.7.1, which is not installed.\n",
      "aext-assistant-server 4.1.0 requires anaconda-cloud-auth>=0.7.1, which is not installed.\n",
      "aext-share-notebook-server 4.1.0 requires anaconda-cloud-auth>=0.7.1, which is not installed.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.5 which is incompatible.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.16.3 which is incompatible.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.3.5 which is incompatible.\n",
      "s3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2025.10.0 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-3.0.3 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 anyio-4.12.0 attrs-25.4.0 certifi-2025.11.12 charset_normalizer-3.4.4 datasets-4.4.1 dill-0.4.0 filelock-3.20.0 frozenlist-1.8.0 fsspec-2025.10.0 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.36.0 idna-3.11 jinja2-3.1.6 joblib-1.5.2 mpmath-1.3.0 multidict-6.7.0 multiprocess-0.70.18 networkx-3.6.1 numpy-2.3.5 packaging-25.0 pandas-2.3.3 propcache-0.4.1 pyarrow-22.0.0 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.3 regex-2025.11.3 requests-2.32.5 safetensors-0.7.0 scikit-learn-1.8.0 scipy-1.16.3 sentence-transformers-5.2.0 setuptools-80.9.0 six-1.17.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.22.1 torch-2.9.1 tqdm-4.67.1 transformers-4.57.3 typing_extensions-4.15.0 tzdata-2025.3 urllib3-2.6.2 xxhash-3.6.0 yarl-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --force-reinstall datasets sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba0a0c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.3.5-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.3.5-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting scipy>=1.7.0 (from gensim)\n",
      "  Using cached scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting smart_open>=1.8.1 (from gensim)\n",
      "Collecting scipy>=1.7.0 (from gensim)\n",
      "  Using cached scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting smart_open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart_open>=1.8.1->gensim)\n",
      "  Downloading wrapt-2.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Downloading gensim-4.4.0-cp312-cp312-macosx_11_0_arm64.whl (24.5 MB)\n",
      "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/24.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mCollecting wrapt (from smart_open>=1.8.1->gensim)\n",
      "  Downloading wrapt-2.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Downloading gensim-4.4.0-cp312-cp312-macosx_11_0_arm64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-2.3.5-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Using cached scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-2.3.5-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Using cached scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl (20.9 MB)\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Downloading wrapt-2.0.1-cp312-cp312-macosx_11_0_arm64.whl (61 kB)\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Downloading wrapt-2.0.1-cp312-cp312-macosx_11_0_arm64.whl (61 kB)\n",
      "Installing collected packages: wrapt, numpy, smart_open, scipy, gensim\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.14.1\n",
      "    Uninstalling wrapt-1.14.1:\n",
      "      Successfully uninstalled wrapt-1.14.1\n",
      "  Attempting uninstall: numpy\n",
      "Installing collected packages: wrapt, numpy, smart_open, scipy, gensim\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.14.1\n",
      "    Uninstalling wrapt-1.14.1:\n",
      "      Successfully uninstalled wrapt-1.14.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.5\n",
      "    Uninstalling numpy-2.3.5:\n",
      "    Found existing installation: numpy 2.3.5\n",
      "    Uninstalling numpy-2.3.5:\n",
      "      Successfully uninstalled numpy-2.3.5\n",
      "      Successfully uninstalled numpy-2.3.5\n",
      "  Attempting uninstall: smart_open\n",
      "    Found existing installation: smart-open 5.2.1\n",
      "    Uninstalling smart-open-5.2.1:\n",
      "      Successfully uninstalled smart-open-5.2.1\n",
      "  Attempting uninstall: scipy\n",
      "  Attempting uninstall: smart_open\n",
      "    Found existing installation: smart-open 5.2.1\n",
      "    Uninstalling smart-open-5.2.1:\n",
      "      Successfully uninstalled smart-open-5.2.1\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.16.3\n",
      "    Found existing installation: scipy 1.16.3\n",
      "    Uninstalling scipy-1.16.3:\n",
      "      Successfully uninstalled scipy-1.16.3\n",
      "    Uninstalling scipy-1.16.3:\n",
      "      Successfully uninstalled scipy-1.16.3\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 4.3.3\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 4.3.3\n",
      "    Uninstalling gensim-4.3.3:\n",
      "      Successfully uninstalled gensim-4.3.3\n",
      "    Uninstalling gensim-4.3.3:\n",
      "      Successfully uninstalled gensim-4.3.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.3.5 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\n",
      "aiobotocore 2.12.3 requires wrapt<2.0.0,>=1.10.10, but you have wrapt 2.0.1 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "astroid 2.14.2 requires wrapt<2,>=1.14; python_version >= \"3.11\", but you have wrapt 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed gensim-4.4.0 numpy-2.3.5 scipy-1.16.3 smart_open-7.5.0 wrapt-2.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.3.5 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\n",
      "aiobotocore 2.12.3 requires wrapt<2.0.0,>=1.10.10, but you have wrapt 2.0.1 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "astroid 2.14.2 requires wrapt<2,>=1.14; python_version >= \"3.11\", but you have wrapt 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed gensim-4.4.0 numpy-2.3.5 scipy-1.16.3 smart_open-7.5.0 wrapt-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --force-reinstall gensim numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e41be412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/w4/207pk0bd1k932myh93lvc6jr0000gn/T/ipykernel_92226/3558336275.py\", line 3, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/arrays/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/arrays/arrow/__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/arrays/arrow/array.py\", line 53, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/ops/__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/ops/array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/computation/expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/computation/check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/compat/_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"/opt/anaconda3/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/numexpr/__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/w4/207pk0bd1k932myh93lvc6jr0000gn/T/ipykernel_92226/3558336275.py\", line 3, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/arrays/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/arrays/arrow/__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/arrays/arrow/array.py\", line 67, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/arrays/masked.py\", line 61, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/core/nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pandas/compat/_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"/opt/anaconda3/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/bottleneck/__init__.py\", line 7, in <module>\n",
      "    from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.3.5 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/numpy/core/_multiarray_umath.py:46\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     48\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.3.5 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringIO\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Field\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_classic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AgentExecutor, create_react_agent\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tool\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_classic/agents/__init__.py:27\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_classic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     Agent,\n\u001b[1;32m     20\u001b[0m     AgentExecutor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     LLMSingleActionAgent,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_classic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_iterator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AgentExecutorIterator\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_classic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     create_vectorstore_agent,\n\u001b[1;32m     29\u001b[0m     create_vectorstore_router_agent,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_classic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AgentType\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_classic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversational\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversationalAgent\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_classic/agents/agent_toolkits/__init__.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretriever\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_retriever_tool\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_classic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_importer\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_classic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversational_retrieval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     create_conversational_retrieval_agent,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_classic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     create_vectorstore_agent,\n\u001b[1;32m     29\u001b[0m     create_vectorstore_router_agent,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_classic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtoolkit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     VectorStoreInfo,\n\u001b[1;32m     33\u001b[0m     VectorStoreRouterToolkit,\n\u001b[1;32m     34\u001b[0m     VectorStoreToolkit,\n\u001b[1;32m     35\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_classic/agents/agent_toolkits/conversational_retrieval/openai_functions.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseTool\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_classic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AgentExecutor\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_classic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai_functions_agent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_token_buffer_memory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     AgentTokenBufferMemory,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_classic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai_functions_agent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIFunctionsAgent\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_classic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_memory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseMemory\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_classic/agents/openai_functions_agent/agent_token_buffer_memory.py:13\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m override\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_classic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformat_scratchpad\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     format_to_openai_function_messages,\n\u001b[1;32m     11\u001b[0m     format_to_tool_messages,\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_classic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_memory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseChatMemory\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAgentTokenBufferMemory\u001b[39;00m(BaseChatMemory):\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Memory used to save agent output AND intermediate steps.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m        format_as_tools: Whether to format as tools.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_classic/memory/__init__.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_classic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtoken_buffer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversationTokenBufferMemory\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_classic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorStoreRetrieverMemory\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_classic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstore_token_buffer_memory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     ConversationVectorStoreTokenBufferMemory,  \u001b[38;5;66;03m# avoid circular import\u001b[39;00m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_message_histories\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     31\u001b[0m         AstraDBChatMessageHistory,\n\u001b[1;32m     32\u001b[0m         CassandraChatMessageHistory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m         ZepChatMessageHistory,\n\u001b[1;32m     48\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_classic/memory/vectorstore_token_buffer_memory.py:23\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_classic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     ConversationTokenBufferMemory,\n\u001b[1;32m     20\u001b[0m     VectorStoreRetrieverMemory,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_classic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_memory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseChatMemory\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_classic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[1;32m     25\u001b[0m DEFAULT_HISTORY_TEMPLATE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124mCurrent date and time: \u001b[39m\u001b[38;5;132;01m{current_time}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m     27\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     34\u001b[0m TIMESTAMP_FORMAT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_classic/text_splitter.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Kept for backwards compatibility.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     Language,\n\u001b[1;32m      5\u001b[0m     RecursiveCharacterTextSplitter,\n\u001b[1;32m      6\u001b[0m     TextSplitter,\n\u001b[1;32m      7\u001b[0m     Tokenizer,\n\u001b[1;32m      8\u001b[0m     TokenTextSplitter,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_text_on_tokens\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcharacter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CharacterTextSplitter\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_text_splitters/__init__.py:19\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     Language,\n\u001b[1;32m     10\u001b[0m     TextSplitter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     split_text_on_tokens,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcharacter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     CharacterTextSplitter,\n\u001b[1;32m     17\u001b[0m     RecursiveCharacterTextSplitter,\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhtml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     ElementType,\n\u001b[1;32m     21\u001b[0m     HTMLHeaderTextSplitter,\n\u001b[1;32m     22\u001b[0m     HTMLSectionSplitter,\n\u001b[1;32m     23\u001b[0m     HTMLSemanticPreservingSplitter,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveJsonSplitter\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjsx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JSFrameworkTextSplitter\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_text_splitters/html.py:32\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01melement\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResultSet\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     _HAS_NLTK \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/__init__.py:146\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjsontags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# PACKAGES\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/chunk/__init__.py:155\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Natural Language Toolkit: Chunkers\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2001-2024 NLTK Project\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# For license information, see LICENSE.TXT\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mClasses and interfaces for identifying non-overlapping linguistic\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mgroups (such as base noun phrases) in unrestricted text.  This task is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m     pattern is valid.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChunkParserI\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnamed_entity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Maxent_NE_Chunker\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregexp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RegexpChunkParser, RegexpParser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/chunk/api.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChunkScore\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParserI\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mChunkParserI\u001b[39;00m(ParserI):\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    A processing interface for identifying non-overlapping groups in\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    unrestricted text.  Typically, chunk parsers are used to find base\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    will always generate a parse.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/parse/__init__.py:100\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecursivedescent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     96\u001b[0m     RecursiveDescentParser,\n\u001b[1;32m     97\u001b[0m     SteppingRecursiveDescentParser,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshiftreduce\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ShiftReduceParser, SteppingShiftReduceParser\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransitionparser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransitionParser\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TestGrammar, extract_test_sentences, load_parser\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mviterbi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ViterbiParser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/parse/transitionparser.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m svm\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_svmlight_file\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/__init__.py:130\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(name):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _submodules:\n\u001b[0;32m--> 130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msklearn.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/svm/__init__.py:10\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# See http://scikit-learn.sourceforge.net/modules/svm.html for complete\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# documentation.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Authors: The scikit-learn developers\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# SPDX-License-Identifier: BSD-3-Clause\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bounds\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m l1_min_c\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_classes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     SVC,\n\u001b[1;32m     12\u001b[0m     SVR,\n\u001b[1;32m     13\u001b[0m     LinearSVC,\n\u001b[1;32m     14\u001b[0m     LinearSVR,\n\u001b[1;32m     15\u001b[0m     NuSVC,\n\u001b[1;32m     16\u001b[0m     NuSVR,\n\u001b[1;32m     17\u001b[0m     OneClassSVM,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1_min_c\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/svm/_classes.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator, OutlierMixin, RegressorMixin, _fit_context\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     LinearClassifierMixin,\n\u001b[1;32m     11\u001b[0m     LinearModel,\n\u001b[1;32m     12\u001b[0m     SparseCoefMixin,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     BaseLibSVM,\n\u001b[1;32m     16\u001b[0m     BaseSVC,\n\u001b[1;32m     17\u001b[0m     _fit_liblinear,\n\u001b[1;32m     18\u001b[0m     _get_liblinear_solver_type,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, StrOptions\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/__init__.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ARDRegression, BayesianRidge\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_coordinate_descent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     ElasticNet,\n\u001b[1;32m     14\u001b[0m     ElasticNetCV,\n\u001b[1;32m     15\u001b[0m     Lasso,\n\u001b[1;32m     16\u001b[0m     LassoCV,\n\u001b[1;32m     17\u001b[0m     MultiTaskElasticNet,\n\u001b[1;32m     18\u001b[0m     MultiTaskElasticNetCV,\n\u001b[1;32m     19\u001b[0m     MultiTaskLasso,\n\u001b[1;32m     20\u001b[0m     MultiTaskLassoCV,\n\u001b[1;32m     21\u001b[0m     enet_path,\n\u001b[1;32m     22\u001b[0m     lasso_path,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_glm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GammaRegressor, PoissonRegressor, TweedieRegressor\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_huber\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuberRegressor\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _cd_fast \u001b[38;5;28;01mas\u001b[39;00m cd_fast  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearModel, _pre_fit, _preprocess_data\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_cv\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch, check_array, check_scalar, metadata_routing\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     MetadataRouter,\n\u001b[1;32m     24\u001b[0m     MethodMapping,\n\u001b[1;32m     25\u001b[0m     _raise_for_params,\n\u001b[1;32m     26\u001b[0m     get_routing_for_object,\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Authors: The scikit-learn developers\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# SPDX-License-Identifier: BSD-3-Clause\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_classification_threshold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     FixedThresholdClassifier,\n\u001b[1;32m     10\u001b[0m     TunedThresholdClassifierCV,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LearningCurveDisplay, ValidationCurveDisplay\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_search\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     GridSearchCV,\n\u001b[1;32m     15\u001b[0m     ParameterGrid,\n\u001b[1;32m     16\u001b[0m     ParameterSampler,\n\u001b[1;32m     17\u001b[0m     RandomizedSearchCV,\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_classification_threshold.py:17\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     BaseEstimator,\n\u001b[1;32m     11\u001b[0m     ClassifierMixin,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     clone,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NotFittedError\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_scoring, get_scorer_names\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_scorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _CurveScorer, _threshold_scores_to_class_labels\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_split\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StratifiedShuffleSplit, check_cv\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Score functions, performance metrics, pairwise metrics and distance computations.\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Authors: The scikit-learn developers\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# SPDX-License-Identifier: BSD-3-Clause\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_classification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     accuracy_score,\n\u001b[1;32m      9\u001b[0m     balanced_accuracy_score,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     zero_one_loss,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dist_metrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistanceMetric\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/cluster/__init__.py:28\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bicluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m consensus_score\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_supervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     adjusted_mutual_info_score,\n\u001b[1;32m     14\u001b[0m     adjusted_rand_score,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     v_measure_score,\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_unsupervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     calinski_harabasz_score,\n\u001b[1;32m     30\u001b[0m     davies_bouldin_score,\n\u001b[1;32m     31\u001b[0m     silhouette_samples,\n\u001b[1;32m     32\u001b[0m     silhouette_score,\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     35\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjusted_mutual_info_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjusted_rand_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv_measure_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     56\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/cluster/_unsupervised.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternals\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray_api_compat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_array\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     _VALID_METRICS,\n\u001b[1;32m     15\u001b[0m     pairwise_distances,\n\u001b[1;32m     16\u001b[0m     pairwise_distances_chunked,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _safe_indexing, check_random_state, check_X_y\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/pairwise.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pairwise_distances_reduction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArgKmin\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pairwise_fast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _chi2_kernel_fast, _sparse_manhattan\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_pairwise_distances_reduction/__init__.py:94\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Authors: The scikit-learn developers\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# SPDX-License-Identifier: BSD-3-Clause\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m#    (see :class:`MiddleTermComputer{32,64}`).\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pairwise_distances_reduction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dispatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     95\u001b[0m     ArgKmin,\n\u001b[1;32m     96\u001b[0m     ArgKminClassMode,\n\u001b[1;32m     97\u001b[0m     BaseDistancesReductionDispatcher,\n\u001b[1;32m     98\u001b[0m     RadiusNeighbors,\n\u001b[1;32m     99\u001b[0m     RadiusNeighborsClassMode,\n\u001b[1;32m    100\u001b[0m     sqeuclidean_row_norms,\n\u001b[1;32m    101\u001b[0m )\n\u001b[1;32m    103\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgKmin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgKminClassMode\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqeuclidean_row_norms\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    110\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dist_metrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BOOL_METRICS, METRIC_MAPPING64, DistanceMetric\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pairwise_distances_reduction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_argkmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArgKmin32, ArgKmin64\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pairwise_distances_reduction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_argkmin_classmode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     ArgKminClassMode32,\n\u001b[1;32m     15\u001b[0m     ArgKminClassMode64,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pairwise_distances_reduction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     _sqeuclidean_row_norms32,\n\u001b[1;32m     19\u001b[0m     _sqeuclidean_row_norms64,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pairwise_distances_reduction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_radius_neighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     RadiusNeighbors32,\n\u001b[1;32m     23\u001b[0m     RadiusNeighbors64,\n\u001b[1;32m     24\u001b[0m )\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:645\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_classic.agents import AgentExecutor, create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c835fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ReAct Agent built successfully.\n"
     ]
    }
   ],
   "source": [
    "# Connected to LLM running locally\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://127.0.0.1:1234/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    model=\"local-model\",\n",
    "    temperature=0,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Define the Tool\n",
    "@tool\n",
    "def fetch_csv_dataset(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads a CSV dataset from a URL and returns a summary.\n",
    "    Input should be the full URL string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse CSV\n",
    "        content = response.content.decode('utf-8')\n",
    "        df = pd.read_csv(StringIO(content), on_bad_lines='skip')\n",
    "        \n",
    "        return (\n",
    "            f\"SUCCESS: Downloaded data from {url}\\n\"\n",
    "            f\"Shape: {df.shape}\\n\"\n",
    "            f\"Columns: {list(df.columns)}\\n\"\n",
    "            f\"First 5 rows:\\n{df.head().to_string()}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "tools = [fetch_csv_dataset]\n",
    "\n",
    "# Define the ReAct Prompt (Hardcoded for stability)\n",
    "# This teaches the model explicitly how to think and act.\n",
    "template = '''Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}'''\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 4. Create the ReAct Agent\n",
    "# This uses simple text generation, avoiding the Pydantic/Tool Binding error completely.\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "\n",
    "# 5. Create the Executor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    verbose=True, \n",
    "    handle_parsing_errors=True # IMPORTANT for local models\n",
    ")\n",
    "\n",
    "print(\" ReAct Agent built successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8a0a1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: fetch_csv_dataset\n",
      "Action Input: \"https://raw.githubusercontent.com/gramener/datasets/refs/heads/main/card_transactions.csv\"\u001b[0m\u001b[32;1m\u001b[1;3mAction: fetch_csv_dataset\n",
      "Action Input: \"https://raw.githubusercontent.com/gramener/datasets/refs/heads/main/card_transactions.csv\"\u001b[0m\u001b[36;1m\u001b[1;3mSUCCESS: Downloaded data from https://raw.githubusercontent.com/gramener/datasets/refs/heads/main/card_transactions.csv\n",
      "Shape: (10000, 21)\n",
      "Columns: ['ID', 'Transaction Time', 'Transaction Type', 'Channel', 'Is 3DS', 'Is Token', 'Decision', 'Decline Reason Code', 'Issuer Name', 'Issuer Country', 'Issuer Region', 'Funding source', 'Acquirer Name', 'Acquirer Country', 'Acquirer Region', 'Jurisdiction', 'Is Dispute', 'Is Fraud', 'Dispute Type', 'Amount', 'Industry Segment']\n",
      "First 5 rows:\n",
      "                ID          Transaction Time Transaction Type Channel Is 3DS Is Token  Decision Decline Reason Code                             Issuer Name            Issuer Country Issuer Region Funding source               Acquirer Name          Acquirer Country Acquirer Region  Jurisdiction  Is Dispute  Is Fraud Dispute Type      Amount     Industry Segment\n",
      "0  T89784025544295  2024-02-20T11:02:49.000Z             Chip      CP     No       No  Approved            Approved                                    BPCE                    FRANCE        EUROPE        Credit           BOURSORAMA BANQUE                     FRANCE          EUROPE      Domestic           0         0   No Dispute  196.975798  Government services\n",
      "1  T94028747687247  2024-02-18T10:42:19.000Z        Other CNP     CNP     No       No  Approved            Approved  BANK OF AMERICA, NATIONAL ASSOCIATION   UNITED STATES OF AMERICA           USA        Credit   NAVY FEDERAL CREDIT UNION   UNITED STATES OF AMERICA             USA       Domstic           0         0   No Dispute  340.852463  Contracted services\n",
      "2  T84800389951860  2024-04-30T11:10:17.000Z            ECI-7     CNP     No       No  Approved            Approved                       HSBC UK BANK PLC             UNITED KINGDOM        EUROPE        Credit        BARCLAYS BANK UK PLC             UNITED KINGDOM          EUROPE      Domestic           0         0   No Dispute  192.134049      Clothing stores\n",
      "3  T88418230200974  2024-01-08T19:01:35.000Z      Contactless      CP     No      Yes  Approved            Approved                JPMORGAN CHASE BANK N.A   UNITED STATES OF AMERICA           USA        Credit        CHAINA MERCHANTS BANK           CHAINA MAINLAND            APAC  Cross Border           0         0   No Dispute  100.581589  Contracted services\n",
      "4  T19060182064275  2024-05-14T21:03:51.000Z             Chip      CP     No       No  Approved            Approved                    BANCO BRADESCO S.A.                     BRAZIL           LAC        Credit                 SUTTON BANK   UNITED STATES OF AMERICA             USA  Cross Border           0         0   No Dispute  424.332753     Utility services\u001b[0m\u001b[36;1m\u001b[1;3mSUCCESS: Downloaded data from https://raw.githubusercontent.com/gramener/datasets/refs/heads/main/card_transactions.csv\n",
      "Shape: (10000, 21)\n",
      "Columns: ['ID', 'Transaction Time', 'Transaction Type', 'Channel', 'Is 3DS', 'Is Token', 'Decision', 'Decline Reason Code', 'Issuer Name', 'Issuer Country', 'Issuer Region', 'Funding source', 'Acquirer Name', 'Acquirer Country', 'Acquirer Region', 'Jurisdiction', 'Is Dispute', 'Is Fraud', 'Dispute Type', 'Amount', 'Industry Segment']\n",
      "First 5 rows:\n",
      "                ID          Transaction Time Transaction Type Channel Is 3DS Is Token  Decision Decline Reason Code                             Issuer Name            Issuer Country Issuer Region Funding source               Acquirer Name          Acquirer Country Acquirer Region  Jurisdiction  Is Dispute  Is Fraud Dispute Type      Amount     Industry Segment\n",
      "0  T89784025544295  2024-02-20T11:02:49.000Z             Chip      CP     No       No  Approved            Approved                                    BPCE                    FRANCE        EUROPE        Credit           BOURSORAMA BANQUE                     FRANCE          EUROPE      Domestic           0         0   No Dispute  196.975798  Government services\n",
      "1  T94028747687247  2024-02-18T10:42:19.000Z        Other CNP     CNP     No       No  Approved            Approved  BANK OF AMERICA, NATIONAL ASSOCIATION   UNITED STATES OF AMERICA           USA        Credit   NAVY FEDERAL CREDIT UNION   UNITED STATES OF AMERICA             USA       Domstic           0         0   No Dispute  340.852463  Contracted services\n",
      "2  T84800389951860  2024-04-30T11:10:17.000Z            ECI-7     CNP     No       No  Approved            Approved                       HSBC UK BANK PLC             UNITED KINGDOM        EUROPE        Credit        BARCLAYS BANK UK PLC             UNITED KINGDOM          EUROPE      Domestic           0         0   No Dispute  192.134049      Clothing stores\n",
      "3  T88418230200974  2024-01-08T19:01:35.000Z      Contactless      CP     No      Yes  Approved            Approved                JPMORGAN CHASE BANK N.A   UNITED STATES OF AMERICA           USA        Credit        CHAINA MERCHANTS BANK           CHAINA MAINLAND            APAC  Cross Border           0         0   No Dispute  100.581589  Contracted services\n",
      "4  T19060182064275  2024-05-14T21:03:51.000Z             Chip      CP     No       No  Approved            Approved                    BANCO BRADESCO S.A.                     BRAZIL           LAC        Credit                 SUTTON BANK   UNITED STATES OF AMERICA             USA  Cross Border           0         0   No Dispute  424.332753     Utility services\u001b[0m\u001b[32;1m\u001b[1;3mFinal Answer: The dataset contains the following columns:\n",
      "\n",
      "1. ID\n",
      "2. Transaction Time\n",
      "3. Transaction Type\n",
      "4. Channel\n",
      "5. Is 3DS\n",
      "6. Is Token\n",
      "7. Decision\n",
      "8. Decline Reason Code\n",
      "9. Issuer Name\n",
      "10. Issuer Country\n",
      "11. Issuer Region\n",
      "12. Funding source\n",
      "13. Acquirer Name\n",
      "14. Acquirer Country\n",
      "15. Acquirer Region\n",
      "16. Jurisdiction\n",
      "17. Is Dispute\n",
      "18. Is Fraud\n",
      "19. Dispute Type\n",
      "20. Amount\n",
      "21. Industry Segment\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "--- FINAL ANSWER ---\n",
      "The dataset contains the following columns:\n",
      "\n",
      "1. ID\n",
      "2. Transaction Time\n",
      "3. Transaction Type\n",
      "4. Channel\n",
      "5. Is 3DS\n",
      "6. Is Token\n",
      "7. Decision\n",
      "8. Decline Reason Code\n",
      "9. Issuer Name\n",
      "10. Issuer Country\n",
      "11. Issuer Region\n",
      "12. Funding source\n",
      "13. Acquirer Name\n",
      "14. Acquirer Country\n",
      "15. Acquirer Region\n",
      "16. Jurisdiction\n",
      "17. Is Dispute\n",
      "18. Is Fraud\n",
      "19. Dispute Type\n",
      "20. Amount\n",
      "21. Industry Segment\n",
      "\u001b[32;1m\u001b[1;3mFinal Answer: The dataset contains the following columns:\n",
      "\n",
      "1. ID\n",
      "2. Transaction Time\n",
      "3. Transaction Type\n",
      "4. Channel\n",
      "5. Is 3DS\n",
      "6. Is Token\n",
      "7. Decision\n",
      "8. Decline Reason Code\n",
      "9. Issuer Name\n",
      "10. Issuer Country\n",
      "11. Issuer Region\n",
      "12. Funding source\n",
      "13. Acquirer Name\n",
      "14. Acquirer Country\n",
      "15. Acquirer Region\n",
      "16. Jurisdiction\n",
      "17. Is Dispute\n",
      "18. Is Fraud\n",
      "19. Dispute Type\n",
      "20. Amount\n",
      "21. Industry Segment\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "--- FINAL ANSWER ---\n",
      "The dataset contains the following columns:\n",
      "\n",
      "1. ID\n",
      "2. Transaction Time\n",
      "3. Transaction Type\n",
      "4. Channel\n",
      "5. Is 3DS\n",
      "6. Is Token\n",
      "7. Decision\n",
      "8. Decline Reason Code\n",
      "9. Issuer Name\n",
      "10. Issuer Country\n",
      "11. Issuer Region\n",
      "12. Funding source\n",
      "13. Acquirer Name\n",
      "14. Acquirer Country\n",
      "15. Acquirer Region\n",
      "16. Jurisdiction\n",
      "17. Is Dispute\n",
      "18. Is Fraud\n",
      "19. Dispute Type\n",
      "20. Amount\n",
      "21. Industry Segment\n"
     ]
    }
   ],
   "source": [
    "test_url = \"https://raw.githubusercontent.com/gramener/datasets/refs/heads/main/card_transactions.csv\"\n",
    "query = f\"Download the dataset from {test_url} and tell me the columns.\"\n",
    "\n",
    "response = agent_executor.invoke({\"input\": query})\n",
    "print(\"\\n--- FINAL ANSWER ---\")\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a40c1ee",
   "metadata": {},
   "source": [
    "# Vector embedding of 2022-2024 news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0848cf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w4/207pk0bd1k932myh93lvc6jr0000gn/T/ipykernel_95702/1031960684.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['date'], errors=\"coerce\", utc=True).dt.tz_convert(None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>tags</th>\n",
       "      <th>extracted_from_tag</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>use</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>environment/2024/dec/31/country-diary-a-hogman...</td>\n",
       "      <td>Country diary: A Hogmanay fire to see in the n...</td>\n",
       "      <td>Its a few hundred metres from where the crab ...</td>\n",
       "      <td>['environment/series/country-diary', 'environm...</td>\n",
       "      <td>environment/forests</td>\n",
       "      <td>BIODIVERSITY</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>True</td>\n",
       "      <td>BIODIVERSITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>us-news/2024/dec/31/arbor-day-foundation-trees...</td>\n",
       "      <td>10m trees to be planted in US to replace ones ...</td>\n",
       "      <td>Some costs of the recently ended supercharged ...</td>\n",
       "      <td>['us-news/us-news', 'environment/forests', 'us...</td>\n",
       "      <td>environment/forests</td>\n",
       "      <td>BIODIVERSITY</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>True</td>\n",
       "      <td>BIODIVERSITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>environment/commentisfree/2024/dec/31/cicada-c...</td>\n",
       "      <td>A cicada: What cicadas leave behind is a kind...</td>\n",
       "      <td>Of all the languages words for cicada, Croati...</td>\n",
       "      <td>['environment/series/the-nature-of', 'commenti...</td>\n",
       "      <td>environment/wildlife</td>\n",
       "      <td>BIODIVERSITY</td>\n",
       "      <td>2024-12-30</td>\n",
       "      <td>True</td>\n",
       "      <td>BIODIVERSITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>environment/2024/dec/30/im-obsessed-with-broke...</td>\n",
       "      <td>Im obsessed with broken shells: they are mark...</td>\n",
       "      <td>I have collected hundreds and hundreds of brok...</td>\n",
       "      <td>['environment/series/why-i-m-obsessed-with', '...</td>\n",
       "      <td>environment/series/seascape-the-state-of-our-o...</td>\n",
       "      <td>BIODIVERSITY</td>\n",
       "      <td>2024-12-30</td>\n",
       "      <td>True</td>\n",
       "      <td>BIODIVERSITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>environment/2024/dec/30/2024s-most-costly-clim...</td>\n",
       "      <td>2024s most costly climate disasters killed 2,...</td>\n",
       "      <td>The worlds 10 most costly climate disasters o...</td>\n",
       "      <td>['environment/climate-crisis', 'world/world', ...</td>\n",
       "      <td>us-news/hurricane-helene</td>\n",
       "      <td>EXTREME_CLIMATE_IMPACTS</td>\n",
       "      <td>2024-12-30</td>\n",
       "      <td>True</td>\n",
       "      <td>EXTREME_CLIMATE_IMPACTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7757</th>\n",
       "      <td>environment/2022/jan/03/country-diary-on-this-...</td>\n",
       "      <td>Country diary: On this hazy morning, the dista...</td>\n",
       "      <td>Ahead of me at the far end of the path, just w...</td>\n",
       "      <td>['environment/series/country-diary', 'uk/rural...</td>\n",
       "      <td>environment/forests</td>\n",
       "      <td>BIODIVERSITY</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>True</td>\n",
       "      <td>BIODIVERSITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7758</th>\n",
       "      <td>commentisfree/2022/jan/03/as-i-bum-shuffled-my...</td>\n",
       "      <td>As I bum-shuffled my way down the scree at Ava...</td>\n",
       "      <td>Nothing beats the New Zealand bush. The writer...</td>\n",
       "      <td>['commentisfree/series/my-wild-place', 'world/...</td>\n",
       "      <td>environment/forests</td>\n",
       "      <td>BIODIVERSITY</td>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>True</td>\n",
       "      <td>BIODIVERSITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7759</th>\n",
       "      <td>culture/2022/jan/02/villagers-fight-to-keep-bb...</td>\n",
       "      <td>Villagers fight to keep BBC Victorian Farm in ...</td>\n",
       "      <td>Of the handful of historic working farms in th...</td>\n",
       "      <td>['culture/museums', 'environment/farming', 'ed...</td>\n",
       "      <td>environment/farming</td>\n",
       "      <td>BIODIVERSITY</td>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>True</td>\n",
       "      <td>BIODIVERSITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7760</th>\n",
       "      <td>us-news/2022/jan/01/colorado-wildfires-destroy...</td>\n",
       "      <td>We lost everything: Colorado wildfire destro...</td>\n",
       "      <td>Just as the flakes of the seasons first winte...</td>\n",
       "      <td>['us-news/colorado', 'world/wildfires', 'us-ne...</td>\n",
       "      <td>world/wildfires</td>\n",
       "      <td>EXTREME_CLIMATE_IMPACTS</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>True</td>\n",
       "      <td>EXTREME_CLIMATE_IMPACTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7761</th>\n",
       "      <td>news/2022/jan/01/weatherwatch-when-rivers-froz...</td>\n",
       "      <td>Weatherwatch: when rivers froze over in 1795</td>\n",
       "      <td>One upside from the climate crisis is that win...</td>\n",
       "      <td>['news/series/weatherwatch', 'environment/envi...</td>\n",
       "      <td>news/series/weatherwatch</td>\n",
       "      <td>EXTREME_CLIMATE_IMPACTS</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>True</td>\n",
       "      <td>EXTREME_CLIMATE_IMPACTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7762 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     id  \\\n",
       "0     environment/2024/dec/31/country-diary-a-hogman...   \n",
       "1     us-news/2024/dec/31/arbor-day-foundation-trees...   \n",
       "2     environment/commentisfree/2024/dec/31/cicada-c...   \n",
       "3     environment/2024/dec/30/im-obsessed-with-broke...   \n",
       "4     environment/2024/dec/30/2024s-most-costly-clim...   \n",
       "...                                                 ...   \n",
       "7757  environment/2022/jan/03/country-diary-on-this-...   \n",
       "7758  commentisfree/2022/jan/03/as-i-bum-shuffled-my...   \n",
       "7759  culture/2022/jan/02/villagers-fight-to-keep-bb...   \n",
       "7760  us-news/2022/jan/01/colorado-wildfires-destroy...   \n",
       "7761  news/2022/jan/01/weatherwatch-when-rivers-froz...   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Country diary: A Hogmanay fire to see in the n...   \n",
       "1     10m trees to be planted in US to replace ones ...   \n",
       "2     A cicada: What cicadas leave behind is a kind...   \n",
       "3     Im obsessed with broken shells: they are mark...   \n",
       "4     2024s most costly climate disasters killed 2,...   \n",
       "...                                                 ...   \n",
       "7757  Country diary: On this hazy morning, the dista...   \n",
       "7758  As I bum-shuffled my way down the scree at Ava...   \n",
       "7759  Villagers fight to keep BBC Victorian Farm in ...   \n",
       "7760  We lost everything: Colorado wildfire destro...   \n",
       "7761       Weatherwatch: when rivers froze over in 1795   \n",
       "\n",
       "                                                   body  \\\n",
       "0     Its a few hundred metres from where the crab ...   \n",
       "1     Some costs of the recently ended supercharged ...   \n",
       "2     Of all the languages words for cicada, Croati...   \n",
       "3     I have collected hundreds and hundreds of brok...   \n",
       "4     The worlds 10 most costly climate disasters o...   \n",
       "...                                                 ...   \n",
       "7757  Ahead of me at the far end of the path, just w...   \n",
       "7758  Nothing beats the New Zealand bush. The writer...   \n",
       "7759  Of the handful of historic working farms in th...   \n",
       "7760  Just as the flakes of the seasons first winte...   \n",
       "7761  One upside from the climate crisis is that win...   \n",
       "\n",
       "                                                   tags  \\\n",
       "0     ['environment/series/country-diary', 'environm...   \n",
       "1     ['us-news/us-news', 'environment/forests', 'us...   \n",
       "2     ['environment/series/the-nature-of', 'commenti...   \n",
       "3     ['environment/series/why-i-m-obsessed-with', '...   \n",
       "4     ['environment/climate-crisis', 'world/world', ...   \n",
       "...                                                 ...   \n",
       "7757  ['environment/series/country-diary', 'uk/rural...   \n",
       "7758  ['commentisfree/series/my-wild-place', 'world/...   \n",
       "7759  ['culture/museums', 'environment/farming', 'ed...   \n",
       "7760  ['us-news/colorado', 'world/wildfires', 'us-ne...   \n",
       "7761  ['news/series/weatherwatch', 'environment/envi...   \n",
       "\n",
       "                                     extracted_from_tag  \\\n",
       "0                                   environment/forests   \n",
       "1                                   environment/forests   \n",
       "2                                  environment/wildlife   \n",
       "3     environment/series/seascape-the-state-of-our-o...   \n",
       "4                              us-news/hurricane-helene   \n",
       "...                                                 ...   \n",
       "7757                                environment/forests   \n",
       "7758                                environment/forests   \n",
       "7759                                environment/farming   \n",
       "7760                                    world/wildfires   \n",
       "7761                           news/series/weatherwatch   \n",
       "\n",
       "                     category       date   use                    label  \n",
       "0                BIODIVERSITY 2024-12-31  True             BIODIVERSITY  \n",
       "1                BIODIVERSITY 2024-12-31  True             BIODIVERSITY  \n",
       "2                BIODIVERSITY 2024-12-30  True             BIODIVERSITY  \n",
       "3                BIODIVERSITY 2024-12-30  True             BIODIVERSITY  \n",
       "4     EXTREME_CLIMATE_IMPACTS 2024-12-30  True  EXTREME_CLIMATE_IMPACTS  \n",
       "...                       ...        ...   ...                      ...  \n",
       "7757             BIODIVERSITY 2022-01-03  True             BIODIVERSITY  \n",
       "7758             BIODIVERSITY 2022-01-02  True             BIODIVERSITY  \n",
       "7759             BIODIVERSITY 2022-01-02  True             BIODIVERSITY  \n",
       "7760  EXTREME_CLIMATE_IMPACTS 2022-01-01  True  EXTREME_CLIMATE_IMPACTS  \n",
       "7761  EXTREME_CLIMATE_IMPACTS 2022-01-01  True  EXTREME_CLIMATE_IMPACTS  \n",
       "\n",
       "[7762 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"dataset/guardian_climate_news_corpus.csv\")\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "df = df[df['date'].dt.year >= 2022].copy()\n",
    "\n",
    "df = df[df['label'] != 'UNRELATED_TO_CLIMATE'].copy()\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d94bd620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "BIODIVERSITY               2575\n",
       "EXTREME_CLIMATE_IMPACTS    1541\n",
       "ENERGY                     1536\n",
       "POLLUTION_AND_WASTE         979\n",
       "EMISSIONS                   525\n",
       "CLIMATE_ACTIVISM            284\n",
       "CLIMATE_POLICY              269\n",
       "GLOBAL_CRISIS                38\n",
       "CLIMATE_DENIAL               15\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaaa4411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                            object\n",
       "title                         object\n",
       "body                          object\n",
       "tags                          object\n",
       "extracted_from_tag            object\n",
       "category                      object\n",
       "date                  datetime64[ns]\n",
       "use                             bool\n",
       "label                         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f402dc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata (Word2Vec) Shape: (7762, 100)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18468e2c38404355893f3cd2cfc7b40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d106afb66b4c10803d50245c638e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c968d9d1d042c2940c1f507a49a216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc2cbcb14984704b9d4edfcd1718f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36dc0aa5715478b9e803c4eb0c8b688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd0595363944b18b1735bf1b533f231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca7a6f0b042644a48456d860c94e42c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e834fe5ce82f44fdb55263f9a712f1c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3ed76b009641d883d7e13884e49bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325d080ac7b84523845a2751723c9e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2e1550dfe240cd88bab755ef6226c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5c768f9fbd4cc2b91b49e08a231ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/243 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text (BERT) Shape: (7762, 384)\n",
      "Final Feature Matrix Shape: (7762, 484)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 1. SETUP: Load your data (Assuming 'df' is your existing DataFrame)\n",
    "# ------------------------------------------------------------------\n",
    "# Ensure date is datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# 2. PREPROCESSING FOR WORD2VEC\n",
    "# ------------------------------------------------------------------\n",
    "# We need to combine tags, date, and category into a single list of \"tokens\" per row.\n",
    "# Word2Vec learns relationships between these tokens (e.g., \"winter\" appears with \"january\").\n",
    "\n",
    "def create_metadata_tokens(row):\n",
    "    # a. Parse tags (convert string representation \"['a', 'b']\" to actual list)\n",
    "    # If your data is already a list, remove ast.literal_eval\n",
    "    try:\n",
    "        tags = ast.literal_eval(row['tags']) if isinstance(row['tags'], str) else row['tags']\n",
    "    except:\n",
    "        tags = []\n",
    "        \n",
    "    # b. Process Category\n",
    "    category_token = f\"CAT_{row['category']}\" # Prefix to avoid collision with common words\n",
    "    \n",
    "    # c. Process Date (Break into semantically useful parts)\n",
    "    # This allows the model to learn patterns like \"Month_12\" -> \"Christmas\"\n",
    "    date_tokens = [\n",
    "        f\"YEAR_{row['date'].year}\",\n",
    "        f\"MONTH_{row['date'].month}\", \n",
    "        f\"DAY_{row['date'].day_name()}\" # e.g., 'Tuesday'\n",
    "    ]\n",
    "    \n",
    "    # Combine all into one \"sentence\"\n",
    "    return tags + [category_token] + date_tokens\n",
    "\n",
    "# Apply function to create a new column of token lists\n",
    "df['metadata_tokens'] = df.apply(create_metadata_tokens, axis=1)\n",
    "\n",
    "# 3. TRAIN WORD2VEC (Tags + Date + Category)\n",
    "# ------------------------------------------------------------------\n",
    "# Train model on your specific metadata patterns\n",
    "w2v_model = Word2Vec(sentences=df['metadata_tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to average word vectors for a single row\n",
    "def get_average_w2v(tokens, model, vector_size):\n",
    "    # Filter out tokens not in vocabulary (just in case)\n",
    "    valid_tokens = [t for t in tokens if t in model.wv]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(vector_size)\n",
    "    # Stack vectors and calculate mean\n",
    "    vectors = np.stack([model.wv[t] for t in valid_tokens])\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Apply to create a feature matrix for metadata\n",
    "metadata_vectors = np.stack(df['metadata_tokens'].apply(\n",
    "    lambda x: get_average_w2v(x, w2v_model, 100)\n",
    "))\n",
    "\n",
    "print(f\"Metadata (Word2Vec) Shape: {metadata_vectors.shape}\")\n",
    "\n",
    "# 4. BERT EMBEDDINGS (Title + Body)\n",
    "# ------------------------------------------------------------------\n",
    "# Load a lightweight, high-performance BERT model\n",
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Clean and Combine Title + Body for a richer text representation\n",
    "# You can keep them separate if you want distinct features for title vs body\n",
    "df['full_text'] = df['title'].fillna('') + \". \" + df['body'].fillna('')\n",
    "\n",
    "# Encode (This might take a moment on large datasets)\n",
    "text_vectors = bert_model.encode(df['full_text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "print(f\"Text (BERT) Shape: {text_vectors.shape}\")\n",
    "\n",
    "# 5. COMBINE EVERYTHING\n",
    "# ------------------------------------------------------------------\n",
    "# Concatenate the Metadata vectors (100 dims) with BERT vectors (384 dims)\n",
    "final_features = np.hstack([metadata_vectors, text_vectors])\n",
    "\n",
    "print(f\"Final Feature Matrix Shape: {final_features.shape}\")\n",
    "# final_features is now ready for your classifier or clustering algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
