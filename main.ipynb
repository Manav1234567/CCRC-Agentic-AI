{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3c403f",
   "metadata": {},
   "source": [
    "# Pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a35a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Force upgrade the critical libraries\n",
    "%pip install -U langchain langchain-core langchain-openai langchain-community pydantic\n",
    "\n",
    "# 2. IMPORTANT: You must restart the kernel after running this!\n",
    "# In VS Code/Jupyter: Click \"Restart\" or \"Restart Kernel\" in the top toolbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentence-transformers gensim datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f93c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --force-reinstall datasets sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a0c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --force-reinstall gensim numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f62fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U sentence-transformers transformers flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe8d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the missing local server engine\n",
    "%pip install \"pymilvus[milvus_lite]\"\n",
    "\n",
    "# CRITICAL: Restart your kernel again after this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71959f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pymilvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2bc651",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U pymilvus milvus-lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe2eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove the libraries causing the binary conflict\n",
    "# (These are optional speed-boosters for Pandas, not required for functionality)\n",
    "%pip uninstall -y bottleneck numexpr\n",
    "\n",
    "# 2. Force install a compatible version of Pandas and PyArrow\n",
    "# This ensures your Pandas matches your current NumPy version\n",
    "%pip install --upgrade pandas pyarrow numpy>=2.0\n",
    "\n",
    "# 3. CRITICAL: Restart your kernel now!\n",
    "# Click \"Kernel\" -> \"Restart Kernel\" in the menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58760a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Downgrade NumPy to the 1.x version (most compatible)\n",
    "%pip install \"numpy<2.0\"\n",
    "\n",
    "# 2. You MUST restart your kernel after this!\n",
    "# In VS Code/Jupyter: Click \"Restart\" or \"Restart Kernel\" in the top toolbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c5a2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-milvus langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e5d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fastapi uvicorn nest-asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865de2b5",
   "metadata": {},
   "source": [
    "# Simple scraping agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41be412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_classic.agents import AgentExecutor, create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c835fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connected to LLM running locally\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://127.0.0.1:1234/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    model=\"local-model\",\n",
    "    temperature=0,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Define the Tool\n",
    "@tool\n",
    "def fetch_csv_dataset(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads a CSV dataset from a URL and returns a summary.\n",
    "    Input should be the full URL string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse CSV\n",
    "        content = response.content.decode('utf-8')\n",
    "        df = pd.read_csv(StringIO(content), on_bad_lines='skip')\n",
    "        \n",
    "        return (\n",
    "            f\"SUCCESS: Downloaded data from {url}\\n\"\n",
    "            f\"Shape: {df.shape}\\n\"\n",
    "            f\"Columns: {list(df.columns)}\\n\"\n",
    "            f\"First 5 rows:\\n{df.head().to_string()}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "tools = [fetch_csv_dataset]\n",
    "\n",
    "# Define the ReAct Prompt (Hardcoded for stability)\n",
    "# This teaches the model explicitly how to think and act.\n",
    "template = '''Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}'''\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 4. Create the ReAct Agent\n",
    "# This uses simple text generation, avoiding the Pydantic/Tool Binding error completely.\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "\n",
    "# 5. Create the Executor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    verbose=True, \n",
    "    handle_parsing_errors=True # IMPORTANT for local models\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ReAct Agent built successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a0a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = \"https://raw.githubusercontent.com/gramener/datasets/refs/heads/main/card_transactions.csv\"\n",
    "query = f\"Download the dataset from {test_url} and tell me the columns.\"\n",
    "\n",
    "response = agent_executor.invoke({\"input\": query})\n",
    "print(\"\\n--- FINAL ANSWER ---\")\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a40c1ee",
   "metadata": {},
   "source": [
    "# Vector embedding of 2022-2024 news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d70f9a",
   "metadata": {},
   "source": [
    "### Load and filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0848cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"dataset/guardian_climate_news_corpus.csv\")\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "df = df[df['date'].dt.year >= 2022].copy()\n",
    "\n",
    "df = df[df['label'] != 'UNRELATED_TO_CLIMATE'].copy()\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94bd620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa4411",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bd94f4",
   "metadata": {},
   "source": [
    "### Making vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f402dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 1. SETUP: Load your data\n",
    "# ------------------------------------------------------------------\n",
    "# df = pd.read_csv(\"your_data.csv\") # Uncomment to load your real file\n",
    "# Ensure date is datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# 2. CREATE UNIFIED TEXT REPRESENTATION\n",
    "# ------------------------------------------------------------------\n",
    "# Instead of training a separate Word2Vec model, we will format the metadata \n",
    "# into a structured string that the 8B model can \"read\" and understand semantically.\n",
    "# This technique is often called \"Text Serialization\".\n",
    "\n",
    "def serialize_row_for_embedding(row):\n",
    "    # Parse tags safely\n",
    "    try:\n",
    "        tags = ast.literal_eval(row['tags']) if isinstance(row['tags'], str) else row['tags']\n",
    "        tags_str = \", \".join(tags)\n",
    "    except:\n",
    "        tags_str = \"None\"\n",
    "        \n",
    "    # Create a rich text block that describes the entire data point\n",
    "    # We put the most important semantic info (Category, Tags, Date) at the start or end.\n",
    "    combined_text = (\n",
    "        f\"Category: {row['category']}. \"\n",
    "        f\"Tags: {tags_str}. \"\n",
    "        f\"Date: {row['date'].strftime('%Y-%m-%d')}. \"\n",
    "        f\"Title: {row['title']}\\n\"\n",
    "        f\"Content: {row['body']}\"\n",
    "    )\n",
    "    return combined_text\n",
    "\n",
    "# Apply the function\n",
    "df['serialized_text'] = df.apply(serialize_row_for_embedding, axis=1)\n",
    "\n",
    "# 3. EMBED WITH LOCAL LLAMA MODEL (via OpenAI Compatible API)\n",
    "# ------------------------------------------------------------------\n",
    "# Assuming you are running the model in LM Studio / Ollama on port 1234\n",
    "# Check your local server settings for the exact URL.\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    base_url=\"http://127.0.0.1:1234/v1\", # Point to your local server\n",
    "    api_key=\"lm-studio\",                 # Arbitrary key\n",
    "    model=\"Qwen3-Embedding-4B-GGUF\",     # The specific model name loaded in your server\n",
    "    check_embedding_ctx_length=False     # Important for long texts\n",
    ")\n",
    "\n",
    "print(\"Starting embedding process... (This may take time depending on GPU)\")\n",
    "\n",
    "# We process in batches to be safe with memory\n",
    "batch_size = 32\n",
    "all_embeddings = []\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_texts = df['serialized_text'].iloc[i:i+batch_size].tolist()\n",
    "    \n",
    "    # Generate embeddings for the batch\n",
    "    # embed_documents returns a list of lists (vectors)\n",
    "    batch_embeddings = embedding_model.embed_documents(batch_texts)\n",
    "    all_embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    print(f\"Processed rows {i} to {min(i+batch_size, len(df))}\")\n",
    "\n",
    "# 4. STORE RESULTS\n",
    "# ------------------------------------------------------------------\n",
    "# Convert to numpy array for use in classifiers or Vector DB\n",
    "final_features = np.array(all_embeddings)\n",
    "\n",
    "print(f\"Final Feature Matrix Shape: {final_features.shape}\")\n",
    "\n",
    "# Optional: Add back to DataFrame\n",
    "df['embedding_vector'] = list(final_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd16ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Save the DataFrame (Contains text, metadata, and vectors)\n",
    "# Pickle is better than CSV because it preserves lists/arrays perfectly.\n",
    "df.to_pickle(\"climate_news_data.pkl\")\n",
    "\n",
    "# 2. Save the Raw Numpy Array (Just in case)\n",
    "# This is the safest way to store the pure mathematical vectors.\n",
    "np.save(\"climate_vectors.npy\", final_features)\n",
    "\n",
    "print(\"Saved 'climate_news_data.pkl' and 'climate_vectors.npy' to disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c086d9",
   "metadata": {},
   "source": [
    "### Storing embeddings with Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafcf5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"üöÄ Attempting to rescue data...\")\n",
    "\n",
    "# 1. Load the Pickle\n",
    "# Since we are on NumPy 2.x (installed above), this will read the file correctly.\n",
    "df = pd.read_pickle(\"climate_news_data.pkl\")\n",
    "print(f\"‚úÖ Pickle loaded successfully. Shape: {df.shape}\")\n",
    "\n",
    "# 2. Save as Parquet\n",
    "# We drop the vector column if it exists to keep the file light (we have the .npy file)\n",
    "if 'embedding_vector' in df.columns:\n",
    "    df = df.drop(columns=['embedding_vector'])\n",
    "\n",
    "df.to_parquet(\"climate_news_data.parquet\")\n",
    "print(\"‚úÖ SUCCESS: Data saved to 'climate_news_data.parquet'\")\n",
    "\n",
    "# 3. Verify Vector File\n",
    "# This usually loads fine regardless of version, but let's check.\n",
    "vectors = np.load(\"climate_vectors.npy\")\n",
    "print(f\"‚úÖ SUCCESS: Vectors verified. Shape: {vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc31116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymilvus import MilvusClient, DataType\n",
    "\n",
    "# 1. RELOAD YOUR SAVED DATA\n",
    "# ------------------------------------------------------------------\n",
    "print(\"üîÑ Reloading rescued data...\")\n",
    "df = pd.read_parquet(\"climate_news_data.parquet\")\n",
    "final_features = np.load(\"climate_vectors.npy\")\n",
    "print(f\"‚úÖ Data Loaded. Articles: {len(df)} | Vector Dim: {final_features.shape[1]}\")\n",
    "\n",
    "# 2. SETUP MILVUS LITE\n",
    "# ------------------------------------------------------------------\n",
    "client = MilvusClient(\"./climate_news.db\")\n",
    "COLLECTION_NAME = \"climate_articles\"\n",
    "\n",
    "# 3. DEFINE THE SCHEMA\n",
    "# ------------------------------------------------------------------\n",
    "if client.has_collection(COLLECTION_NAME):\n",
    "    client.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "schema = client.create_schema(auto_id=True, enable_dynamic_field=True)\n",
    "\n",
    "# Add Fields\n",
    "schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "schema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=2560) # Matches your Qwen size\n",
    "schema.add_field(field_name=\"category\", datatype=DataType.VARCHAR, max_length=512)\n",
    "schema.add_field(field_name=\"date\", datatype=DataType.VARCHAR, max_length=50)\n",
    "schema.add_field(field_name=\"text\", datatype=DataType.VARCHAR, max_length=65535)\n",
    "\n",
    "# 4. DEFINE INDEX\n",
    "# ------------------------------------------------------------------\n",
    "index_params = client.prepare_index_params()\n",
    "index_params.add_index(\n",
    "    field_name=\"vector\", \n",
    "    index_type=\"AUTOINDEX\", \n",
    "    metric_type=\"COSINE\"\n",
    ")\n",
    "\n",
    "# 5. CREATE COLLECTION\n",
    "# ------------------------------------------------------------------\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    schema=schema,\n",
    "    index_params=index_params\n",
    ")\n",
    "print(f\"‚úÖ Collection '{COLLECTION_NAME}' created.\")\n",
    "\n",
    "# 6. INSERT DATA\n",
    "# ------------------------------------------------------------------\n",
    "data_to_insert = []\n",
    "print(\"Preparing data for insertion...\")\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    vector_list = final_features[idx].tolist()\n",
    "    date_str = row['date'].strftime('%Y-%m-%d') if pd.notnull(row['date']) else \"\"\n",
    "    \n",
    "    entry = {\n",
    "        \"vector\": vector_list,\n",
    "        \"text\": str(row['body']),\n",
    "        \"title\": str(row['title']),\n",
    "        \"category\": str(row['category']),\n",
    "        \"date\": date_str,\n",
    "        \"tags\": str(row['tags'])\n",
    "    }\n",
    "    data_to_insert.append(entry)\n",
    "\n",
    "# Insert in batches\n",
    "batch_size = 100\n",
    "total_inserted = 0\n",
    "\n",
    "for i in range(0, len(data_to_insert), batch_size):\n",
    "    batch = data_to_insert[i:i+batch_size]\n",
    "    res = client.insert(collection_name=COLLECTION_NAME, data=batch)\n",
    "    total_inserted += res['insert_count']\n",
    "    print(f\"Inserted batch {i} to {i+len(batch)}...\")\n",
    "\n",
    "print(f\"‚úÖ SUCCESS! Stored {total_inserted} articles in 'climate_news.db'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4317b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymilvus import MilvusClient, DataType\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# 0. CLEANUP OLD CORRUPTED DB (Optional but recommended)\n",
    "# ------------------------------------------------------------------\n",
    "db_path = \"./climate_news.db\"\n",
    "if os.path.exists(db_path):\n",
    "    print(f\"‚ö†Ô∏è Found existing database at {db_path}. Removing it...\")\n",
    "    try:\n",
    "        if os.path.isdir(db_path):\n",
    "            shutil.rmtree(db_path)\n",
    "        else:\n",
    "            os.remove(db_path)\n",
    "        print(\"‚úÖ Old database removed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not remove old db: {e}\")\n",
    "\n",
    "# 1. RELOAD YOUR SAVED DATA\n",
    "# ------------------------------------------------------------------\n",
    "print(\"üîÑ Reloading rescued data...\")\n",
    "df = pd.read_parquet(\"climate_news_data.parquet\")\n",
    "final_features = np.load(\"climate_vectors.npy\")\n",
    "print(f\"‚úÖ Data Loaded. Articles: {len(df)} | Vector Dim: {final_features.shape[1]}\")\n",
    "\n",
    "# 2. SETUP MILVUS LITE (Fresh instance)\n",
    "# ------------------------------------------------------------------\n",
    "# Create a fresh client pointing to a clean database file\n",
    "client = MilvusClient(uri=db_path)\n",
    "COLLECTION_NAME = \"climate_articles\"\n",
    "\n",
    "# 3. DEFINE THE SCHEMA\n",
    "# ------------------------------------------------------------------\n",
    "if client.has_collection(COLLECTION_NAME):\n",
    "    client.drop_collection(COLLECTION_NAME)\n",
    "    print(f\"Dropped existing collection '{COLLECTION_NAME}'\")\n",
    "\n",
    "schema = client.create_schema(auto_id=True, enable_dynamic_field=True)\n",
    "\n",
    "# Add Fields\n",
    "schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "schema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=2560) # Matches your Qwen size\n",
    "schema.add_field(field_name=\"category\", datatype=DataType.VARCHAR, max_length=512)\n",
    "schema.add_field(field_name=\"date\", datatype=DataType.VARCHAR, max_length=50)\n",
    "schema.add_field(field_name=\"text\", datatype=DataType.VARCHAR, max_length=65535)\n",
    "\n",
    "# 4. DEFINE INDEX\n",
    "# ------------------------------------------------------------------\n",
    "index_params = client.prepare_index_params()\n",
    "index_params.add_index(\n",
    "    field_name=\"vector\", \n",
    "    index_type=\"AUTOINDEX\", \n",
    "    metric_type=\"COSINE\"\n",
    ")\n",
    "\n",
    "# 5. CREATE COLLECTION\n",
    "# ------------------------------------------------------------------\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    schema=schema,\n",
    "    index_params=index_params\n",
    ")\n",
    "print(f\"‚úÖ Collection '{COLLECTION_NAME}' created.\")\n",
    "\n",
    "# 6. INSERT DATA\n",
    "# ------------------------------------------------------------------\n",
    "data_to_insert = []\n",
    "print(\"Preparing data for insertion...\")\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    vector_list = final_features[idx].tolist()\n",
    "    date_str = row['date'].strftime('%Y-%m-%d') if pd.notnull(row['date']) else \"\"\n",
    "    \n",
    "    entry = {\n",
    "        \"vector\": vector_list,\n",
    "        \"text\": str(row['body']),\n",
    "        \"title\": str(row['title']),\n",
    "        \"category\": str(row['category']),\n",
    "        \"date\": date_str,\n",
    "        \"tags\": str(row['tags'])\n",
    "    }\n",
    "    data_to_insert.append(entry)\n",
    "\n",
    "# Insert in batches\n",
    "batch_size = 100\n",
    "total_inserted = 0\n",
    "\n",
    "for i in range(0, len(data_to_insert), batch_size):\n",
    "    batch = data_to_insert[i:i+batch_size]\n",
    "    res = client.insert(collection_name=COLLECTION_NAME, data=batch)\n",
    "    total_inserted += res['insert_count']\n",
    "    print(f\"Inserted batch {i} to {i+len(batch)}...\")\n",
    "\n",
    "print(f\"‚úÖ SUCCESS! Stored {total_inserted} articles in '{db_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d50db5",
   "metadata": {},
   "source": [
    "# RAG Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "client = MilvusClient(uri=\"./climate_news.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbde41ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG Chain initialized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. SETUP MODELS\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    base_url=\"http://127.0.0.1:1234/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    model=\"Qwen3-Embedding-4B-GGUF\",\n",
    "    check_embedding_ctx_length=False\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://127.0.0.1:8000/v1\", # Verify this is your Ministral server port\n",
    "    api_key=\"local-key\",\n",
    "    model=\"mistralai/Ministral-3-14B-Reasoning-2512\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# 2. CONNECT TO VECTOR STORE\n",
    "# Note: Ensure climate_news.db is NOT open in any other software\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"uri\": \"./climate_news.db\"},\n",
    "    collection_name=\"climate_articles\",\n",
    "    text_field=\"text\",\n",
    "    auto_id=True\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# 3. DEFINE RAG LOGIC\n",
    "template = \"\"\"You are a specialized Climate News Assistant.\n",
    "Use the context below to answer. If unsure, say you can't find it.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(\"‚úÖ RAG Chain initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ac5b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the system\n",
    "query = \"Tell me any news about tiny snails on Atlantic Island\"\n",
    "response = rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5980d990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: Tell me any news about tiny snails on Atlantic Island\n",
      "\n",
      "ü§ñ Agent Answer:\n",
      "More than 1,300 tiny, critically endangered snails have been set free to roam on an island off the coast of Morocco after a breeding programme rescued two obscure species from the brink of extinction. The Desertas Island land snails had not been recorded for more than 100 years and were believed to have disappeared from their natural habitat on the windswept, mountainous island of Deserta Grande, close to Portugal-owned Madeira. Experts at the Instituto das Florestas e Conserva√ß√£o da Natureza (IFCN) rediscovered minute populations of two species of the snail, each consisting of fewer than 200 survivors, in conservation expeditions between 2012 and 2017 amid fears that invasive predators might have eaten the pea-sized molluscs into oblivion. The snails were taken to zoos in the UK and France, with 60 flown to Chester zoo, where the conservation science team liaised with experts in Madeira and constructed new homes for them in mini habitat tanks as part of a breeding programme to quickly boost numbers. Dr Gerardo Garcia, Chester zoo‚Äôs head of ectotherms, said the future of the species ‚Äúwas in our hands‚Äù when the snails first arrived. ‚ÄúIt was a huge responsibility to begin caring for them,‚Äù he said. ‚ÄúAs a zoo conservation community, we knew nothing about them. They‚Äôd never been in human care before and we had to start from a blank piece of a paper and try to figure out what makes them tick.‚Äù Garcia said the snails ‚Äúreally were on the edge of extinction‚Äù and he paid tribute to his team of zookeepers who ‚Äúspent countless hours caring for every individual snail‚Äù. The snails have been released into a wild refuge on Bugio, a smaller neighbouring island in the Madeira archipelago that has been off-limits to humans since 1990 to protect its fragile ecosystem, where invasive species like rats, mice and goats have been eradicated. Heather Prince, an invertebrate specialist at Chester zoo, said: ‚ÄúWithin a few months we were able to crack the breeding of the Desertas land snails. Crucially, we were then successful in breeding multiple generations. This was key, because it meant we could then bring in the support of other zoos and establish a network, breeding them in the substantial numbers needed to have a chance of saving the species.‚Äù Each of the snails reintroduced has been individually marked for monitoring. If successful, many more will be released to bolster numbers. Dinarte Teixeira, a conservation biologist at IFCN, said: ‚ÄúThese snails are incredibly precious. The Desertas Islands are the only place in the world where they can be found and so we‚Äôre striving to do everything we can to give them the best possible chance for the future. For 100 years we thought they‚Äôd gone for ever, but now there‚Äôs new hope.‚Äù\n"
     ]
    }
   ],
   "source": [
    "print(f\"‚ùì Question: {query}\\n\")\n",
    "print(\"ü§ñ Agent Answer:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38babca8",
   "metadata": {},
   "source": [
    "# Embedding PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d9c173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
