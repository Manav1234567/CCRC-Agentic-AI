{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3c403f",
   "metadata": {},
   "source": [
    "# Pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a35a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Force upgrade the critical libraries\n",
    "%pip install -U langchain langchain-core langchain-openai langchain-community pydantic\n",
    "\n",
    "# 2. IMPORTANT: You must restart the kernel after running this!\n",
    "# In VS Code/Jupyter: Click \"Restart\" or \"Restart Kernel\" in the top toolbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentence-transformers gensim datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f93c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --force-reinstall datasets sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a0c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --force-reinstall gensim numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f62fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U sentence-transformers transformers flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cfe8d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pymilvus[milvus_lite] in /opt/anaconda3/lib/python3.12/site-packages (2.6.5)\n",
      "Requirement already satisfied: setuptools>69 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus[milvus_lite]) (80.9.0)\n",
      "Requirement already satisfied: grpcio!=1.68.0,!=1.68.1,!=1.69.0,!=1.70.0,!=1.70.1,!=1.71.0,!=1.72.1,!=1.73.0,>=1.66.2 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus[milvus_lite]) (1.76.0)\n",
      "Requirement already satisfied: orjson>=3.10.15 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus[milvus_lite]) (3.11.5)\n",
      "Requirement already satisfied: protobuf>=5.27.2 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus[milvus_lite]) (6.33.2)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus[milvus_lite]) (1.2.1)\n",
      "Requirement already satisfied: pandas>=1.2.4 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus[milvus_lite]) (2.3.3)\n",
      "Collecting milvus-lite>=2.4.0 (from pymilvus[milvus_lite])\n",
      "Collecting milvus-lite>=2.4.0 (from pymilvus[milvus_lite])\n",
      "  Downloading milvus_lite-2.5.1-py3-none-macosx_11_0_arm64.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /opt/anaconda3/lib/python3.12/site-packages (from grpcio!=1.68.0,!=1.68.1,!=1.69.0,!=1.70.0,!=1.70.1,!=1.71.0,!=1.72.1,!=1.73.0,>=1.66.2->pymilvus[milvus_lite]) (4.15.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from milvus-lite>=2.4.0->pymilvus[milvus_lite]) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus[milvus_lite]) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus[milvus_lite]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus[milvus_lite]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus[milvus_lite]) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus[milvus_lite]) (1.17.0)\n",
      "Downloading milvus_lite-2.5.1-py3-none-macosx_11_0_arm64.whl (24.4 MB)\n",
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/24.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m  Downloading milvus_lite-2.5.1-py3-none-macosx_11_0_arm64.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /opt/anaconda3/lib/python3.12/site-packages (from grpcio!=1.68.0,!=1.68.1,!=1.69.0,!=1.70.0,!=1.70.1,!=1.71.0,!=1.72.1,!=1.73.0,>=1.66.2->pymilvus[milvus_lite]) (4.15.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from milvus-lite>=2.4.0->pymilvus[milvus_lite]) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus[milvus_lite]) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus[milvus_lite]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus[milvus_lite]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus[milvus_lite]) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus[milvus_lite]) (1.17.0)\n",
      "Downloading milvus_lite-2.5.1-py3-none-macosx_11_0_arm64.whl (24.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.4/24.4 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.4/24.4 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: milvus-lite\n",
      "Installing collected packages: milvus-lite\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed milvus-lite-2.5.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed milvus-lite-2.5.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the missing local server engine\n",
    "%pip install \"pymilvus[milvus_lite]\"\n",
    "\n",
    "# CRITICAL: Restart your kernel again after this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71959f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pymilvus in /opt/anaconda3/lib/python3.12/site-packages (2.6.5)\n",
      "Requirement already satisfied: setuptools>69 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (80.9.0)\n",
      "Requirement already satisfied: grpcio!=1.68.0,!=1.68.1,!=1.69.0,!=1.70.0,!=1.70.1,!=1.71.0,!=1.72.1,!=1.73.0,>=1.66.2 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (1.76.0)\n",
      "Requirement already satisfied: orjson>=3.10.15 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (3.11.5)\n",
      "Requirement already satisfied: protobuf>=5.27.2 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (6.33.2)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (1.2.1)\n",
      "Requirement already satisfied: pandas>=1.2.4 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (2.3.3)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /opt/anaconda3/lib/python3.12/site-packages (from grpcio!=1.68.0,!=1.68.1,!=1.69.0,!=1.70.0,!=1.70.1,!=1.71.0,!=1.72.1,!=1.73.0,>=1.66.2->pymilvus) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.17.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.17.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pymilvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fe2eea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: Bottleneck 1.3.7\n",
      "Uninstalling Bottleneck-1.3.7:\n",
      "  Successfully uninstalled Bottleneck-1.3.7\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: numexpr 2.8.7\n",
      "Uninstalling numexpr-2.8.7:\n",
      "  Successfully uninstalled numexpr-2.8.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "zsh:1: 2.0 not found\n",
      "zsh:1: 2.0 not found\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 1. Remove the libraries causing the binary conflict\n",
    "# (These are optional speed-boosters for Pandas, not required for functionality)\n",
    "%pip uninstall -y bottleneck numexpr\n",
    "\n",
    "# 2. Force install a compatible version of Pandas and PyArrow\n",
    "# This ensures your Pandas matches your current NumPy version\n",
    "%pip install --upgrade pandas pyarrow numpy>=2.0\n",
    "\n",
    "# 3. CRITICAL: Restart your kernel now!\n",
    "# Click \"Kernel\" -> \"Restart Kernel\" in the menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58760a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting numpy<2.0\n",
      "Collecting numpy<2.0\n",
      "  Downloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/13.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m  Downloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: numpy 2.3.5\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: numpy 2.3.5\n",
      "    Uninstalling numpy-2.3.5:\n",
      "      Successfully uninstalled numpy-2.3.5\n",
      "    Uninstalling numpy-2.3.5:\n",
      "      Successfully uninstalled numpy-2.3.5\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.37.1 requires pyarrow>=7.0, which is not installed.\n",
      "datasets 4.4.1 requires pyarrow>=21.0.0, which is not installed.\n",
      "dask-expr 1.1.13 requires pyarrow>=14.0.1, which is not installed.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires protobuf<6,>=3.20, but you have protobuf 6.33.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.37.1 requires pyarrow>=7.0, which is not installed.\n",
      "datasets 4.4.1 requires pyarrow>=21.0.0, which is not installed.\n",
      "dask-expr 1.1.13 requires pyarrow>=14.0.1, which is not installed.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires protobuf<6,>=3.20, but you have protobuf 6.33.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 1. Downgrade NumPy to the 1.x version (most compatible)\n",
    "%pip install \"numpy<2.0\"\n",
    "\n",
    "# 2. You MUST restart your kernel after this!\n",
    "# In VS Code/Jupyter: Click \"Restart\" or \"Restart Kernel\" in the top toolbar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865de2b5",
   "metadata": {},
   "source": [
    "# Simple scraping agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41be412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_classic.agents import AgentExecutor, create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c835fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connected to LLM running locally\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://127.0.0.1:1234/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    model=\"local-model\",\n",
    "    temperature=0,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Define the Tool\n",
    "@tool\n",
    "def fetch_csv_dataset(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads a CSV dataset from a URL and returns a summary.\n",
    "    Input should be the full URL string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse CSV\n",
    "        content = response.content.decode('utf-8')\n",
    "        df = pd.read_csv(StringIO(content), on_bad_lines='skip')\n",
    "        \n",
    "        return (\n",
    "            f\"SUCCESS: Downloaded data from {url}\\n\"\n",
    "            f\"Shape: {df.shape}\\n\"\n",
    "            f\"Columns: {list(df.columns)}\\n\"\n",
    "            f\"First 5 rows:\\n{df.head().to_string()}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "tools = [fetch_csv_dataset]\n",
    "\n",
    "# Define the ReAct Prompt (Hardcoded for stability)\n",
    "# This teaches the model explicitly how to think and act.\n",
    "template = '''Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}'''\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 4. Create the ReAct Agent\n",
    "# This uses simple text generation, avoiding the Pydantic/Tool Binding error completely.\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "\n",
    "# 5. Create the Executor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    verbose=True, \n",
    "    handle_parsing_errors=True # IMPORTANT for local models\n",
    ")\n",
    "\n",
    "print(\"âœ… ReAct Agent built successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a0a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = \"https://raw.githubusercontent.com/gramener/datasets/refs/heads/main/card_transactions.csv\"\n",
    "query = f\"Download the dataset from {test_url} and tell me the columns.\"\n",
    "\n",
    "response = agent_executor.invoke({\"input\": query})\n",
    "print(\"\\n--- FINAL ANSWER ---\")\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a40c1ee",
   "metadata": {},
   "source": [
    "# Vector embedding of 2022-2024 news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d70f9a",
   "metadata": {},
   "source": [
    "### Load and filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0848cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"dataset/guardian_climate_news_corpus.csv\")\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "df = df[df['date'].dt.year >= 2022].copy()\n",
    "\n",
    "df = df[df['label'] != 'UNRELATED_TO_CLIMATE'].copy()\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94bd620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa4411",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bd94f4",
   "metadata": {},
   "source": [
    "### Making vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f402dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 1. SETUP: Load your data\n",
    "# ------------------------------------------------------------------\n",
    "# df = pd.read_csv(\"your_data.csv\") # Uncomment to load your real file\n",
    "# Ensure date is datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# 2. CREATE UNIFIED TEXT REPRESENTATION\n",
    "# ------------------------------------------------------------------\n",
    "# Instead of training a separate Word2Vec model, we will format the metadata \n",
    "# into a structured string that the 8B model can \"read\" and understand semantically.\n",
    "# This technique is often called \"Text Serialization\".\n",
    "\n",
    "def serialize_row_for_embedding(row):\n",
    "    # Parse tags safely\n",
    "    try:\n",
    "        tags = ast.literal_eval(row['tags']) if isinstance(row['tags'], str) else row['tags']\n",
    "        tags_str = \", \".join(tags)\n",
    "    except:\n",
    "        tags_str = \"None\"\n",
    "        \n",
    "    # Create a rich text block that describes the entire data point\n",
    "    # We put the most important semantic info (Category, Tags, Date) at the start or end.\n",
    "    combined_text = (\n",
    "        f\"Category: {row['category']}. \"\n",
    "        f\"Tags: {tags_str}. \"\n",
    "        f\"Date: {row['date'].strftime('%Y-%m-%d')}. \"\n",
    "        f\"Title: {row['title']}\\n\"\n",
    "        f\"Content: {row['body']}\"\n",
    "    )\n",
    "    return combined_text\n",
    "\n",
    "# Apply the function\n",
    "df['serialized_text'] = df.apply(serialize_row_for_embedding, axis=1)\n",
    "\n",
    "# 3. EMBED WITH LOCAL LLAMA MODEL (via OpenAI Compatible API)\n",
    "# ------------------------------------------------------------------\n",
    "# Assuming you are running the model in LM Studio / Ollama on port 1234\n",
    "# Check your local server settings for the exact URL.\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    base_url=\"http://127.0.0.1:1234/v1\", # Point to your local server\n",
    "    api_key=\"lm-studio\",                 # Arbitrary key\n",
    "    model=\"Qwen3-Embedding-4B-GGUF\",     # The specific model name loaded in your server\n",
    "    check_embedding_ctx_length=False     # Important for long texts\n",
    ")\n",
    "\n",
    "print(\"Starting embedding process... (This may take time depending on GPU)\")\n",
    "\n",
    "# We process in batches to be safe with memory\n",
    "batch_size = 32\n",
    "all_embeddings = []\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_texts = df['serialized_text'].iloc[i:i+batch_size].tolist()\n",
    "    \n",
    "    # Generate embeddings for the batch\n",
    "    # embed_documents returns a list of lists (vectors)\n",
    "    batch_embeddings = embedding_model.embed_documents(batch_texts)\n",
    "    all_embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    print(f\"Processed rows {i} to {min(i+batch_size, len(df))}\")\n",
    "\n",
    "# 4. STORE RESULTS\n",
    "# ------------------------------------------------------------------\n",
    "# Convert to numpy array for use in classifiers or Vector DB\n",
    "final_features = np.array(all_embeddings)\n",
    "\n",
    "print(f\"Final Feature Matrix Shape: {final_features.shape}\")\n",
    "\n",
    "# Optional: Add back to DataFrame\n",
    "df['embedding_vector'] = list(final_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd16ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Save the DataFrame (Contains text, metadata, and vectors)\n",
    "# Pickle is better than CSV because it preserves lists/arrays perfectly.\n",
    "df.to_pickle(\"climate_news_data.pkl\")\n",
    "\n",
    "# 2. Save the Raw Numpy Array (Just in case)\n",
    "# This is the safest way to store the pure mathematical vectors.\n",
    "np.save(\"climate_vectors.npy\", final_features)\n",
    "\n",
    "print(\"Saved 'climate_news_data.pkl' and 'climate_vectors.npy' to disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c086d9",
   "metadata": {},
   "source": [
    "### Storing embeddings with Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fafcf5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Attempting to rescue data...\n",
      "âœ… Pickle loaded successfully. Shape: (7762, 11)\n",
      "âœ… SUCCESS: Data saved to 'climate_news_data.parquet'\n",
      "âœ… SUCCESS: Vectors verified. Shape: (7762, 2560)\n",
      "âœ… SUCCESS: Data saved to 'climate_news_data.parquet'\n",
      "âœ… SUCCESS: Vectors verified. Shape: (7762, 2560)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"ðŸš€ Attempting to rescue data...\")\n",
    "\n",
    "# 1. Load the Pickle\n",
    "# Since we are on NumPy 2.x (installed above), this will read the file correctly.\n",
    "df = pd.read_pickle(\"climate_news_data.pkl\")\n",
    "print(f\"âœ… Pickle loaded successfully. Shape: {df.shape}\")\n",
    "\n",
    "# 2. Save as Parquet\n",
    "# We drop the vector column if it exists to keep the file light (we have the .npy file)\n",
    "if 'embedding_vector' in df.columns:\n",
    "    df = df.drop(columns=['embedding_vector'])\n",
    "\n",
    "df.to_parquet(\"climate_news_data.parquet\")\n",
    "print(\"âœ… SUCCESS: Data saved to 'climate_news_data.parquet'\")\n",
    "\n",
    "# 3. Verify Vector File\n",
    "# This usually loads fine regardless of version, but let's check.\n",
    "vectors = np.load(\"climate_vectors.npy\")\n",
    "print(f\"âœ… SUCCESS: Vectors verified. Shape: {vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fc31116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Reloading rescued data...\n",
      "âœ… Data Loaded. Articles: 7762 | Vector Dim: 2560\n",
      "âœ… Data Loaded. Articles: 7762 | Vector Dim: 2560\n",
      "âœ… Collection 'climate_articles' created.\n",
      "Preparing data for insertion...\n",
      "âœ… Collection 'climate_articles' created.\n",
      "Preparing data for insertion...\n",
      "Inserted batch 0 to 100...\n",
      "Inserted batch 100 to 200...\n",
      "Inserted batch 200 to 300...\n",
      "Inserted batch 300 to 400...\n",
      "Inserted batch 400 to 500...\n",
      "Inserted batch 500 to 600...\n",
      "Inserted batch 600 to 700...\n",
      "Inserted batch 0 to 100...\n",
      "Inserted batch 100 to 200...\n",
      "Inserted batch 200 to 300...\n",
      "Inserted batch 300 to 400...\n",
      "Inserted batch 400 to 500...\n",
      "Inserted batch 500 to 600...\n",
      "Inserted batch 600 to 700...\n",
      "Inserted batch 700 to 800...\n",
      "Inserted batch 800 to 900...\n",
      "Inserted batch 900 to 1000...\n",
      "Inserted batch 1000 to 1100...\n",
      "Inserted batch 1100 to 1200...\n",
      "Inserted batch 1200 to 1300...\n",
      "Inserted batch 1300 to 1400...\n",
      "Inserted batch 700 to 800...\n",
      "Inserted batch 800 to 900...\n",
      "Inserted batch 900 to 1000...\n",
      "Inserted batch 1000 to 1100...\n",
      "Inserted batch 1100 to 1200...\n",
      "Inserted batch 1200 to 1300...\n",
      "Inserted batch 1300 to 1400...\n",
      "Inserted batch 1400 to 1500...\n",
      "Inserted batch 1500 to 1600...\n",
      "Inserted batch 1600 to 1700...\n",
      "Inserted batch 1700 to 1800...\n",
      "Inserted batch 1800 to 1900...\n",
      "Inserted batch 1900 to 2000...\n",
      "Inserted batch 2000 to 2100...\n",
      "Inserted batch 1400 to 1500...\n",
      "Inserted batch 1500 to 1600...\n",
      "Inserted batch 1600 to 1700...\n",
      "Inserted batch 1700 to 1800...\n",
      "Inserted batch 1800 to 1900...\n",
      "Inserted batch 1900 to 2000...\n",
      "Inserted batch 2000 to 2100...\n",
      "Inserted batch 2100 to 2200...\n",
      "Inserted batch 2200 to 2300...\n",
      "Inserted batch 2300 to 2400...\n",
      "Inserted batch 2400 to 2500...\n",
      "Inserted batch 2500 to 2600...\n",
      "Inserted batch 2600 to 2700...\n",
      "Inserted batch 2700 to 2800...\n",
      "Inserted batch 2100 to 2200...\n",
      "Inserted batch 2200 to 2300...\n",
      "Inserted batch 2300 to 2400...\n",
      "Inserted batch 2400 to 2500...\n",
      "Inserted batch 2500 to 2600...\n",
      "Inserted batch 2600 to 2700...\n",
      "Inserted batch 2700 to 2800...\n",
      "Inserted batch 2800 to 2900...\n",
      "Inserted batch 2900 to 3000...\n",
      "Inserted batch 3000 to 3100...\n",
      "Inserted batch 3100 to 3200...\n",
      "Inserted batch 3200 to 3300...\n",
      "Inserted batch 3300 to 3400...\n",
      "Inserted batch 3400 to 3500...\n",
      "Inserted batch 2800 to 2900...\n",
      "Inserted batch 2900 to 3000...\n",
      "Inserted batch 3000 to 3100...\n",
      "Inserted batch 3100 to 3200...\n",
      "Inserted batch 3200 to 3300...\n",
      "Inserted batch 3300 to 3400...\n",
      "Inserted batch 3400 to 3500...\n",
      "Inserted batch 3500 to 3600...\n",
      "Inserted batch 3600 to 3700...\n",
      "Inserted batch 3700 to 3800...\n",
      "Inserted batch 3800 to 3900...\n",
      "Inserted batch 3900 to 4000...\n",
      "Inserted batch 4000 to 4100...\n",
      "Inserted batch 4100 to 4200...\n",
      "Inserted batch 3500 to 3600...\n",
      "Inserted batch 3600 to 3700...\n",
      "Inserted batch 3700 to 3800...\n",
      "Inserted batch 3800 to 3900...\n",
      "Inserted batch 3900 to 4000...\n",
      "Inserted batch 4000 to 4100...\n",
      "Inserted batch 4100 to 4200...\n",
      "Inserted batch 4200 to 4300...\n",
      "Inserted batch 4300 to 4400...\n",
      "Inserted batch 4400 to 4500...\n",
      "Inserted batch 4500 to 4600...\n",
      "Inserted batch 4600 to 4700...\n",
      "Inserted batch 4700 to 4800...\n",
      "Inserted batch 4800 to 4900...\n",
      "Inserted batch 4200 to 4300...\n",
      "Inserted batch 4300 to 4400...\n",
      "Inserted batch 4400 to 4500...\n",
      "Inserted batch 4500 to 4600...\n",
      "Inserted batch 4600 to 4700...\n",
      "Inserted batch 4700 to 4800...\n",
      "Inserted batch 4800 to 4900...\n",
      "Inserted batch 4900 to 5000...\n",
      "Inserted batch 5000 to 5100...\n",
      "Inserted batch 5100 to 5200...\n",
      "Inserted batch 5200 to 5300...\n",
      "Inserted batch 5300 to 5400...\n",
      "Inserted batch 5400 to 5500...\n",
      "Inserted batch 5500 to 5600...\n",
      "Inserted batch 4900 to 5000...\n",
      "Inserted batch 5000 to 5100...\n",
      "Inserted batch 5100 to 5200...\n",
      "Inserted batch 5200 to 5300...\n",
      "Inserted batch 5300 to 5400...\n",
      "Inserted batch 5400 to 5500...\n",
      "Inserted batch 5500 to 5600...\n",
      "Inserted batch 5600 to 5700...\n",
      "Inserted batch 5700 to 5800...\n",
      "Inserted batch 5800 to 5900...\n",
      "Inserted batch 5900 to 6000...\n",
      "Inserted batch 6000 to 6100...\n",
      "Inserted batch 6100 to 6200...\n",
      "Inserted batch 6200 to 6300...\n",
      "Inserted batch 5600 to 5700...\n",
      "Inserted batch 5700 to 5800...\n",
      "Inserted batch 5800 to 5900...\n",
      "Inserted batch 5900 to 6000...\n",
      "Inserted batch 6000 to 6100...\n",
      "Inserted batch 6100 to 6200...\n",
      "Inserted batch 6200 to 6300...\n",
      "Inserted batch 6300 to 6400...\n",
      "Inserted batch 6400 to 6500...\n",
      "Inserted batch 6500 to 6600...\n",
      "Inserted batch 6600 to 6700...\n",
      "Inserted batch 6700 to 6800...\n",
      "Inserted batch 6800 to 6900...\n",
      "Inserted batch 6900 to 7000...\n",
      "Inserted batch 6300 to 6400...\n",
      "Inserted batch 6400 to 6500...\n",
      "Inserted batch 6500 to 6600...\n",
      "Inserted batch 6600 to 6700...\n",
      "Inserted batch 6700 to 6800...\n",
      "Inserted batch 6800 to 6900...\n",
      "Inserted batch 6900 to 7000...\n",
      "Inserted batch 7000 to 7100...\n",
      "Inserted batch 7100 to 7200...\n",
      "Inserted batch 7200 to 7300...\n",
      "Inserted batch 7300 to 7400...\n",
      "Inserted batch 7400 to 7500...\n",
      "Inserted batch 7500 to 7600...\n",
      "Inserted batch 7600 to 7700...\n",
      "Inserted batch 7700 to 7762...\n",
      "âœ… SUCCESS! Stored 7762 articles in 'climate_news.db'\n",
      "Inserted batch 7000 to 7100...\n",
      "Inserted batch 7100 to 7200...\n",
      "Inserted batch 7200 to 7300...\n",
      "Inserted batch 7300 to 7400...\n",
      "Inserted batch 7400 to 7500...\n",
      "Inserted batch 7500 to 7600...\n",
      "Inserted batch 7600 to 7700...\n",
      "Inserted batch 7700 to 7762...\n",
      "âœ… SUCCESS! Stored 7762 articles in 'climate_news.db'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymilvus import MilvusClient, DataType\n",
    "\n",
    "# 1. RELOAD YOUR SAVED DATA\n",
    "# ------------------------------------------------------------------\n",
    "print(\"ðŸ”„ Reloading rescued data...\")\n",
    "df = pd.read_parquet(\"climate_news_data.parquet\")\n",
    "final_features = np.load(\"climate_vectors.npy\")\n",
    "print(f\"âœ… Data Loaded. Articles: {len(df)} | Vector Dim: {final_features.shape[1]}\")\n",
    "\n",
    "# 2. SETUP MILVUS LITE\n",
    "# ------------------------------------------------------------------\n",
    "client = MilvusClient(\"./climate_news.db\")\n",
    "COLLECTION_NAME = \"climate_articles\"\n",
    "\n",
    "# 3. DEFINE THE SCHEMA\n",
    "# ------------------------------------------------------------------\n",
    "if client.has_collection(COLLECTION_NAME):\n",
    "    client.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "schema = client.create_schema(auto_id=True, enable_dynamic_field=True)\n",
    "\n",
    "# Add Fields\n",
    "schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "schema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=2560) # Matches your Qwen size\n",
    "schema.add_field(field_name=\"category\", datatype=DataType.VARCHAR, max_length=512)\n",
    "schema.add_field(field_name=\"date\", datatype=DataType.VARCHAR, max_length=50)\n",
    "schema.add_field(field_name=\"text\", datatype=DataType.VARCHAR, max_length=65535)\n",
    "\n",
    "# 4. DEFINE INDEX\n",
    "# ------------------------------------------------------------------\n",
    "index_params = client.prepare_index_params()\n",
    "index_params.add_index(\n",
    "    field_name=\"vector\", \n",
    "    index_type=\"AUTOINDEX\", \n",
    "    metric_type=\"COSINE\"\n",
    ")\n",
    "\n",
    "# 5. CREATE COLLECTION\n",
    "# ------------------------------------------------------------------\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    schema=schema,\n",
    "    index_params=index_params\n",
    ")\n",
    "print(f\"âœ… Collection '{COLLECTION_NAME}' created.\")\n",
    "\n",
    "# 6. INSERT DATA\n",
    "# ------------------------------------------------------------------\n",
    "data_to_insert = []\n",
    "print(\"Preparing data for insertion...\")\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    vector_list = final_features[idx].tolist()\n",
    "    date_str = row['date'].strftime('%Y-%m-%d') if pd.notnull(row['date']) else \"\"\n",
    "    \n",
    "    entry = {\n",
    "        \"vector\": vector_list,\n",
    "        \"text\": str(row['body']),\n",
    "        \"title\": str(row['title']),\n",
    "        \"category\": str(row['category']),\n",
    "        \"date\": date_str,\n",
    "        \"tags\": str(row['tags'])\n",
    "    }\n",
    "    data_to_insert.append(entry)\n",
    "\n",
    "# Insert in batches\n",
    "batch_size = 100\n",
    "total_inserted = 0\n",
    "\n",
    "for i in range(0, len(data_to_insert), batch_size):\n",
    "    batch = data_to_insert[i:i+batch_size]\n",
    "    res = client.insert(collection_name=COLLECTION_NAME, data=batch)\n",
    "    total_inserted += res['insert_count']\n",
    "    print(f\"Inserted batch {i} to {i+len(batch)}...\")\n",
    "\n",
    "print(f\"âœ… SUCCESS! Stored {total_inserted} articles in 'climate_news.db'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
