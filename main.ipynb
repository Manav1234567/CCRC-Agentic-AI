{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3c403f",
   "metadata": {},
   "source": [
    "# Pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c7a35a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1767004252.089963 11876611 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langchain in /opt/anaconda3/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: langchain in /opt/anaconda3/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: langchain-core in /opt/anaconda3/lib/python3.12/site-packages (1.2.5)\n",
      "Requirement already satisfied: langchain-core in /opt/anaconda3/lib/python3.12/site-packages (1.2.5)\n",
      "Requirement already satisfied: langchain-openai in /opt/anaconda3/lib/python3.12/site-packages (1.1.3)\n",
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-1.1.6-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain-community in /opt/anaconda3/lib/python3.12/site-packages (0.4.1)\n",
      "Requirement already satisfied: langchain-openai in /opt/anaconda3/lib/python3.12/site-packages (1.1.3)\n",
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-1.1.6-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain-community in /opt/anaconda3/lib/python3.12/site-packages (0.4.1)\n",
      "Requirement already satisfied: pydantic in /opt/anaconda3/lib/python3.12/site-packages (2.12.5)\n",
      "Requirement already satisfied: pydantic in /opt/anaconda3/lib/python3.12/site-packages (2.12.5)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (1.0.4)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (0.4.59)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (0.12.0)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-openai) (2.11.0)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.0.45)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (3.13.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.2.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic) (0.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (2.1)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (1.0.4)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (0.4.59)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (0.12.0)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-openai) (2.11.0)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.0.45)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (3.13.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.2.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic) (0.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (2.1)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.15)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.12.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.15)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.12.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.0.0)\n",
      "Using cached langchain_openai-1.1.6-py3-none-any.whl (84 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.0.0)\n",
      "Using cached langchain_openai-1.1.6-py3-none-any.whl (84 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: langchain-openai\n",
      "  Attempting uninstall: langchain-openai\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: langchain-openai 1.1.3\n",
      "    Uninstalling langchain-openai-1.1.3:\n",
      "      Successfully uninstalled langchain-openai-1.1.3\n",
      "Installing collected packages: langchain-openai\n",
      "  Attempting uninstall: langchain-openai\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: langchain-openai 1.1.3\n",
      "    Uninstalling langchain-openai-1.1.3:\n",
      "      Successfully uninstalled langchain-openai-1.1.3\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed langchain-openai-1.1.6\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed langchain-openai-1.1.6\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 1. Force upgrade the critical libraries\n",
    "%pip install -U langchain langchain-core langchain-openai langchain-community pydantic\n",
    "\n",
    "# 2. IMPORTANT: You must restart the kernel after running this!\n",
    "# In VS Code/Jupyter: Click \"Restart\" or \"Restart Kernel\" in the top toolbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentence-transformers gensim datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f93c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --force-reinstall datasets sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a0c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --force-reinstall gensim numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f62fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U sentence-transformers transformers flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe8d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the missing local server engine\n",
    "%pip install \"pymilvus[milvus_lite]\"\n",
    "\n",
    "# CRITICAL: Restart your kernel again after this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71959f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pymilvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2bc651",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U pymilvus milvus-lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe2eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove the libraries causing the binary conflict\n",
    "# (These are optional speed-boosters for Pandas, not required for functionality)\n",
    "%pip uninstall -y bottleneck numexpr\n",
    "\n",
    "# 2. Force install a compatible version of Pandas and PyArrow\n",
    "# This ensures your Pandas matches your current NumPy version\n",
    "%pip install --upgrade pandas pyarrow numpy>=2.0\n",
    "\n",
    "# 3. CRITICAL: Restart your kernel now!\n",
    "# Click \"Kernel\" -> \"Restart Kernel\" in the menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58760a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Downgrade NumPy to the 1.x version (most compatible)\n",
    "%pip install \"numpy<2.0\"\n",
    "\n",
    "# 2. You MUST restart your kernel after this!\n",
    "# In VS Code/Jupyter: Click \"Restart\" or \"Restart Kernel\" in the top toolbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c5a2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-milvus langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e5d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fastapi uvicorn nest-asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865de2b5",
   "metadata": {},
   "source": [
    "# Simple scraping agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41be412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_classic.agents import AgentExecutor, create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c835fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connected to LLM running locally\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://127.0.0.1:1234/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    model=\"local-model\",\n",
    "    temperature=0,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Define the Tool\n",
    "@tool\n",
    "def fetch_csv_dataset(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads a CSV dataset from a URL and returns a summary.\n",
    "    Input should be the full URL string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse CSV\n",
    "        content = response.content.decode('utf-8')\n",
    "        df = pd.read_csv(StringIO(content), on_bad_lines='skip')\n",
    "        \n",
    "        return (\n",
    "            f\"SUCCESS: Downloaded data from {url}\\n\"\n",
    "            f\"Shape: {df.shape}\\n\"\n",
    "            f\"Columns: {list(df.columns)}\\n\"\n",
    "            f\"First 5 rows:\\n{df.head().to_string()}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "tools = [fetch_csv_dataset]\n",
    "\n",
    "# Define the ReAct Prompt (Hardcoded for stability)\n",
    "# This teaches the model explicitly how to think and act.\n",
    "template = '''Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}'''\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 4. Create the ReAct Agent\n",
    "# This uses simple text generation, avoiding the Pydantic/Tool Binding error completely.\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "\n",
    "# 5. Create the Executor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    verbose=True, \n",
    "    handle_parsing_errors=True # IMPORTANT for local models\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ReAct Agent built successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a0a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = \"https://raw.githubusercontent.com/gramener/datasets/refs/heads/main/card_transactions.csv\"\n",
    "query = f\"Download the dataset from {test_url} and tell me the columns.\"\n",
    "\n",
    "response = agent_executor.invoke({\"input\": query})\n",
    "print(\"\\n--- FINAL ANSWER ---\")\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a40c1ee",
   "metadata": {},
   "source": [
    "# Vector embedding of 2022-2024 news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d70f9a",
   "metadata": {},
   "source": [
    "### Load and filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0848cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"dataset/guardian_climate_news_corpus.csv\")\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "df = df[df['date'].dt.year >= 2022].copy()\n",
    "\n",
    "df = df[df['label'] != 'UNRELATED_TO_CLIMATE'].copy()\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94bd620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa4411",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bd94f4",
   "metadata": {},
   "source": [
    "### Making vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f402dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 1. SETUP: Load your data\n",
    "# ------------------------------------------------------------------\n",
    "# df = pd.read_csv(\"your_data.csv\") # Uncomment to load your real file\n",
    "# Ensure date is datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# 2. CREATE UNIFIED TEXT REPRESENTATION\n",
    "# ------------------------------------------------------------------\n",
    "# Instead of training a separate Word2Vec model, we will format the metadata \n",
    "# into a structured string that the 8B model can \"read\" and understand semantically.\n",
    "# This technique is often called \"Text Serialization\".\n",
    "\n",
    "def serialize_row_for_embedding(row):\n",
    "    # Parse tags safely\n",
    "    try:\n",
    "        tags = ast.literal_eval(row['tags']) if isinstance(row['tags'], str) else row['tags']\n",
    "        tags_str = \", \".join(tags)\n",
    "    except:\n",
    "        tags_str = \"None\"\n",
    "        \n",
    "    # Create a rich text block that describes the entire data point\n",
    "    # We put the most important semantic info (Category, Tags, Date) at the start or end.\n",
    "    combined_text = (\n",
    "        f\"Category: {row['category']}. \"\n",
    "        f\"Tags: {tags_str}. \"\n",
    "        f\"Date: {row['date'].strftime('%Y-%m-%d')}. \"\n",
    "        f\"Title: {row['title']}\\n\"\n",
    "        f\"Content: {row['body']}\"\n",
    "    )\n",
    "    return combined_text\n",
    "\n",
    "# Apply the function\n",
    "df['serialized_text'] = df.apply(serialize_row_for_embedding, axis=1)\n",
    "\n",
    "# 3. EMBED WITH LOCAL LLAMA MODEL (via OpenAI Compatible API)\n",
    "# ------------------------------------------------------------------\n",
    "# Assuming you are running the model in LM Studio / Ollama on port 1234\n",
    "# Check your local server settings for the exact URL.\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    base_url=\"http://127.0.0.1:1234/v1\", # Point to your local server\n",
    "    api_key=\"lm-studio\",                 # Arbitrary key\n",
    "    model=\"Qwen3-Embedding-4B-GGUF\",     # The specific model name loaded in your server\n",
    "    check_embedding_ctx_length=False     # Important for long texts\n",
    ")\n",
    "\n",
    "print(\"Starting embedding process... (This may take time depending on GPU)\")\n",
    "\n",
    "# We process in batches to be safe with memory\n",
    "batch_size = 32\n",
    "all_embeddings = []\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_texts = df['serialized_text'].iloc[i:i+batch_size].tolist()\n",
    "    \n",
    "    # Generate embeddings for the batch\n",
    "    # embed_documents returns a list of lists (vectors)\n",
    "    batch_embeddings = embedding_model.embed_documents(batch_texts)\n",
    "    all_embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    print(f\"Processed rows {i} to {min(i+batch_size, len(df))}\")\n",
    "\n",
    "# 4. STORE RESULTS\n",
    "# ------------------------------------------------------------------\n",
    "# Convert to numpy array for use in classifiers or Vector DB\n",
    "final_features = np.array(all_embeddings)\n",
    "\n",
    "print(f\"Final Feature Matrix Shape: {final_features.shape}\")\n",
    "\n",
    "# Optional: Add back to DataFrame\n",
    "df['embedding_vector'] = list(final_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd16ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Save the DataFrame (Contains text, metadata, and vectors)\n",
    "# Pickle is better than CSV because it preserves lists/arrays perfectly.\n",
    "df.to_pickle(\"climate_news_data.pkl\")\n",
    "\n",
    "# 2. Save the Raw Numpy Array (Just in case)\n",
    "# This is the safest way to store the pure mathematical vectors.\n",
    "np.save(\"climate_vectors.npy\", final_features)\n",
    "\n",
    "print(\"Saved 'climate_news_data.pkl' and 'climate_vectors.npy' to disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c086d9",
   "metadata": {},
   "source": [
    "### Storing embeddings with Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafcf5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"üöÄ Attempting to rescue data...\")\n",
    "\n",
    "# 1. Load the Pickle\n",
    "# Since we are on NumPy 2.x (installed above), this will read the file correctly.\n",
    "df = pd.read_pickle(\"climate_news_data.pkl\")\n",
    "print(f\"‚úÖ Pickle loaded successfully. Shape: {df.shape}\")\n",
    "\n",
    "# 2. Save as Parquet\n",
    "# We drop the vector column if it exists to keep the file light (we have the .npy file)\n",
    "if 'embedding_vector' in df.columns:\n",
    "    df = df.drop(columns=['embedding_vector'])\n",
    "\n",
    "df.to_parquet(\"climate_news_data.parquet\")\n",
    "print(\"‚úÖ SUCCESS: Data saved to 'climate_news_data.parquet'\")\n",
    "\n",
    "# 3. Verify Vector File\n",
    "# This usually loads fine regardless of version, but let's check.\n",
    "vectors = np.load(\"climate_vectors.npy\")\n",
    "print(f\"‚úÖ SUCCESS: Vectors verified. Shape: {vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc31116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymilvus import MilvusClient, DataType\n",
    "\n",
    "# 1. RELOAD YOUR SAVED DATA\n",
    "# ------------------------------------------------------------------\n",
    "print(\"üîÑ Reloading rescued data...\")\n",
    "df = pd.read_parquet(\"climate_news_data.parquet\")\n",
    "final_features = np.load(\"climate_vectors.npy\")\n",
    "print(f\"‚úÖ Data Loaded. Articles: {len(df)} | Vector Dim: {final_features.shape[1]}\")\n",
    "\n",
    "# 2. SETUP MILVUS LITE\n",
    "# ------------------------------------------------------------------\n",
    "client = MilvusClient(\"./climate_news.db\")\n",
    "COLLECTION_NAME = \"climate_articles\"\n",
    "\n",
    "# 3. DEFINE THE SCHEMA\n",
    "# ------------------------------------------------------------------\n",
    "if client.has_collection(COLLECTION_NAME):\n",
    "    client.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "schema = client.create_schema(auto_id=True, enable_dynamic_field=True)\n",
    "\n",
    "# Add Fields\n",
    "schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "schema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=2560) # Matches your Qwen size\n",
    "schema.add_field(field_name=\"category\", datatype=DataType.VARCHAR, max_length=512)\n",
    "schema.add_field(field_name=\"date\", datatype=DataType.VARCHAR, max_length=50)\n",
    "schema.add_field(field_name=\"text\", datatype=DataType.VARCHAR, max_length=65535)\n",
    "\n",
    "# 4. DEFINE INDEX\n",
    "# ------------------------------------------------------------------\n",
    "index_params = client.prepare_index_params()\n",
    "index_params.add_index(\n",
    "    field_name=\"vector\", \n",
    "    index_type=\"AUTOINDEX\", \n",
    "    metric_type=\"COSINE\"\n",
    ")\n",
    "\n",
    "# 5. CREATE COLLECTION\n",
    "# ------------------------------------------------------------------\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    schema=schema,\n",
    "    index_params=index_params\n",
    ")\n",
    "print(f\"‚úÖ Collection '{COLLECTION_NAME}' created.\")\n",
    "\n",
    "# 6. INSERT DATA\n",
    "# ------------------------------------------------------------------\n",
    "data_to_insert = []\n",
    "print(\"Preparing data for insertion...\")\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    vector_list = final_features[idx].tolist()\n",
    "    date_str = row['date'].strftime('%Y-%m-%d') if pd.notnull(row['date']) else \"\"\n",
    "    \n",
    "    entry = {\n",
    "        \"vector\": vector_list,\n",
    "        \"text\": str(row['body']),\n",
    "        \"title\": str(row['title']),\n",
    "        \"category\": str(row['category']),\n",
    "        \"date\": date_str,\n",
    "        \"tags\": str(row['tags'])\n",
    "    }\n",
    "    data_to_insert.append(entry)\n",
    "\n",
    "# Insert in batches\n",
    "batch_size = 100\n",
    "total_inserted = 0\n",
    "\n",
    "for i in range(0, len(data_to_insert), batch_size):\n",
    "    batch = data_to_insert[i:i+batch_size]\n",
    "    res = client.insert(collection_name=COLLECTION_NAME, data=batch)\n",
    "    total_inserted += res['insert_count']\n",
    "    print(f\"Inserted batch {i} to {i+len(batch)}...\")\n",
    "\n",
    "print(f\"‚úÖ SUCCESS! Stored {total_inserted} articles in 'climate_news.db'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4317b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymilvus import MilvusClient, DataType\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# 0. CLEANUP OLD CORRUPTED DB (Optional but recommended)\n",
    "# ------------------------------------------------------------------\n",
    "db_path = \"./climate_news.db\"\n",
    "if os.path.exists(db_path):\n",
    "    print(f\"‚ö†Ô∏è Found existing database at {db_path}. Removing it...\")\n",
    "    try:\n",
    "        if os.path.isdir(db_path):\n",
    "            shutil.rmtree(db_path)\n",
    "        else:\n",
    "            os.remove(db_path)\n",
    "        print(\"‚úÖ Old database removed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not remove old db: {e}\")\n",
    "\n",
    "# 1. RELOAD YOUR SAVED DATA\n",
    "# ------------------------------------------------------------------\n",
    "print(\"üîÑ Reloading rescued data...\")\n",
    "df = pd.read_parquet(\"climate_news_data.parquet\")\n",
    "final_features = np.load(\"climate_vectors.npy\")\n",
    "print(f\"‚úÖ Data Loaded. Articles: {len(df)} | Vector Dim: {final_features.shape[1]}\")\n",
    "\n",
    "# 2. SETUP MILVUS LITE (Fresh instance)\n",
    "# ------------------------------------------------------------------\n",
    "# Create a fresh client pointing to a clean database file\n",
    "client = MilvusClient(uri=db_path)\n",
    "COLLECTION_NAME = \"climate_articles\"\n",
    "\n",
    "# 3. DEFINE THE SCHEMA\n",
    "# ------------------------------------------------------------------\n",
    "if client.has_collection(COLLECTION_NAME):\n",
    "    client.drop_collection(COLLECTION_NAME)\n",
    "    print(f\"Dropped existing collection '{COLLECTION_NAME}'\")\n",
    "\n",
    "schema = client.create_schema(auto_id=True, enable_dynamic_field=True)\n",
    "\n",
    "# Add Fields\n",
    "schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "schema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=2560) # Matches your Qwen size\n",
    "schema.add_field(field_name=\"category\", datatype=DataType.VARCHAR, max_length=512)\n",
    "schema.add_field(field_name=\"date\", datatype=DataType.VARCHAR, max_length=50)\n",
    "schema.add_field(field_name=\"text\", datatype=DataType.VARCHAR, max_length=65535)\n",
    "\n",
    "# 4. DEFINE INDEX\n",
    "# ------------------------------------------------------------------\n",
    "index_params = client.prepare_index_params()\n",
    "index_params.add_index(\n",
    "    field_name=\"vector\", \n",
    "    index_type=\"AUTOINDEX\", \n",
    "    metric_type=\"COSINE\"\n",
    ")\n",
    "\n",
    "# 5. CREATE COLLECTION\n",
    "# ------------------------------------------------------------------\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    schema=schema,\n",
    "    index_params=index_params\n",
    ")\n",
    "print(f\"‚úÖ Collection '{COLLECTION_NAME}' created.\")\n",
    "\n",
    "# 6. INSERT DATA\n",
    "# ------------------------------------------------------------------\n",
    "data_to_insert = []\n",
    "print(\"Preparing data for insertion...\")\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    vector_list = final_features[idx].tolist()\n",
    "    date_str = row['date'].strftime('%Y-%m-%d') if pd.notnull(row['date']) else \"\"\n",
    "    \n",
    "    entry = {\n",
    "        \"vector\": vector_list,\n",
    "        \"text\": str(row['body']),\n",
    "        \"title\": str(row['title']),\n",
    "        \"category\": str(row['category']),\n",
    "        \"date\": date_str,\n",
    "        \"tags\": str(row['tags'])\n",
    "    }\n",
    "    data_to_insert.append(entry)\n",
    "\n",
    "# Insert in batches\n",
    "batch_size = 100\n",
    "total_inserted = 0\n",
    "\n",
    "for i in range(0, len(data_to_insert), batch_size):\n",
    "    batch = data_to_insert[i:i+batch_size]\n",
    "    res = client.insert(collection_name=COLLECTION_NAME, data=batch)\n",
    "    total_inserted += res['insert_count']\n",
    "    print(f\"Inserted batch {i} to {i+len(batch)}...\")\n",
    "\n",
    "print(f\"‚úÖ SUCCESS! Stored {total_inserted} articles in '{db_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d50db5",
   "metadata": {},
   "source": [
    "# RAG Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "client = MilvusClient(uri=\"./climate_news.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbde41ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG Chain initialized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. SETUP MODELS\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    base_url=\"http://127.0.0.1:1234/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    model=\"Qwen3-Embedding-4B-GGUF\",\n",
    "    check_embedding_ctx_length=False\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://127.0.0.1:8000/v1\", # Verify this is your Ministral server port\n",
    "    api_key=\"local-key\",\n",
    "    model=\"mistralai/Ministral-3-14B-Reasoning-2512\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# 2. CONNECT TO VECTOR STORE\n",
    "# Note: Ensure climate_news.db is NOT open in any other software\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"uri\": \"./climate_news.db\"},\n",
    "    collection_name=\"climate_articles\",\n",
    "    text_field=\"text\",\n",
    "    auto_id=True\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# 3. DEFINE RAG LOGIC\n",
    "template = \"\"\"You are a specialized Climate News Assistant.\n",
    "Use the context below to answer. If unsure, say you can't find it.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(\"‚úÖ RAG Chain initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ac5b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the system\n",
    "query = \"Tell me any news about tiny snails on Atlantic Island\"\n",
    "response = rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5980d990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: Tell me any news about tiny snails on Atlantic Island\n",
      "\n",
      "ü§ñ Agent Answer:\n",
      "More than 1,300 tiny, critically endangered snails have been set free to roam on an island off the coast of Morocco after a breeding programme rescued two obscure species from the brink of extinction. The Desertas Island land snails had not been recorded for more than 100 years and were believed to have disappeared from their natural habitat on the windswept, mountainous island of Deserta Grande, close to Portugal-owned Madeira. Experts at the Instituto das Florestas e Conserva√ß√£o da Natureza (IFCN) rediscovered minute populations of two species of the snail, each consisting of fewer than 200 survivors, in conservation expeditions between 2012 and 2017 amid fears that invasive predators might have eaten the pea-sized molluscs into oblivion. The snails were taken to zoos in the UK and France, with 60 flown to Chester zoo, where the conservation science team liaised with experts in Madeira and constructed new homes for them in mini habitat tanks as part of a breeding programme to quickly boost numbers. Dr Gerardo Garcia, Chester zoo‚Äôs head of ectotherms, said the future of the species ‚Äúwas in our hands‚Äù when the snails first arrived. ‚ÄúIt was a huge responsibility to begin caring for them,‚Äù he said. ‚ÄúAs a zoo conservation community, we knew nothing about them. They‚Äôd never been in human care before and we had to start from a blank piece of a paper and try to figure out what makes them tick.‚Äù Garcia said the snails ‚Äúreally were on the edge of extinction‚Äù and he paid tribute to his team of zookeepers who ‚Äúspent countless hours caring for every individual snail‚Äù. The snails have been released into a wild refuge on Bugio, a smaller neighbouring island in the Madeira archipelago that has been off-limits to humans since 1990 to protect its fragile ecosystem, where invasive species like rats, mice and goats have been eradicated. Heather Prince, an invertebrate specialist at Chester zoo, said: ‚ÄúWithin a few months we were able to crack the breeding of the Desertas land snails. Crucially, we were then successful in breeding multiple generations. This was key, because it meant we could then bring in the support of other zoos and establish a network, breeding them in the substantial numbers needed to have a chance of saving the species.‚Äù Each of the snails reintroduced has been individually marked for monitoring. If successful, many more will be released to bolster numbers. Dinarte Teixeira, a conservation biologist at IFCN, said: ‚ÄúThese snails are incredibly precious. The Desertas Islands are the only place in the world where they can be found and so we‚Äôre striving to do everything we can to give them the best possible chance for the future. For 100 years we thought they‚Äôd gone for ever, but now there‚Äôs new hope.‚Äù\n"
     ]
    }
   ],
   "source": [
    "print(f\"‚ùì Question: {query}\\n\")\n",
    "print(\"ü§ñ Agent Answer:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38babca8",
   "metadata": {},
   "source": [
    "# Embedding PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e657d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Dropping old collection 'research_papers'\n",
      "‚úÖ Clean slate ready.\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "DB_PATH = \"./climate_news.db\"\n",
    "COLLECTION_NAME = \"research_papers\"\n",
    "\n",
    "client = MilvusClient(uri=DB_PATH)\n",
    "\n",
    "if client.has_collection(COLLECTION_NAME):\n",
    "    print(f\"‚ö†Ô∏è Dropping old collection '{COLLECTION_NAME}'\")\n",
    "    client.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "print(\"‚úÖ Clean slate ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92d9c173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedded 20 / 411\n",
      "‚úÖ Embedded 40 / 411\n",
      "‚úÖ Embedded 40 / 411\n",
      "‚úÖ Embedded 60 / 411\n",
      "‚úÖ Embedded 60 / 411\n",
      "‚úÖ Embedded 80 / 411\n",
      "‚úÖ Embedded 80 / 411\n",
      "‚úÖ Embedded 100 / 411\n",
      "‚úÖ Embedded 100 / 411\n",
      "‚úÖ Embedded 120 / 411\n",
      "‚úÖ Embedded 120 / 411\n",
      "‚úÖ Embedded 140 / 411\n",
      "‚úÖ Embedded 140 / 411\n",
      "‚úÖ Embedded 160 / 411\n",
      "‚úÖ Embedded 160 / 411\n",
      "‚úÖ Embedded 180 / 411\n",
      "‚úÖ Embedded 180 / 411\n",
      "‚úÖ Embedded 200 / 411\n",
      "‚úÖ Embedded 200 / 411\n",
      "‚úÖ Embedded 220 / 411\n",
      "‚úÖ Embedded 220 / 411\n",
      "‚úÖ Embedded 240 / 411\n",
      "‚úÖ Embedded 240 / 411\n",
      "‚úÖ Embedded 260 / 411\n",
      "‚úÖ Embedded 260 / 411\n",
      "‚úÖ Embedded 280 / 411\n",
      "‚úÖ Embedded 280 / 411\n",
      "‚úÖ Embedded 300 / 411\n",
      "‚úÖ Embedded 300 / 411\n",
      "‚úÖ Embedded 320 / 411\n",
      "‚úÖ Embedded 320 / 411\n",
      "‚úÖ Embedded 340 / 411\n",
      "‚úÖ Embedded 340 / 411\n",
      "‚úÖ Embedded 360 / 411\n",
      "‚úÖ Embedded 360 / 411\n",
      "‚úÖ Embedded 380 / 411\n",
      "‚úÖ Embedded 380 / 411\n",
      "‚úÖ Embedded 400 / 411\n",
      "‚úÖ Embedded 400 / 411\n",
      "‚úÖ Embedded 411 / 411\n",
      "‚úÖ Embedded 411 / 411\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "PAPER_FOLDER = \"./research_paper\"\n",
    "EMBED_URL = \"http://127.0.0.1:1234/v1/embeddings\"\n",
    "MODEL_NAME = \"text-embedding-qwen3-embedding-4b\"\n",
    "\n",
    "loader = PyPDFDirectoryLoader(PAPER_FOLDER)\n",
    "raw_docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=80)\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "\n",
    "def embed_batch(texts):\n",
    "    resp = requests.post(\n",
    "        EMBED_URL,\n",
    "        json={\"model\": MODEL_NAME, \"input\": texts},\n",
    "        timeout=60\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    return [x[\"embedding\"] for x in resp.json()[\"data\"]]\n",
    "\n",
    "vectors = []\n",
    "texts = []\n",
    "metas = []\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "for i in range(0, len(docs), batch_size):\n",
    "    batch = docs[i:i+batch_size]\n",
    "    batch_texts = [d.page_content.strip() for d in batch]\n",
    "\n",
    "    emb = embed_batch(batch_texts)\n",
    "\n",
    "    vectors.extend(emb)\n",
    "    texts.extend(batch_texts)\n",
    "\n",
    "    for d in batch:\n",
    "        metas.append({\n",
    "            \"source\": d.metadata.get(\"source\", \"\"),\n",
    "            \"page\": int(d.metadata.get(\"page\", -1))\n",
    "        })\n",
    "\n",
    "    print(f\"‚úÖ Embedded {i+len(batch)} / {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b6aa7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Collection created.\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient, DataType\n",
    "\n",
    "DIM = len(vectors[0])\n",
    "client = MilvusClient(uri=DB_PATH)\n",
    "\n",
    "schema = client.create_schema(auto_id=True, enable_dynamic_field=False)\n",
    "\n",
    "schema.add_field(\"id\", DataType.INT64, is_primary=True)\n",
    "schema.add_field(\"vector\", DataType.FLOAT_VECTOR, dim=DIM)\n",
    "schema.add_field(\"text\", DataType.VARCHAR, max_length=65535)\n",
    "schema.add_field(\"source\", DataType.VARCHAR, max_length=1024)\n",
    "schema.add_field(\"page\", DataType.INT64)\n",
    "\n",
    "index_params = client.prepare_index_params()\n",
    "index_params.add_index(\n",
    "    field_name=\"vector\",\n",
    "    index_type=\"AUTOINDEX\",\n",
    "    metric_type=\"COSINE\"\n",
    ")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"research_papers\",\n",
    "    schema=schema,\n",
    "    index_params=index_params\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Collection created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c8b189e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 100 / 411\n",
      "Inserted 200 / 411\n",
      "Inserted 300 / 411\n",
      "Inserted 400 / 411\n",
      "Inserted 500 / 411\n",
      "üöÄ SUCCESS: research_papers indexed.\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for i in range(len(vectors)):\n",
    "    data.append({\n",
    "        \"vector\": vectors[i],\n",
    "        \"text\": texts[i],\n",
    "        \"source\": metas[i][\"source\"],\n",
    "        \"page\": metas[i][\"page\"]\n",
    "    })\n",
    "\n",
    "batch_size = 100\n",
    "for i in range(0, len(data), batch_size):\n",
    "    client.insert(\n",
    "        collection_name=\"research_papers\",\n",
    "        data=data[i:i+batch_size]\n",
    "    )\n",
    "    print(f\"Inserted {i+batch_size} / {len(data)}\")\n",
    "\n",
    "print(\"üöÄ SUCCESS: research_papers indexed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "764d25ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "Score: 0.5711402297019958\n",
      "Source: research_paper/2401.09646v1.pdf\n",
      "Page: 46\n",
      "tions on climate change.\\n Cite the documents provided in\n",
      "the context.\n",
      "StackExchange You‚Äôre an AI assistant generating answers to questions on\n",
      "the website stackexchange on the topic {source}.\n",
      "AppTek General You‚Äôre a helpful and harmless AI assistant.\n",
      "OASST-1 You‚Äôre Open Assistant, an AI language model, developed\n",
      "by Laion AI together with an open source community and\n",
      "trained using crowdsourced data.\n",
      "Dolly You‚Äôre an AI language model trained on data generated by\n",
      "employees of databricks.\n",
      "Llama-2 Sa\n",
      "\n",
      "---\n",
      "Score: 0.5260053277015686\n",
      "Source: research_paper/2505.18653v1.pdf\n",
      "Page: 7\n",
      "Task 0-Shot 5-Shot Diff.\n",
      "CDP-QA-Cities .55 (.10) .63 (.06) .07\n",
      "CDP-QA-Corp. .53 (.11) .62 (.05) .09\n",
      "CDP-QA-States .56 (.11) .64 (.06) .07\n",
      "CDP-Topic-Cities .32 (.02) .35 (.01) .03\n",
      "Climate Commit. .61 (.09) .67 (.04) .06\n",
      "Climate Detection .57 (.15) .71 (.06) .14\n",
      "Climate Eng .50 (.08) .54 (.05) .04\n",
      "Climate NER .13 (.06) .20 (.05) .08\n",
      "Climate Sentiment .60 (.11) .70 (.04) .09\n",
      "Climate Specificity .58 (.13) .69 (.07) .10\n",
      "Climate Stance .17 (.07) .51 (.09) .34\n",
      "Climate-Fever .43 (.11) .46 (.08) .03\n",
      "Clim\n",
      "\n",
      "---\n",
      "Score: 0.517491340637207\n",
      "Source: research_paper/2506.13796v1.pdf\n",
      "Page: 11\n",
      "assumptions about future socio-economic conditions and related mitigation measures.45 These are quantitative projections and are neither predictions nor forecasts. \n",
      "Around half of all modelled global emission scenarios assume cost-effective approaches that rely on least-cost mission abatement options globally. The other half look at \n",
      "existing policies and regionally and sectorally differentiated actions. Most do not make explicit assumptions about global equity, environmental justice\n",
      "Chunk 2 (fr\n"
     ]
    }
   ],
   "source": [
    "query = \"What does the paper say about climate mitigation strategies?\"\n",
    "\n",
    "query_emb = embed_batch([query])[0]\n",
    "\n",
    "res = client.search(\n",
    "    collection_name=\"research_papers\",\n",
    "    data=[query_emb],\n",
    "    anns_field=\"vector\",\n",
    "    limit=3,\n",
    "    output_fields=[\"text\", \"source\", \"page\"]\n",
    ")\n",
    "\n",
    "for hit in res[0]:\n",
    "    print(\"\\n---\")\n",
    "    print(\"Score:\", hit[\"distance\"])\n",
    "    print(\"Source:\", hit.get(\"source\", \"N/A\"))\n",
    "    print(\"Page:\", hit.get(\"page\", \"N/A\"))\n",
    "    print(hit[\"text\"][:500])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66676566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Research Paper RAG Chain initialized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. FAKE EMBEDDINGS (IMPORTANT)\n",
    "# We already embedded research papers manually.\n",
    "# LangChain must NOT try to re-embed stored documents.\n",
    "# ------------------------------------------------------------------\n",
    "class FakeEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts): \n",
    "        return []\n",
    "\n",
    "    def embed_query(self, text): \n",
    "        return []\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. REAL QUERY EMBEDDINGS (for search only)\n",
    "# ------------------------------------------------------------------\n",
    "query_embeddings = OpenAIEmbeddings(\n",
    "    base_url=\"http://127.0.0.1:1234/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    model=\"text-embedding-qwen3-embedding-4b\",\n",
    "    check_embedding_ctx_length=False\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. LLM\n",
    "# ------------------------------------------------------------------\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://127.0.0.1:1234/v1\",  # Ministral server\n",
    "    api_key=\"local-key\",\n",
    "    model=\"nvidia/nemotron-3-nano\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. CONNECT TO RESEARCH PAPERS VECTOR STORE\n",
    "# ------------------------------------------------------------------\n",
    "vector_store = Milvus(\n",
    "    embedding_function=FakeEmbeddings(),   # ‚¨ÖÔ∏è CRITICAL\n",
    "    connection_args={\"uri\": \"./climate_news.db\"},\n",
    "    collection_name=\"research_papers\",\n",
    "    vector_field=\"vector\",                 # ‚¨ÖÔ∏è matches schema\n",
    "    text_field=\"text\",\n",
    "    auto_id=True\n",
    ")\n",
    "\n",
    "def retrieve_docs(query: str, k: int = 3):\n",
    "    # 1. Embed query explicitly (DENSE)\n",
    "    query_vector = query_embeddings.embed_query(query)\n",
    "\n",
    "    # 2. Search using vector directly\n",
    "    docs = vector_store.similarity_search_by_vector(\n",
    "        embedding=query_vector,\n",
    "        k=k\n",
    "    )\n",
    "    return docs\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5. PROMPT (paper-aware, citation-friendly)\n",
    "# ------------------------------------------------------------------\n",
    "template = \"\"\"You are an academic research assistant.\n",
    "\n",
    "You are given excerpts from research papers.\n",
    "Use ONLY the provided context to answer when possible.\n",
    "If the answer is not found, say you could not find this\n",
    "information in the provided research papers.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER (cite sources if relevant):\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    blocks = []\n",
    "    for d in docs:\n",
    "        source = d.metadata.get(\"source\", \"unknown\")\n",
    "        page = d.metadata.get(\"page\", \"N/A\")\n",
    "        blocks.append(\n",
    "            f\"[Source: {source}, Page: {page}]\\n{d.page_content}\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(blocks)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6. RAG CHAIN\n",
    "# ------------------------------------------------------------------\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retrieve_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Research Paper RAG Chain initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed5736e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How is ClimateGPT model trained, and what is the model architecture like?\"\n",
    "response = rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d23f25c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: How is ClimateGPT model trained, and what is the model architecture like?\n",
      "\n",
      "ü§ñ Agent Answer:\n",
      "\n",
      "**Training**  \n",
      "- ClimateGPT was pre‚Äëtrained on a compute cluster using a fork of the **Megatron‚ÄëLLM** repository‚ÄØ[Document‚ÄØ1, page‚ÄØ40].  \n",
      "- After the pre‚Äëtraining phase, the model underwent **instruction fine‚Äëtuning (IFT)** to align its outputs with the expected format‚ÄØ[Document‚ÄØ1, page‚ÄØ40].\n",
      "\n",
      "**Model Architecture**  \n",
      "- ClimateGPT is built as an **auto‚Äëregressive language model** that employs an **optimized transformer architecture**‚ÄØ[Document‚ÄØ1, page‚ÄØ40].  \n",
      "\n",
      "These details describe both the training pipeline and the structural design of the ClimateGPT models.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"‚ùì Question: {query}\\n\")\n",
    "print(\"ü§ñ Agent Answer:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe8e730",
   "metadata": {},
   "source": [
    "# Model with combined retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d167691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. LOAD COLLECTION REGISTRY (Step 1)\n",
    "# ------------------------------------------------------------------\n",
    "with open(\"db_description.json\", \"r\") as f:\n",
    "    COLLECTION_REGISTRY = json.load(f)\n",
    "\n",
    "def format_registry():\n",
    "    \"\"\"Convert collection registry JSON into readable text for the LLM.\"\"\"\n",
    "    blocks = []\n",
    "    for col in COLLECTION_REGISTRY[\"collections\"]:\n",
    "        blocks.append(\n",
    "            f\"\"\"\n",
    "Collection: {col['name']}\n",
    "Type: {col['type']}\n",
    "Domain: {col['domain']}\n",
    "Granularity: {col['granularity']}\n",
    "Strengths: {\", \".join(col['strengths'])}\n",
    "Weaknesses: {\", \".join(col['weaknesses'])}\n",
    "\"\"\"\n",
    "        )\n",
    "    return \"\\n\".join(blocks)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. MULTI-COLLECTION RETRIEVER (Step 2)\n",
    "# ------------------------------------------------------------------\n",
    "# One Milvus vector store per collection\n",
    "research_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"uri\": \"./climate_news.db\"},\n",
    "    collection_name=\"research_papers\",\n",
    "    text_field=\"text\",\n",
    ")\n",
    "\n",
    "news_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"uri\": \"./climate_news.db\"},\n",
    "    collection_name=\"climate_articles\",\n",
    "    text_field=\"text\",\n",
    ")\n",
    "\n",
    "research_retriever = research_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "news_retriever = news_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "\n",
    "def retrieve_docs(query: str):\n",
    "    \"\"\"\n",
    "    Retrieve from BOTH collections.\n",
    "    Later we‚Äôll make this LLM-controlled.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    docs.extend(research_retriever.invoke(query))\n",
    "    docs.extend(news_retriever.invoke(query))\n",
    "    return docs\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    blocks = []\n",
    "    for d in docs:\n",
    "        source = d.metadata.get(\"source\", \"unknown\")\n",
    "        blocks.append(f\"[SOURCE: {source}]\\n{d.page_content}\")\n",
    "    return \"\\n\\n\".join(blocks)\n",
    "\n",
    "\n",
    "# Wrap functions as Runnables ‚úÖ\n",
    "retrieve_docs_runnable = RunnableLambda(retrieve_docs)\n",
    "format_docs_runnable = RunnableLambda(format_docs)\n",
    "registry_runnable = RunnableLambda(lambda _: format_registry())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50f4fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# COLLECTION ROUTER PROMPT\n",
    "# ------------------------------------------------------------------\n",
    "router_template = \"\"\"You are a routing assistant.\n",
    "\n",
    "You are given a list of available data collections.\n",
    "Your job is to decide which collections (if any) should be queried\n",
    "to answer the user question.\n",
    "\n",
    "Rules:\n",
    "- Only select collections that are clearly relevant.\n",
    "- If none are relevant, return \"none\".\n",
    "- Return ONLY valid JSON.\n",
    "- Do NOT explain your reasoning.\n",
    "\n",
    "Available collections:\n",
    "{registry}\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\n",
    "Respond in this exact JSON format:\n",
    "{{\n",
    "  \"collections\": [\"collection_name_1\", \"collection_name_2\"]\n",
    "}}\n",
    "\n",
    "OR\n",
    "\n",
    "{{\n",
    "  \"collections\": \"none\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "router_prompt = PromptTemplate.from_template(router_template)\n",
    "\n",
    "router_chain = (\n",
    "    {\n",
    "        \"registry\": registry_runnable,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | router_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "def select_collections(query: str):\n",
    "    \"\"\"Stage 1: Decide which collections to use.\"\"\"\n",
    "    raw = router_chain.invoke(query)\n",
    "    try:\n",
    "        return json.loads(raw)\n",
    "    except json.JSONDecodeError:\n",
    "        # Hard fail-safe\n",
    "        return {\"collections\": \"none\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ee0fc7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_VECTORSTORES = {\n",
    "    \"research_papers\": research_store,\n",
    "    \"climate_articles\": news_store,\n",
    "}\n",
    "\n",
    "def retrieve_from_collections(query: str, collections):\n",
    "    \"\"\"Retrieve documents from selected collections using direct vector search.\"\"\"\n",
    "    docs = []\n",
    "\n",
    "    # Embed the query once using the real embedding model\n",
    "    query_vector = query_embeddings.embed_query(query)\n",
    "\n",
    "    for name in collections:\n",
    "        store = COLLECTION_VECTORSTORES.get(name)\n",
    "        if store:\n",
    "            # Use similarity_search_by_vector instead of similarity_search\n",
    "            # to avoid re-embedding with FakeEmbeddings\n",
    "            docs.extend(store.similarity_search_by_vector(embedding=query_vector, k=3))\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf7a7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_template = \"\"\"You are a careful climate research assistant.\n",
    "\n",
    "Use ONLY the provided context to answer the question.\n",
    "If the context does not contain the answer, say so clearly.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "final_prompt = PromptTemplate.from_template(final_template)\n",
    "\n",
    "final_chain = (\n",
    "    {\n",
    "        \"context\": RunnablePassthrough(),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | final_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8eb2464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query: str):\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "    1. Decide collections\n",
    "    2. Retrieve context if needed\n",
    "    3. Answer with or without RAG\n",
    "    \"\"\"\n",
    "\n",
    "    routing = select_collections(query)\n",
    "    collections = routing.get(\"collections\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # CASE 1: No retrieval needed\n",
    "    # --------------------------------------------------\n",
    "    if collections == \"none\":\n",
    "        return llm.invoke(\n",
    "            f\"The following question is not answered using the provided datasets. \"\n",
    "            f\"Answer from general knowledge and say so explicitly if question is technical.\\n\\nQuestion: {query}\"\n",
    "        )\n",
    "    \n",
    "    if isinstance(collections, str):\n",
    "        collections = [collections]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # CASE 2: Targeted RAG\n",
    "    # --------------------------------------------------\n",
    "    docs = retrieve_from_collections(query, collections)\n",
    "    context = format_docs(docs)\n",
    "\n",
    "    if not context.strip():\n",
    "        return \"The selected datasets do not contain relevant information.\"\n",
    "\n",
    "    return final_chain.invoke(\n",
    "        {\n",
    "            \"context\": context,\n",
    "            \"question\": query\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b52aa273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Training**  \n",
      "- ClimateGPT was pre‚Äëtrained on a large corpus using a cluster provided by **MLFoundry**.  \n",
      "- The pre‚Äëtraining was carried out with a fork of the **Megatron‚ÄëLLM** repository (EPFL LLM Team).  \n",
      "- After the base pre‚Äëtraining, the models underwent **instruction fine‚Äëtuning (IFT)** to align them with the expected output format.\n",
      "\n",
      "**Model Architecture**  \n",
      "- ClimateGPT is an **auto‚Äëregressive language model** that employs an **optimized transformer architecture**.  \n",
      "- It comes in several sizes (7‚ÄØB, 13‚ÄØB, and 70‚ÄØB parameters), with two distinct 7‚ÄØB variants trained from scratch.  \n",
      "- The model accepts **text‚Äëonly input** and produces **text‚Äëonly output**.  \n",
      "\n",
      "These details are drawn directly from the provided research paper excerpts.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The news reports that more than‚ÄØ1,300 tiny, critically‚Äëendangered snails have been released onto a wild refuge on Bugio‚ÄØ‚Äî‚ÄØa small island in the Madeira archipelago off the coast of Morocco. The snails belong to two Desertas Island land‚Äësnail species that were thought extinct for over a century; conservationists at Chester‚ÄØzoo and the Instituto das Florestas e Conserva√ß√£o da Natureza (IFCN) bred them in captivity, raised multiple generations, and then set the individuals free on Bugio, where invasive predators have been eradicated. Each released snail is individually marked for monitoring, and the aim is to boost their numbers and secure the species‚Äô future.\n",
      "\n",
      "The news reports that more than‚ÄØ1,300 tiny, critically‚Äëendangered snails have been released onto a wild refuge on Bugio‚ÄØ‚Äî‚ÄØa small island in the Madeira archipelago off the coast of Morocco. The snails belong to two Desertas Island land‚Äësnail species that were thought extinct for over a century; conservationists at Chester‚ÄØzoo and the Instituto das Florestas e Conserva√ß√£o da Natureza (IFCN) bred them in captivity, raised multiple generations, and then set the individuals free on Bugio, where invasive predators have been eradicated. Each released snail is individually marked for monitoring, and the aim is to boost their numbers and secure the species‚Äô future.\n"
     ]
    }
   ],
   "source": [
    "print(answer_query(\"How is ClimateGPT model trained, and what is the model architecture like?\"))\n",
    "print(\"\\n\\n\\n\")\n",
    "print(answer_query(\"Tell me any news about tiny snails on Atlantic Island\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
