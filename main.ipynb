{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3c403f",
   "metadata": {},
   "source": [
    "# Pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42d88473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1767278840.030783 17718560 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting neo4j\n",
      "Collecting neo4j\n",
      "  Downloading neo4j-6.0.3-py3-none-any.whl.metadata (5.2 kB)\n",
      "  Downloading neo4j-6.0.3-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting langchain-neo4j\n",
      "  Downloading langchain_neo4j-0.6.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: pytz in /opt/anaconda3/lib/python3.12/site-packages (from neo4j) (2025.2)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-neo4j) (1.0.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-neo4j) (1.2.5)\n",
      "Collecting neo4j\n",
      "  Downloading neo4j-5.28.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-neo4j\n",
      "  Downloading langchain_neo4j-0.6.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: pytz in /opt/anaconda3/lib/python3.12/site-packages (from neo4j) (2025.2)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-neo4j) (1.0.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-neo4j) (1.2.5)\n",
      "Collecting neo4j\n",
      "  Downloading neo4j-5.28.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting neo4j-graphrag<2.0.0,>=1.9.0 (from langchain-neo4j)\n",
      "Collecting neo4j-graphrag<2.0.0,>=1.9.0 (from langchain-neo4j)\n",
      "  Downloading neo4j_graphrag-1.11.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (1.1.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.17 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (0.4.59)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (2.12.5)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (6.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (2.32.5)\n",
      "Requirement already satisfied: sqlalchemy<3.0.0,>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (2.0.45)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-neo4j) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-neo4j) (25.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-neo4j) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-neo4j) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-neo4j) (0.12.0)\n",
      "Collecting fsspec<2025.0.0,>=2024.9.0 (from neo4j-graphrag<2.0.0,>=1.9.0->langchain-neo4j)\n",
      "  Downloading neo4j_graphrag-1.11.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (1.1.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.17 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (0.4.59)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (2.12.5)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (6.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (2.32.5)\n",
      "Requirement already satisfied: sqlalchemy<3.0.0,>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (2.0.45)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-neo4j) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-neo4j) (25.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-neo4j) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-neo4j) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-neo4j) (0.12.0)\n",
      "Collecting fsspec<2025.0.0,>=2024.9.0 (from neo4j-graphrag<2.0.0,>=1.9.0->langchain-neo4j)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting json-repair<0.45.0,>=0.44.1 (from neo4j-graphrag<2.0.0,>=1.9.0->langchain-neo4j)\n",
      "  Downloading json_repair-0.44.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting json-repair<0.45.0,>=0.44.1 (from neo4j-graphrag<2.0.0,>=1.9.0->langchain-neo4j)\n",
      "  Downloading json_repair-0.44.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting numpy<3.0.0,>=2.0.0 (from neo4j-graphrag<2.0.0,>=1.9.0->langchain-neo4j)\n",
      "  Using cached numpy-2.4.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: pypdf<7.0.0,>=6.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from neo4j-graphrag<2.0.0,>=1.9.0->langchain-neo4j) (6.5.0)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from neo4j-graphrag<2.0.0,>=1.9.0->langchain-neo4j) (1.16.3)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<2.0.0,>=1.0.0->langchain-neo4j)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting types-pyyaml<7.0.0.0,>=6.0.12.20240917 (from neo4j-graphrag<2.0.0,>=1.9.0->langchain-neo4j)\n",
      "Collecting numpy<3.0.0,>=2.0.0 (from neo4j-graphrag<2.0.0,>=1.9.0->langchain-neo4j)\n",
      "  Using cached numpy-2.4.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: pypdf<7.0.0,>=6.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from neo4j-graphrag<2.0.0,>=1.9.0->langchain-neo4j) (6.5.0)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from neo4j-graphrag<2.0.0,>=1.9.0->langchain-neo4j) (1.16.3)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<2.0.0,>=1.0.0->langchain-neo4j)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting types-pyyaml<7.0.0.0,>=6.0.12.20240917 (from neo4j-graphrag<2.0.0,>=1.9.0->langchain-neo4j)\n",
      "  Downloading types_pyyaml-6.0.12.20250915-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-neo4j) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.17->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.17->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.17->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.17->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (2025.11.12)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (4.12.0)\n",
      "  Downloading types_pyyaml-6.0.12.20250915-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-neo4j) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.17->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.17->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.17->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.17->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (2025.11.12)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (4.12.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (0.16.0)\n",
      "Downloading langchain_neo4j-0.6.0-py3-none-any.whl (31 kB)\n",
      "Downloading neo4j-5.28.2-py3-none-any.whl (313 kB)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain-classic<2.0.0,>=1.0.0->langchain-neo4j) (0.16.0)\n",
      "Downloading langchain_neo4j-0.6.0-py3-none-any.whl (31 kB)\n",
      "Downloading neo4j-5.28.2-py3-none-any.whl (313 kB)\n",
      "Downloading neo4j_graphrag-1.11.0-py3-none-any.whl (206 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading neo4j_graphrag-1.11.0-py3-none-any.whl (206 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading json_repair-0.44.1-py3-none-any.whl (22 kB)\n",
      "Using cached numpy-2.4.0-cp312-cp312-macosx_14_0_arm64.whl (5.2 MB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading types_pyyaml-6.0.12.20250915-py3-none-any.whl (20 kB)\n",
      "Downloading json_repair-0.44.1-py3-none-any.whl (22 kB)\n",
      "Using cached numpy-2.4.0-cp312-cp312-macosx_14_0_arm64.whl (5.2 MB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading types_pyyaml-6.0.12.20250915-py3-none-any.whl (20 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: types-pyyaml, tenacity, numpy, neo4j, json-repair, fsspec, neo4j-graphrag, langchain-neo4j\n",
      "  Attempting uninstall: tenacity\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: tenacity 8.2.3\n",
      "    Uninstalling tenacity-8.2.3:\n",
      "      Successfully uninstalled tenacity-8.2.3\n",
      "  Attempting uninstall: numpy\n",
      "Installing collected packages: types-pyyaml, tenacity, numpy, neo4j, json-repair, fsspec, neo4j-graphrag, langchain-neo4j\n",
      "  Attempting uninstall: tenacity\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: tenacity 8.2.3\n",
      "    Uninstalling tenacity-8.2.3:\n",
      "      Successfully uninstalled tenacity-8.2.3\n",
      "  Attempting uninstall: numpy\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: fsspec\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: fsspec 2025.10.0\n",
      "    Uninstalling fsspec-2025.10.0:\n",
      "  Attempting uninstall: fsspec\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: fsspec 2025.10.0\n",
      "    Uninstalling fsspec-2025.10.0:\n",
      "      Successfully uninstalled fsspec-2025.10.0\n",
      "      Successfully uninstalled fsspec-2025.10.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.10.1 requires numexpr>=2.6.2, which is not installed.\n",
      "open-webui 0.6.43 requires pillow==12.0.0, but you have pillow 11.3.0 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.0 which is incompatible.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.4.0 which is incompatible.\n",
      "s3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2024.12.0 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.0 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.0 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires pillow<11,>=7.1.0, but you have pillow 11.3.0 which is incompatible.\n",
      "streamlit 1.37.1 requires tenacity<9,>=8.1.0, but you have tenacity 9.1.2 which is incompatible.\n",
      "datasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 20.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2024.12.0 json-repair-0.44.1 langchain-neo4j-0.6.0 neo4j-5.28.2 neo4j-graphrag-1.11.0 numpy-2.4.0 tenacity-9.1.2 types-pyyaml-6.0.12.20250915\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.10.1 requires numexpr>=2.6.2, which is not installed.\n",
      "open-webui 0.6.43 requires pillow==12.0.0, but you have pillow 11.3.0 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.0 which is incompatible.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.4.0 which is incompatible.\n",
      "s3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2024.12.0 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.0 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.0 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires pillow<11,>=7.1.0, but you have pillow 11.3.0 which is incompatible.\n",
      "streamlit 1.37.1 requires tenacity<9,>=8.1.0, but you have tenacity 9.1.2 which is incompatible.\n",
      "datasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 20.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2024.12.0 json-repair-0.44.1 langchain-neo4j-0.6.0 neo4j-5.28.2 neo4j-graphrag-1.11.0 numpy-2.4.0 tenacity-9.1.2 types-pyyaml-6.0.12.20250915\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install neo4j langchain-neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c73e3af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting langchain-docling\n",
      "Collecting langchain-docling\n",
      "  Downloading langchain_docling-2.0.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: langchain-core~=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-docling) (1.2.5)\n",
      "  Downloading langchain_docling-2.0.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: langchain-core~=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-docling) (1.2.5)\n",
      "Collecting docling~=2.26 (from langchain-docling)\n",
      "  Downloading docling-2.66.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting docling~=2.26 (from langchain-docling)\n",
      "  Downloading docling-2.66.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (2.12.5)\n",
      "Collecting docling-core<3.0.0,>=2.50.1 (from docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (2.12.5)\n",
      "Collecting docling-core<3.0.0,>=2.50.1 (from docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling)\n",
      "  Downloading docling_core-2.57.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Downloading docling_core-2.57.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting docling-parse<5.0.0,>=4.7.0 (from docling~=2.26->langchain-docling)\n",
      "Collecting docling-parse<5.0.0,>=4.7.0 (from docling~=2.26->langchain-docling)\n",
      "  Downloading docling_parse-4.7.2-cp312-cp312-macosx_14_0_arm64.whl.metadata (10 kB)\n",
      "  Downloading docling_parse-4.7.2-cp312-cp312-macosx_14_0_arm64.whl.metadata (10 kB)\n",
      "Collecting docling-ibm-models<4,>=3.9.1 (from docling~=2.26->langchain-docling)\n",
      "  Downloading docling_ibm_models-3.10.3-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (1.2.0)\n",
      "Collecting docling-ibm-models<4,>=3.9.1 (from docling~=2.26->langchain-docling)\n",
      "  Downloading docling_ibm_models-3.10.3-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (1.2.0)\n",
      "Collecting pypdfium2!=4.30.1,<5.0.0,>=4.30.0 (from docling~=2.26->langchain-docling)\n",
      "  Downloading pypdfium2-4.30.0-py3-none-macosx_11_0_arm64.whl.metadata (48 kB)\n",
      "Collecting pypdfium2!=4.30.1,<5.0.0,>=4.30.0 (from docling~=2.26->langchain-docling)\n",
      "  Downloading pypdfium2-4.30.0-py3-none-macosx_11_0_arm64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (2.12.0)\n",
      "Requirement already satisfied: huggingface_hub<1,>=0.23 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (0.36.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (2.32.5)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (2.12.0)\n",
      "Requirement already satisfied: huggingface_hub<1,>=0.23 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (0.36.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (2.32.5)\n",
      "Collecting ocrmac<2.0.0,>=1.0.0 (from docling~=2.26->langchain-docling)\n",
      "  Downloading ocrmac-1.0.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting ocrmac<2.0.0,>=1.0.0 (from docling~=2.26->langchain-docling)\n",
      "  Downloading ocrmac-1.0.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting rapidocr<4.0.0,>=3.3 (from docling~=2.26->langchain-docling)\n",
      "  Downloading rapidocr-3.4.5-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (2025.11.12)\n",
      "Collecting rapidocr<4.0.0,>=3.3 (from docling~=2.26->langchain-docling)\n",
      "  Downloading rapidocr-3.4.5-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (2025.11.12)\n",
      "Collecting rtree<2.0.0,>=1.3.0 (from docling~=2.26->langchain-docling)\n",
      "  Downloading rtree-1.4.1-py3-none-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting rtree<2.0.0,>=1.3.0 (from docling~=2.26->langchain-docling)\n",
      "  Downloading rtree-1.4.1-py3-none-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting typer<0.20.0,>=0.12.5 (from docling~=2.26->langchain-docling)\n",
      "  Downloading typer-0.19.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting python-docx<2.0.0,>=1.1.2 (from docling~=2.26->langchain-docling)\n",
      "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting typer<0.20.0,>=0.12.5 (from docling~=2.26->langchain-docling)\n",
      "  Downloading typer-0.19.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting python-docx<2.0.0,>=1.1.2 (from docling~=2.26->langchain-docling)\n",
      "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: python-pptx<2.0.0,>=1.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (1.0.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (4.12.3)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (2.3.3)\n",
      "Collecting marko<3.0.0,>=2.1.2 (from docling~=2.26->langchain-docling)\n",
      "  Downloading marko-2.2.1-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: python-pptx<2.0.0,>=1.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (1.0.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (4.12.3)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (2.3.3)\n",
      "Collecting marko<3.0.0,>=2.1.2 (from docling~=2.26->langchain-docling)\n",
      "  Downloading marko-2.2.1-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (3.1.5)\n",
      "Requirement already satisfied: lxml<7.0.0,>=4.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (5.2.1)\n",
      "Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (3.1.5)\n",
      "Requirement already satisfied: lxml<7.0.0,>=4.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (5.2.1)\n",
      "Collecting pillow<12.0.0,>=10.0.0 (from docling~=2.26->langchain-docling)\n",
      "  Downloading pillow-11.3.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (4.67.1)\n",
      "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (1.0.0)\n",
      "Collecting pylatexenc<3.0,>=2.10 (from docling~=2.26->langchain-docling)\n",
      "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
      "Collecting pillow<12.0.0,>=10.0.0 (from docling~=2.26->langchain-docling)\n",
      "  Downloading pillow-11.3.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (4.67.1)\n",
      "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (1.0.0)\n",
      "Collecting pylatexenc<3.0,>=2.10 (from docling~=2.26->langchain-docling)\n",
      "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l  Preparing metadata (setup.py) ... \u001b[?25l-done\n",
      "\u001b[?25hRequirement already satisfied: scipy<2.0.0,>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (1.16.3)\n",
      "Requirement already satisfied: accelerate<2,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (1.12.0)\n",
      "Collecting polyfactory>=2.22.2 (from docling~=2.26->langchain-docling)\n",
      "\bdone\n",
      "\u001b[?25hRequirement already satisfied: scipy<2.0.0,>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (1.16.3)\n",
      "Requirement already satisfied: accelerate<2,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from docling~=2.26->langchain-docling) (1.12.0)\n",
      "Collecting polyfactory>=2.22.2 (from docling~=2.26->langchain-docling)\n",
      "  Downloading polyfactory-3.2.0-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core~=1.0->langchain-docling) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core~=1.0->langchain-docling) (0.4.59)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core~=1.0->langchain-docling) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core~=1.0->langchain-docling) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core~=1.0->langchain-docling) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core~=1.0->langchain-docling) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core~=1.0->langchain-docling) (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate<2,>=1.0.0->docling~=2.26->langchain-docling) (2.2.6)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from accelerate<2,>=1.0.0->docling~=2.26->langchain-docling) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate<2,>=1.0.0->docling~=2.26->langchain-docling) (2.9.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate<2,>=1.0.0->docling~=2.26->langchain-docling) (0.7.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling~=2.26->langchain-docling) (2.5)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /opt/anaconda3/lib/python3.12/site-packages (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling) (4.23.0)\n",
      "Collecting jsonref<2.0.0,>=1.1.0 (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling)\n",
      "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading polyfactory-3.2.0-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core~=1.0->langchain-docling) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core~=1.0->langchain-docling) (0.4.59)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core~=1.0->langchain-docling) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core~=1.0->langchain-docling) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core~=1.0->langchain-docling) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core~=1.0->langchain-docling) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core~=1.0->langchain-docling) (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate<2,>=1.0.0->docling~=2.26->langchain-docling) (2.2.6)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from accelerate<2,>=1.0.0->docling~=2.26->langchain-docling) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate<2,>=1.0.0->docling~=2.26->langchain-docling) (2.9.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate<2,>=1.0.0->docling~=2.26->langchain-docling) (0.7.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling~=2.26->langchain-docling) (2.5)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /opt/anaconda3/lib/python3.12/site-packages (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling) (4.23.0)\n",
      "Collecting jsonref<2.0.0,>=1.1.0 (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling)\n",
      "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling) (0.9.0)\n",
      "Collecting latex2mathml<4.0.0,>=3.77.0 (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling) (0.9.0)\n",
      "Collecting latex2mathml<4.0.0,>=3.77.0 (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling)\n",
      "  Downloading latex2mathml-3.78.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting semchunk<3.0.0,>=2.2.0 (from docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling)\n",
      "  Downloading semchunk-2.2.2-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading latex2mathml-3.78.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting semchunk<3.0.0,>=2.2.0 (from docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling)\n",
      "  Downloading semchunk-2.2.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tree-sitter<1.0.0,>=0.23.2 (from docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling)\n",
      "Collecting tree-sitter<1.0.0,>=0.23.2 (from docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling)\n",
      "  Downloading tree_sitter-0.25.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (10.0 kB)\n",
      "Collecting tree-sitter-python<1.0.0,>=0.23.6 (from docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling)\n",
      "  Downloading tree_sitter_python-0.25.0-cp310-abi3-macosx_11_0_arm64.whl.metadata (1.9 kB)\n",
      "  Downloading tree_sitter-0.25.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (10.0 kB)\n",
      "Collecting tree-sitter-python<1.0.0,>=0.23.6 (from docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling)\n",
      "  Downloading tree_sitter_python-0.25.0-cp310-abi3-macosx_11_0_arm64.whl.metadata (1.9 kB)\n",
      "Collecting tree-sitter-c<1.0.0,>=0.23.4 (from docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling)\n",
      "  Downloading tree_sitter_c-0.24.1-cp310-abi3-macosx_11_0_arm64.whl.metadata (1.8 kB)\n",
      "Collecting tree-sitter-java<1.0.0,>=0.23.5 (from docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling)\n",
      "Collecting tree-sitter-c<1.0.0,>=0.23.4 (from docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling)\n",
      "  Downloading tree_sitter_c-0.24.1-cp310-abi3-macosx_11_0_arm64.whl.metadata (1.8 kB)\n",
      "Collecting tree-sitter-java<1.0.0,>=0.23.5 (from docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling)\n",
      "  Downloading tree_sitter_java-0.23.5-cp39-abi3-macosx_11_0_arm64.whl.metadata (1.7 kB)\n",
      "Collecting tree-sitter-javascript<1.0.0,>=0.23.1 (from docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling)\n",
      "  Downloading tree_sitter_javascript-0.25.0-cp310-abi3-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "  Downloading tree_sitter_java-0.23.5-cp39-abi3-macosx_11_0_arm64.whl.metadata (1.7 kB)\n",
      "Collecting tree-sitter-javascript<1.0.0,>=0.23.1 (from docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling)\n",
      "  Downloading tree_sitter_javascript-0.25.0-cp310-abi3-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting tree-sitter-typescript<1.0.0,>=0.23.2 (from docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling)\n",
      "  Downloading tree_sitter_typescript-0.23.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/anaconda3/lib/python3.12/site-packages (from docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling) (4.57.3)\n",
      "Collecting tree-sitter-typescript<1.0.0,>=0.23.2 (from docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling)\n",
      "  Downloading tree_sitter_typescript-0.23.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/anaconda3/lib/python3.12/site-packages (from docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling) (4.57.3)\n",
      "Collecting torchvision<1,>=0 (from docling-ibm-models<4,>=3.9.1->docling~=2.26->langchain-docling)\n",
      "  Downloading torchvision-0.24.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.9 kB)\n",
      "Collecting torchvision<1,>=0 (from docling-ibm-models<4,>=3.9.1->docling~=2.26->langchain-docling)\n",
      "  Downloading torchvision-0.24.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.9 kB)\n",
      "Collecting jsonlines<5.0.0,>=3.1.0 (from docling-ibm-models<4,>=3.9.1->docling~=2.26->langchain-docling)\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub<1,>=0.23->docling~=2.26->langchain-docling) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub<1,>=0.23->docling~=2.26->langchain-docling) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub<1,>=0.23->docling~=2.26->langchain-docling) (1.2.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core~=1.0->langchain-docling) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core~=1.0->langchain-docling) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core~=1.0->langchain-docling) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core~=1.0->langchain-docling) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core~=1.0->langchain-docling) (0.23.0)\n",
      "Requirement already satisfied: Click>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from ocrmac<2.0.0,>=1.0.0->docling~=2.26->langchain-docling) (8.3.1)\n",
      "Collecting jsonlines<5.0.0,>=3.1.0 (from docling-ibm-models<4,>=3.9.1->docling~=2.26->langchain-docling)\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub<1,>=0.23->docling~=2.26->langchain-docling) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub<1,>=0.23->docling~=2.26->langchain-docling) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub<1,>=0.23->docling~=2.26->langchain-docling) (1.2.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core~=1.0->langchain-docling) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core~=1.0->langchain-docling) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core~=1.0->langchain-docling) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core~=1.0->langchain-docling) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core~=1.0->langchain-docling) (0.23.0)\n",
      "Requirement already satisfied: Click>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from ocrmac<2.0.0,>=1.0.0->docling~=2.26->langchain-docling) (8.3.1)\n",
      "Collecting pyobjc-framework-Vision (from ocrmac<2.0.0,>=1.0.0->docling~=2.26->langchain-docling)\n",
      "  Downloading pyobjc_framework_vision-12.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: et-xmlfile in /opt/anaconda3/lib/python3.12/site-packages (from openpyxl<4.0.0,>=3.1.5->docling~=2.26->langchain-docling) (1.1.0)\n",
      "Collecting pyobjc-framework-Vision (from ocrmac<2.0.0,>=1.0.0->docling~=2.26->langchain-docling)\n",
      "  Downloading pyobjc_framework_vision-12.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: et-xmlfile in /opt/anaconda3/lib/python3.12/site-packages (from openpyxl<4.0.0,>=3.1.5->docling~=2.26->langchain-docling) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0.0,>=2.1.4->docling~=2.26->langchain-docling) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0.0,>=2.1.4->docling~=2.26->langchain-docling) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0.0,>=2.1.4->docling~=2.26->langchain-docling) (2025.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0.0,>=2.1.4->docling~=2.26->langchain-docling) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0.0,>=2.1.4->docling~=2.26->langchain-docling) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3.0.0,>=2.1.4->docling~=2.26->langchain-docling) (2025.3)\n",
      "Collecting faker>=5.0.0 (from polyfactory>=2.22.2->docling~=2.26->langchain-docling)\n",
      "  Downloading faker-40.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->docling~=2.26->langchain-docling) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->docling~=2.26->langchain-docling) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->docling~=2.26->langchain-docling) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.3.0->docling~=2.26->langchain-docling) (1.2.1)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /opt/anaconda3/lib/python3.12/site-packages (from python-pptx<2.0.0,>=1.0.2->docling~=2.26->langchain-docling) (3.2.9)\n",
      "Requirement already satisfied: pyclipper>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rapidocr<4.0.0,>=3.3->docling~=2.26->langchain-docling) (1.4.0)\n",
      "Requirement already satisfied: opencv_python>=4.5.1.48 in /opt/anaconda3/lib/python3.12/site-packages (from rapidocr<4.0.0,>=3.3->docling~=2.26->langchain-docling) (4.12.0.88)\n",
      "Requirement already satisfied: six>=1.15.0 in /opt/anaconda3/lib/python3.12/site-packages (from rapidocr<4.0.0,>=3.3->docling~=2.26->langchain-docling) (1.17.0)\n",
      "Requirement already satisfied: Shapely!=2.0.4,>=1.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from rapidocr<4.0.0,>=3.3->docling~=2.26->langchain-docling) (2.1.2)\n",
      "Collecting faker>=5.0.0 (from polyfactory>=2.22.2->docling~=2.26->langchain-docling)\n",
      "  Downloading faker-40.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->docling~=2.26->langchain-docling) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->docling~=2.26->langchain-docling) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->docling~=2.26->langchain-docling) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.3.0->docling~=2.26->langchain-docling) (1.2.1)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /opt/anaconda3/lib/python3.12/site-packages (from python-pptx<2.0.0,>=1.0.2->docling~=2.26->langchain-docling) (3.2.9)\n",
      "Requirement already satisfied: pyclipper>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rapidocr<4.0.0,>=3.3->docling~=2.26->langchain-docling) (1.4.0)\n",
      "Requirement already satisfied: opencv_python>=4.5.1.48 in /opt/anaconda3/lib/python3.12/site-packages (from rapidocr<4.0.0,>=3.3->docling~=2.26->langchain-docling) (4.12.0.88)\n",
      "Requirement already satisfied: six>=1.15.0 in /opt/anaconda3/lib/python3.12/site-packages (from rapidocr<4.0.0,>=3.3->docling~=2.26->langchain-docling) (1.17.0)\n",
      "Requirement already satisfied: Shapely!=2.0.4,>=1.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from rapidocr<4.0.0,>=3.3->docling~=2.26->langchain-docling) (2.1.2)\n",
      "Collecting omegaconf (from rapidocr<4.0.0,>=3.3->docling~=2.26->langchain-docling)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting omegaconf (from rapidocr<4.0.0,>=3.3->docling~=2.26->langchain-docling)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting colorlog (from rapidocr<4.0.0,>=3.3->docling~=2.26->langchain-docling)\n",
      "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.2->docling~=2.26->langchain-docling) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.2->docling~=2.26->langchain-docling) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.2->docling~=2.26->langchain-docling) (2.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<0.20.0,>=0.12.5->docling~=2.26->langchain-docling) (1.5.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<0.20.0,>=0.12.5->docling~=2.26->langchain-docling) (13.7.1)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core~=1.0->langchain-docling) (4.12.0)\n",
      "Collecting colorlog (from rapidocr<4.0.0,>=3.3->docling~=2.26->langchain-docling)\n",
      "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.2->docling~=2.26->langchain-docling) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.2->docling~=2.26->langchain-docling) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.2->docling~=2.26->langchain-docling) (2.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<0.20.0,>=0.12.5->docling~=2.26->langchain-docling) (1.5.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<0.20.0,>=0.12.5->docling~=2.26->langchain-docling) (13.7.1)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core~=1.0->langchain-docling) (4.12.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core~=1.0->langchain-docling) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core~=1.0->langchain-docling) (0.16.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jsonlines<5.0.0,>=3.1.0->docling-ibm-models<4,>=3.9.1->docling~=2.26->langchain-docling) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling) (0.10.6)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling~=2.26->langchain-docling) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling~=2.26->langchain-docling) (2.15.1)\n",
      "Collecting mpire[dill] (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core~=1.0->langchain-docling) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core~=1.0->langchain-docling) (0.16.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jsonlines<5.0.0,>=3.1.0->docling-ibm-models<4,>=3.9.1->docling~=2.26->langchain-docling) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling) (0.10.6)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling~=2.26->langchain-docling) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling~=2.26->langchain-docling) (2.15.1)\n",
      "Collecting mpire[dill] (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling)\n",
      "  Downloading mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling~=2.26->langchain-docling) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling~=2.26->langchain-docling) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling~=2.26->langchain-docling) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling~=2.26->langchain-docling) (3.1.6)\n",
      "  Downloading mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling~=2.26->langchain-docling) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling~=2.26->langchain-docling) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling~=2.26->langchain-docling) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling~=2.26->langchain-docling) (3.1.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling) (0.22.1)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->rapidocr<4.0.0,>=3.3->docling~=2.26->langchain-docling)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling) (0.22.1)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->rapidocr<4.0.0,>=3.3->docling~=2.26->langchain-docling)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l  Preparing metadata (setup.py) ... \u001b[?25l-done\n",
      "\u001b[?25done\n",
      "\u001b[?25hCollecting pyobjc-core>=12.1 (from pyobjc-framework-Vision->ocrmac<2.0.0,>=1.0.0->docling~=2.26->langchain-docling)\n",
      "  Downloading pyobjc_core-12.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (2.8 kB)\n",
      "Collecting pyobjc-core>=12.1 (from pyobjc-framework-Vision->ocrmac<2.0.0,>=1.0.0->docling~=2.26->langchain-docling)\n",
      "  Downloading pyobjc_core-12.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (2.8 kB)\n",
      "Collecting pyobjc-framework-Cocoa>=12.1 (from pyobjc-framework-Vision->ocrmac<2.0.0,>=1.0.0->docling~=2.26->langchain-docling)\n",
      "  Downloading pyobjc_framework_cocoa-12.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (2.6 kB)\n",
      "Collecting pyobjc-framework-Cocoa>=12.1 (from pyobjc-framework-Vision->ocrmac<2.0.0,>=1.0.0->docling~=2.26->langchain-docling)\n",
      "  Downloading pyobjc_framework_cocoa-12.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (2.6 kB)\n",
      "Collecting pyobjc-framework-Quartz>=12.1 (from pyobjc-framework-Vision->ocrmac<2.0.0,>=1.0.0->docling~=2.26->langchain-docling)\n",
      "  Downloading pyobjc_framework_quartz-12.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (3.6 kB)\n",
      "Collecting pyobjc-framework-Quartz>=12.1 (from pyobjc-framework-Vision->ocrmac<2.0.0,>=1.0.0->docling~=2.26->langchain-docling)\n",
      "  Downloading pyobjc_framework_quartz-12.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (3.6 kB)\n",
      "Collecting pyobjc-framework-CoreML>=12.1 (from pyobjc-framework-Vision->ocrmac<2.0.0,>=1.0.0->docling~=2.26->langchain-docling)\n",
      "  Downloading pyobjc_framework_coreml-12.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (2.5 kB)\n",
      "Collecting pyobjc-framework-CoreML>=12.1 (from pyobjc-framework-Vision->ocrmac<2.0.0,>=1.0.0->docling~=2.26->langchain-docling)\n",
      "  Downloading pyobjc_framework_coreml-12.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.20.0,>=0.12.5->docling~=2.26->langchain-docling) (0.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate<2,>=1.0.0->docling~=2.26->langchain-docling) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate<2,>=1.0.0->docling~=2.26->langchain-docling) (3.0.3)\n",
      "Requirement already satisfied: multiprocess>=0.70.15 in /opt/anaconda3/lib/python3.12/site-packages (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling) (0.70.18)\n",
      "Requirement already satisfied: dill>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling) (0.4.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.20.0,>=0.12.5->docling~=2.26->langchain-docling) (0.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate<2,>=1.0.0->docling~=2.26->langchain-docling) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate<2,>=1.0.0->docling~=2.26->langchain-docling) (3.0.3)\n",
      "Requirement already satisfied: multiprocess>=0.70.15 in /opt/anaconda3/lib/python3.12/site-packages (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling) (0.70.18)\n",
      "Requirement already satisfied: dill>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling~=2.26->langchain-docling) (0.4.0)\n",
      "Downloading langchain_docling-2.0.0-py3-none-any.whl (7.4 kB)\n",
      "Downloading docling-2.66.0-py3-none-any.whl (276 kB)\n",
      "Downloading docling_core-2.57.0-py3-none-any.whl (223 kB)\n",
      "Downloading langchain_docling-2.0.0-py3-none-any.whl (7.4 kB)\n",
      "Downloading docling-2.66.0-py3-none-any.whl (276 kB)\n",
      "Downloading docling_core-2.57.0-py3-none-any.whl (223 kB)\n",
      "Downloading docling_ibm_models-3.10.3-py3-none-any.whl (87 kB)\n",
      "Downloading docling_parse-4.7.2-cp312-cp312-macosx_14_0_arm64.whl (14.6 MB)\n",
      "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/14.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading docling_ibm_models-3.10.3-py3-none-any.whl (87 kB)\n",
      "Downloading docling_parse-4.7.2-cp312-cp312-macosx_14_0_arm64.whl (14.6 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading marko-2.2.1-py3-none-any.whl (42 kB)\n",
      "Downloading ocrmac-1.0.0-py2.py3-none-any.whl (12 kB)\n",
      "Downloading pillow-11.3.0-cp312-cp312-macosx_11_0_arm64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marko-2.2.1-py3-none-any.whl (42 kB)\n",
      "Downloading ocrmac-1.0.0-py2.py3-none-any.whl (12 kB)\n",
      "Downloading pillow-11.3.0-cp312-cp312-macosx_11_0_arm64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading polyfactory-3.2.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading polyfactory-3.2.0-py3-none-any.whl (62 kB)\n",
      "Downloading pypdfium2-4.30.0-py3-none-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/2.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading pypdfium2-4.30.0-py3-none-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "Downloading rapidocr-3.4.5-py3-none-any.whl (15.1 MB)\n",
      "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/15.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading rapidocr-3.4.5-py3-none-any.whl (15.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rtree-1.4.1-py3-none-macosx_11_0_arm64.whl (436 kB)\n",
      "Downloading typer-0.19.2-py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rtree-1.4.1-py3-none-macosx_11_0_arm64.whl (436 kB)\n",
      "Downloading typer-0.19.2-py3-none-any.whl (46 kB)\n",
      "Downloading faker-40.1.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading faker-40.1.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
      "Downloading latex2mathml-3.78.1-py3-none-any.whl (73 kB)\n",
      "Downloading semchunk-2.2.2-py3-none-any.whl (10 kB)\n",
      "Downloading torchvision-0.24.1-cp312-cp312-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading latex2mathml-3.78.1-py3-none-any.whl (73 kB)\n",
      "Downloading semchunk-2.2.2-py3-none-any.whl (10 kB)\n",
      "Downloading torchvision-0.24.1-cp312-cp312-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tree_sitter-0.25.2-cp312-cp312-macosx_11_0_arm64.whl (137 kB)\n",
      "Downloading tree_sitter_c-0.24.1-cp310-abi3-macosx_11_0_arm64.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tree_sitter-0.25.2-cp312-cp312-macosx_11_0_arm64.whl (137 kB)\n",
      "Downloading tree_sitter_c-0.24.1-cp310-abi3-macosx_11_0_arm64.whl (86 kB)\n",
      "Downloading tree_sitter_java-0.23.5-cp39-abi3-macosx_11_0_arm64.whl (62 kB)\n",
      "Downloading tree_sitter_javascript-0.25.0-cp310-abi3-macosx_11_0_arm64.whl (66 kB)\n",
      "Downloading tree_sitter_python-0.25.0-cp310-abi3-macosx_11_0_arm64.whl (76 kB)\n",
      "Downloading tree_sitter_java-0.23.5-cp39-abi3-macosx_11_0_arm64.whl (62 kB)\n",
      "Downloading tree_sitter_javascript-0.25.0-cp310-abi3-macosx_11_0_arm64.whl (66 kB)\n",
      "Downloading tree_sitter_python-0.25.0-cp310-abi3-macosx_11_0_arm64.whl (76 kB)\n",
      "Downloading tree_sitter_typescript-0.23.2-cp39-abi3-macosx_11_0_arm64.whl (302 kB)\n",
      "Downloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Downloading tree_sitter_typescript-0.23.2-cp39-abi3-macosx_11_0_arm64.whl (302 kB)\n",
      "Downloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Downloading pyobjc_framework_vision-12.1-cp312-cp312-macosx_10_13_universal2.whl (16 kB)\n",
      "Downloading pyobjc_core-12.1-cp312-cp312-macosx_10_13_universal2.whl (678 kB)\n",
      "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/678.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading pyobjc_framework_vision-12.1-cp312-cp312-macosx_10_13_universal2.whl (16 kB)\n",
      "Downloading pyobjc_core-12.1-cp312-cp312-macosx_10_13_universal2.whl (678 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m678.3/678.3 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyobjc_framework_cocoa-12.1-cp312-cp312-macosx_10_13_universal2.whl (384 kB)\n",
      "Downloading pyobjc_framework_coreml-12.1-cp312-cp312-macosx_10_13_universal2.whl (11 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m678.3/678.3 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyobjc_framework_cocoa-12.1-cp312-cp312-macosx_10_13_universal2.whl (384 kB)\n",
      "Downloading pyobjc_framework_coreml-12.1-cp312-cp312-macosx_10_13_universal2.whl (11 kB)\n",
      "Downloading pyobjc_framework_quartz-12.1-cp312-cp312-macosx_10_13_universal2.whl (218 kB)\n",
      "Downloading mpire-2.10.2-py3-none-any.whl (272 kB)\n",
      "Downloading pyobjc_framework_quartz-12.1-cp312-cp312-macosx_10_13_universal2.whl (218 kB)\n",
      "Downloading mpire-2.10.2-py3-none-any.whl (272 kB)\n",
      "Building wheels for collected packages: pylatexenc, antlr4-python3-runtime\n",
      "  Building wheel for pylatexenc (setup.py) ... \u001b[?25lBuilding wheels for collected packages: pylatexenc, antlr4-python3-runtime\n",
      "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l-done\n",
      "\u001b[?25h  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136897 sha256=2fd1644105990646045a349a5afa819b9176e6912d792346322592d61d864e60\n",
      "  Stored in directory: /Users/manavgargh/Library/Caches/pip/wheels/06/3e/78/fa1588c1ae991bbfd814af2bcac6cef7a178beee1939180d46\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25done\n",
      "\u001b[?25h  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136897 sha256=2fd1644105990646045a349a5afa819b9176e6912d792346322592d61d864e60\n",
      "  Stored in directory: /Users/manavgargh/Library/Caches/pip/wheels/06/3e/78/fa1588c1ae991bbfd814af2bcac6cef7a178beee1939180d46\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l-done\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144591 sha256=492277bbb84fdc4dec156043ba047d9c0b3d7faf55b61ac0e931ba1b0dc9f1dc\n",
      "  Stored in directory: /Users/manavgargh/Library/Caches/pip/wheels/1f/be/48/13754633f1d08d1fbfc60d5e80ae1e5d7329500477685286cd\n",
      "Successfully built pylatexenc antlr4-python3-runtime\n",
      "\bdone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144591 sha256=492277bbb84fdc4dec156043ba047d9c0b3d7faf55b61ac0e931ba1b0dc9f1dc\n",
      "  Stored in directory: /Users/manavgargh/Library/Caches/pip/wheels/1f/be/48/13754633f1d08d1fbfc60d5e80ae1e5d7329500477685286cd\n",
      "Successfully built pylatexenc antlr4-python3-runtime\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pylatexenc, antlr4-python3-runtime, tree-sitter-typescript, tree-sitter-python, tree-sitter-javascript, tree-sitter-java, tree-sitter-c, tree-sitter, rtree, python-docx, pypdfium2, pyobjc-core, pillow, omegaconf, mpire, marko, latex2mathml, jsonref, jsonlines, faker, colorlog, rapidocr, pyobjc-framework-Cocoa, polyfactory, typer, torchvision, semchunk, pyobjc-framework-Quartz, pyobjc-framework-CoreML, pyobjc-framework-Vision, docling-core, ocrmac, docling-parse, docling-ibm-models, docling, langchain-docling\n",
      "Installing collected packages: pylatexenc, antlr4-python3-runtime, tree-sitter-typescript, tree-sitter-python, tree-sitter-javascript, tree-sitter-java, tree-sitter-c, tree-sitter, rtree, python-docx, pypdfium2, pyobjc-core, pillow, omegaconf, mpire, marko, latex2mathml, jsonref, jsonlines, faker, colorlog, rapidocr, pyobjc-framework-Cocoa, polyfactory, typer, torchvision, semchunk, pyobjc-framework-Quartz, pyobjc-framework-CoreML, pyobjc-framework-Vision, docling-core, ocrmac, docling-parse, docling-ibm-models, docling, langchain-docling\n",
      "  Attempting uninstall: rtree\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: Rtree 1.0.1\n",
      "    Uninstalling Rtree-1.0.1:\n",
      "      Successfully uninstalled Rtree-1.0.1\n",
      "  Attempting uninstall: rtree\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: Rtree 1.0.1\n",
      "    Uninstalling Rtree-1.0.1:\n",
      "      Successfully uninstalled Rtree-1.0.1\n",
      "  Attempting uninstall: pyobjc-core\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: pyobjc-core 10.1\n",
      "    Uninstalling pyobjc-core-10.1:\n",
      "      Successfully uninstalled pyobjc-core-10.1\n",
      "  Attempting uninstall: pyobjc-core\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: pyobjc-core 10.1\n",
      "    Uninstalling pyobjc-core-10.1:\n",
      "      Successfully uninstalled pyobjc-core-10.1\n",
      "  Attempting uninstall: pillow\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: pillow 12.0.0\n",
      "    Uninstalling pillow-12.0.0:\n",
      "      Successfully uninstalled pillow-12.0.0\n",
      "  Attempting uninstall: pillow\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: pillow 12.0.0\n",
      "    Uninstalling pillow-12.0.0:\n",
      "      Successfully uninstalled pillow-12.0.0\n",
      "  Attempting uninstall: pyobjc-framework-Cocoa\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: pyobjc-framework-Cocoa 10.1\n",
      "    Uninstalling pyobjc-framework-Cocoa-10.1:\n",
      "  Attempting uninstall: pyobjc-framework-Cocoa\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: pyobjc-framework-Cocoa 10.1\n",
      "    Uninstalling pyobjc-framework-Cocoa-10.1:\n",
      "      Successfully uninstalled pyobjc-framework-Cocoa-10.1\n",
      "      Successfully uninstalled pyobjc-framework-Cocoa-10.1\n",
      "  Attempting uninstall: typer\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: typer 0.9.0\n",
      "    Uninstalling typer-0.9.0:\n",
      "      Successfully uninstalled typer-0.9.0\n",
      "  Attempting uninstall: typer\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: typer 0.9.0\n",
      "    Uninstalling typer-0.9.0:\n",
      "      Successfully uninstalled typer-0.9.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "open-webui 0.6.43 requires pillow==12.0.0, but you have pillow 11.3.0 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires pillow<11,>=7.1.0, but you have pillow 11.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed antlr4-python3-runtime-4.9.3 colorlog-6.10.1 docling-2.66.0 docling-core-2.57.0 docling-ibm-models-3.10.3 docling-parse-4.7.2 faker-40.1.0 jsonlines-4.0.0 jsonref-1.1.0 langchain-docling-2.0.0 latex2mathml-3.78.1 marko-2.2.1 mpire-2.10.2 ocrmac-1.0.0 omegaconf-2.3.0 pillow-11.3.0 polyfactory-3.2.0 pylatexenc-2.10 pyobjc-core-12.1 pyobjc-framework-Cocoa-12.1 pyobjc-framework-CoreML-12.1 pyobjc-framework-Quartz-12.1 pyobjc-framework-Vision-12.1 pypdfium2-4.30.0 python-docx-1.2.0 rapidocr-3.4.5 rtree-1.4.1 semchunk-2.2.2 torchvision-0.24.1 tree-sitter-0.25.2 tree-sitter-c-0.24.1 tree-sitter-java-0.23.5 tree-sitter-javascript-0.25.0 tree-sitter-python-0.25.0 tree-sitter-typescript-0.23.2 typer-0.19.2\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "open-webui 0.6.43 requires pillow==12.0.0, but you have pillow 11.3.0 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires pillow<11,>=7.1.0, but you have pillow 11.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed antlr4-python3-runtime-4.9.3 colorlog-6.10.1 docling-2.66.0 docling-core-2.57.0 docling-ibm-models-3.10.3 docling-parse-4.7.2 faker-40.1.0 jsonlines-4.0.0 jsonref-1.1.0 langchain-docling-2.0.0 latex2mathml-3.78.1 marko-2.2.1 mpire-2.10.2 ocrmac-1.0.0 omegaconf-2.3.0 pillow-11.3.0 polyfactory-3.2.0 pylatexenc-2.10 pyobjc-core-12.1 pyobjc-framework-Cocoa-12.1 pyobjc-framework-CoreML-12.1 pyobjc-framework-Quartz-12.1 pyobjc-framework-Vision-12.1 pypdfium2-4.30.0 python-docx-1.2.0 rapidocr-3.4.5 rtree-1.4.1 semchunk-2.2.2 torchvision-0.24.1 tree-sitter-0.25.2 tree-sitter-c-0.24.1 tree-sitter-java-0.23.5 tree-sitter-javascript-0.25.0 tree-sitter-python-0.25.0 tree-sitter-typescript-0.23.2 typer-0.19.2\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c7a35a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1767004252.089963 11876611 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langchain in /opt/anaconda3/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: langchain in /opt/anaconda3/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: langchain-core in /opt/anaconda3/lib/python3.12/site-packages (1.2.5)\n",
      "Requirement already satisfied: langchain-core in /opt/anaconda3/lib/python3.12/site-packages (1.2.5)\n",
      "Requirement already satisfied: langchain-openai in /opt/anaconda3/lib/python3.12/site-packages (1.1.3)\n",
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-1.1.6-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain-community in /opt/anaconda3/lib/python3.12/site-packages (0.4.1)\n",
      "Requirement already satisfied: langchain-openai in /opt/anaconda3/lib/python3.12/site-packages (1.1.3)\n",
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-1.1.6-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain-community in /opt/anaconda3/lib/python3.12/site-packages (0.4.1)\n",
      "Requirement already satisfied: pydantic in /opt/anaconda3/lib/python3.12/site-packages (2.12.5)\n",
      "Requirement already satisfied: pydantic in /opt/anaconda3/lib/python3.12/site-packages (2.12.5)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (1.0.4)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (0.4.59)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (0.12.0)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-openai) (2.11.0)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.0.45)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (3.13.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.2.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic) (0.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (2.1)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (1.0.4)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (0.4.59)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core) (0.12.0)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-openai) (2.11.0)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-openai) (0.12.0)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.0.45)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (3.13.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-community) (2.2.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic) (0.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (2.1)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.15)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.12.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.15)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.12.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.0.0)\n",
      "Using cached langchain_openai-1.1.6-py3-none-any.whl (84 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.0.0)\n",
      "Using cached langchain_openai-1.1.6-py3-none-any.whl (84 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: langchain-openai\n",
      "  Attempting uninstall: langchain-openai\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: langchain-openai 1.1.3\n",
      "    Uninstalling langchain-openai-1.1.3:\n",
      "      Successfully uninstalled langchain-openai-1.1.3\n",
      "Installing collected packages: langchain-openai\n",
      "  Attempting uninstall: langchain-openai\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: langchain-openai 1.1.3\n",
      "    Uninstalling langchain-openai-1.1.3:\n",
      "      Successfully uninstalled langchain-openai-1.1.3\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed langchain-openai-1.1.6\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed langchain-openai-1.1.6\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 1. Force upgrade the critical libraries\n",
    "%pip install -U langchain langchain-core langchain-openai langchain-community pydantic\n",
    "\n",
    "# 2. IMPORTANT: You must restart the kernel after running this!\n",
    "# In VS Code/Jupyter: Click \"Restart\" or \"Restart Kernel\" in the top toolbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentence-transformers gensim datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f93c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --force-reinstall datasets sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a0c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --force-reinstall gensim numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f62fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U sentence-transformers transformers flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe8d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the missing local server engine\n",
    "%pip install \"pymilvus[milvus_lite]\"\n",
    "\n",
    "# CRITICAL: Restart your kernel again after this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71959f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pymilvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2bc651",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U pymilvus milvus-lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe2eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove the libraries causing the binary conflict\n",
    "# (These are optional speed-boosters for Pandas, not required for functionality)\n",
    "%pip uninstall -y bottleneck numexpr\n",
    "\n",
    "# 2. Force install a compatible version of Pandas and PyArrow\n",
    "# This ensures your Pandas matches your current NumPy version\n",
    "%pip install --upgrade pandas pyarrow numpy>=2.0\n",
    "\n",
    "# 3. CRITICAL: Restart your kernel now!\n",
    "# Click \"Kernel\" -> \"Restart Kernel\" in the menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58760a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1767201156.272086 15143504 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting numpy<2.0\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Collecting numpy<2.0\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: numpy 2.2.6\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.10.1 requires numexpr>=2.6.2, which is not installed.\n",
      "open-webui 0.6.43 requires pillow==12.0.0, but you have pillow 11.3.0 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires pillow<11,>=7.1.0, but you have pillow 11.3.0 which is incompatible.\n",
      "datasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 20.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.10.1 requires numexpr>=2.6.2, which is not installed.\n",
      "open-webui 0.6.43 requires pillow==12.0.0, but you have pillow 11.3.0 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "streamlit 1.37.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n",
      "streamlit 1.37.1 requires pillow<11,>=7.1.0, but you have pillow 11.3.0 which is incompatible.\n",
      "datasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 20.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~yarrow (/opt/anaconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 1. Downgrade NumPy to the 1.x version (most compatible)\n",
    "%pip install \"numpy<2.0\"\n",
    "\n",
    "# 2. You MUST restart your kernel after this!\n",
    "# In VS Code/Jupyter: Click \"Restart\" or \"Restart Kernel\" in the top toolbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c5a2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-milvus langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e5d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fastapi uvicorn nest-asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865de2b5",
   "metadata": {},
   "source": [
    "# Simple scraping agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41be412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_classic.agents import AgentExecutor, create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c835fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connected to LLM running locally\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://127.0.0.1:1234/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    model=\"local-model\",\n",
    "    temperature=0,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Define the Tool\n",
    "@tool\n",
    "def fetch_csv_dataset(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads a CSV dataset from a URL and returns a summary.\n",
    "    Input should be the full URL string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse CSV\n",
    "        content = response.content.decode('utf-8')\n",
    "        df = pd.read_csv(StringIO(content), on_bad_lines='skip')\n",
    "        \n",
    "        return (\n",
    "            f\"SUCCESS: Downloaded data from {url}\\n\"\n",
    "            f\"Shape: {df.shape}\\n\"\n",
    "            f\"Columns: {list(df.columns)}\\n\"\n",
    "            f\"First 5 rows:\\n{df.head().to_string()}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "tools = [fetch_csv_dataset]\n",
    "\n",
    "# Define the ReAct Prompt (Hardcoded for stability)\n",
    "# This teaches the model explicitly how to think and act.\n",
    "template = '''Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}'''\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 4. Create the ReAct Agent\n",
    "# This uses simple text generation, avoiding the Pydantic/Tool Binding error completely.\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "\n",
    "# 5. Create the Executor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    verbose=True, \n",
    "    handle_parsing_errors=True # IMPORTANT for local models\n",
    ")\n",
    "\n",
    "print(\" ReAct Agent built successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a0a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = \"https://raw.githubusercontent.com/gramener/datasets/refs/heads/main/card_transactions.csv\"\n",
    "query = f\"Download the dataset from {test_url} and tell me the columns.\"\n",
    "\n",
    "response = agent_executor.invoke({\"input\": query})\n",
    "print(\"\\n--- FINAL ANSWER ---\")\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a40c1ee",
   "metadata": {},
   "source": [
    "# Vector embedding of 2022-2024 news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d70f9a",
   "metadata": {},
   "source": [
    "### Load and filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0848cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"dataset/guardian_climate_news_corpus.csv\")\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "df = df[df['date'].dt.year >= 2022].copy()\n",
    "\n",
    "df = df[df['label'] != 'UNRELATED_TO_CLIMATE'].copy()\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94bd620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa4411",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bd94f4",
   "metadata": {},
   "source": [
    "### Making vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f402dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 1. SETUP: Load your data\n",
    "# ------------------------------------------------------------------\n",
    "# df = pd.read_csv(\"your_data.csv\") # Uncomment to load your real file\n",
    "# Ensure date is datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# 2. CREATE UNIFIED TEXT REPRESENTATION\n",
    "# ------------------------------------------------------------------\n",
    "# Instead of training a separate Word2Vec model, we will format the metadata \n",
    "# into a structured string that the 8B model can \"read\" and understand semantically.\n",
    "# This technique is often called \"Text Serialization\".\n",
    "\n",
    "def serialize_row_for_embedding(row):\n",
    "    # Parse tags safely\n",
    "    try:\n",
    "        tags = ast.literal_eval(row['tags']) if isinstance(row['tags'], str) else row['tags']\n",
    "        tags_str = \", \".join(tags)\n",
    "    except:\n",
    "        tags_str = \"None\"\n",
    "        \n",
    "    # Create a rich text block that describes the entire data point\n",
    "    # We put the most important semantic info (Category, Tags, Date) at the start or end.\n",
    "    combined_text = (\n",
    "        f\"Category: {row['category']}. \"\n",
    "        f\"Tags: {tags_str}. \"\n",
    "        f\"Date: {row['date'].strftime('%Y-%m-%d')}. \"\n",
    "        f\"Title: {row['title']}\\n\"\n",
    "        f\"Content: {row['body']}\"\n",
    "    )\n",
    "    return combined_text\n",
    "\n",
    "# Apply the function\n",
    "df['serialized_text'] = df.apply(serialize_row_for_embedding, axis=1)\n",
    "\n",
    "# 3. EMBED WITH LOCAL LLAMA MODEL (via OpenAI Compatible API)\n",
    "# ------------------------------------------------------------------\n",
    "# Assuming you are running the model in LM Studio / Ollama on port 1234\n",
    "# Check your local server settings for the exact URL.\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    base_url=\"http://127.0.0.1:1234/v1\", # Point to your local server\n",
    "    api_key=\"lm-studio\",                 # Arbitrary key\n",
    "    model=\"Qwen3-Embedding-4B-GGUF\",     # The specific model name loaded in your server\n",
    "    check_embedding_ctx_length=False     # Important for long texts\n",
    ")\n",
    "\n",
    "print(\"Starting embedding process... (This may take time depending on GPU)\")\n",
    "\n",
    "# We process in batches to be safe with memory\n",
    "batch_size = 32\n",
    "all_embeddings = []\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_texts = df['serialized_text'].iloc[i:i+batch_size].tolist()\n",
    "    \n",
    "    # Generate embeddings for the batch\n",
    "    # embed_documents returns a list of lists (vectors)\n",
    "    batch_embeddings = embedding_model.embed_documents(batch_texts)\n",
    "    all_embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    print(f\"Processed rows {i} to {min(i+batch_size, len(df))}\")\n",
    "\n",
    "# 4. STORE RESULTS\n",
    "# ------------------------------------------------------------------\n",
    "# Convert to numpy array for use in classifiers or Vector DB\n",
    "final_features = np.array(all_embeddings)\n",
    "\n",
    "print(f\"Final Feature Matrix Shape: {final_features.shape}\")\n",
    "\n",
    "# Optional: Add back to DataFrame\n",
    "df['embedding_vector'] = list(final_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd16ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Save the DataFrame (Contains text, metadata, and vectors)\n",
    "# Pickle is better than CSV because it preserves lists/arrays perfectly.\n",
    "df.to_pickle(\"climate_news_data.pkl\")\n",
    "\n",
    "# 2. Save the Raw Numpy Array (Just in case)\n",
    "# This is the safest way to store the pure mathematical vectors.\n",
    "np.save(\"climate_vectors.npy\", final_features)\n",
    "\n",
    "print(\"Saved 'climate_news_data.pkl' and 'climate_vectors.npy' to disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c086d9",
   "metadata": {},
   "source": [
    "### Storing embeddings with Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafcf5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\" Attempting to rescue data...\")\n",
    "\n",
    "# 1. Load the Pickle\n",
    "# Since we are on NumPy 2.x (installed above), this will read the file correctly.\n",
    "df = pd.read_pickle(\"climate_news_data.pkl\")\n",
    "print(f\" Pickle loaded successfully. Shape: {df.shape}\")\n",
    "\n",
    "# 2. Save as Parquet\n",
    "# We drop the vector column if it exists to keep the file light (we have the .npy file)\n",
    "if 'embedding_vector' in df.columns:\n",
    "    df = df.drop(columns=['embedding_vector'])\n",
    "\n",
    "df.to_parquet(\"climate_news_data.parquet\")\n",
    "print(\" SUCCESS: Data saved to 'climate_news_data.parquet'\")\n",
    "\n",
    "# 3. Verify Vector File\n",
    "# This usually loads fine regardless of version, but let's check.\n",
    "vectors = np.load(\"climate_vectors.npy\")\n",
    "print(f\" SUCCESS: Vectors verified. Shape: {vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc31116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymilvus import MilvusClient, DataType\n",
    "\n",
    "# 1. RELOAD YOUR SAVED DATA\n",
    "# ------------------------------------------------------------------\n",
    "print(\" Reloading rescued data...\")\n",
    "df = pd.read_parquet(\"climate_news_data.parquet\")\n",
    "final_features = np.load(\"climate_vectors.npy\")\n",
    "print(f\" Data Loaded. Articles: {len(df)} | Vector Dim: {final_features.shape[1]}\")\n",
    "\n",
    "# 2. SETUP MILVUS LITE\n",
    "# ------------------------------------------------------------------\n",
    "client = MilvusClient(\"./climate_news.db\")\n",
    "COLLECTION_NAME = \"climate_articles\"\n",
    "\n",
    "# 3. DEFINE THE SCHEMA\n",
    "# ------------------------------------------------------------------\n",
    "if client.has_collection(COLLECTION_NAME):\n",
    "    client.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "schema = client.create_schema(auto_id=True, enable_dynamic_field=True)\n",
    "\n",
    "# Add Fields\n",
    "schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "schema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=2560) # Matches your Qwen size\n",
    "schema.add_field(field_name=\"category\", datatype=DataType.VARCHAR, max_length=512)\n",
    "schema.add_field(field_name=\"date\", datatype=DataType.VARCHAR, max_length=50)\n",
    "schema.add_field(field_name=\"text\", datatype=DataType.VARCHAR, max_length=65535)\n",
    "\n",
    "# 4. DEFINE INDEX\n",
    "# ------------------------------------------------------------------\n",
    "index_params = client.prepare_index_params()\n",
    "index_params.add_index(\n",
    "    field_name=\"vector\", \n",
    "    index_type=\"AUTOINDEX\", \n",
    "    metric_type=\"COSINE\"\n",
    ")\n",
    "\n",
    "# 5. CREATE COLLECTION\n",
    "# ------------------------------------------------------------------\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    schema=schema,\n",
    "    index_params=index_params\n",
    ")\n",
    "print(f\" Collection '{COLLECTION_NAME}' created.\")\n",
    "\n",
    "# 6. INSERT DATA\n",
    "# ------------------------------------------------------------------\n",
    "data_to_insert = []\n",
    "print(\"Preparing data for insertion...\")\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    vector_list = final_features[idx].tolist()\n",
    "    date_str = row['date'].strftime('%Y-%m-%d') if pd.notnull(row['date']) else \"\"\n",
    "    \n",
    "    entry = {\n",
    "        \"vector\": vector_list,\n",
    "        \"text\": str(row['body']),\n",
    "        \"title\": str(row['title']),\n",
    "        \"category\": str(row['category']),\n",
    "        \"date\": date_str,\n",
    "        \"tags\": str(row['tags'])\n",
    "    }\n",
    "    data_to_insert.append(entry)\n",
    "\n",
    "# Insert in batches\n",
    "batch_size = 100\n",
    "total_inserted = 0\n",
    "\n",
    "for i in range(0, len(data_to_insert), batch_size):\n",
    "    batch = data_to_insert[i:i+batch_size]\n",
    "    res = client.insert(collection_name=COLLECTION_NAME, data=batch)\n",
    "    total_inserted += res['insert_count']\n",
    "    print(f\"Inserted batch {i} to {i+len(batch)}...\")\n",
    "\n",
    "print(f\" SUCCESS! Stored {total_inserted} articles in 'climate_news.db'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4317b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymilvus import MilvusClient, DataType\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# 0. CLEANUP OLD CORRUPTED DB (Optional but recommended)\n",
    "# ------------------------------------------------------------------\n",
    "db_path = \"./climate_news.db\"\n",
    "if os.path.exists(db_path):\n",
    "    print(f\" Found existing database at {db_path}. Removing it...\")\n",
    "    try:\n",
    "        if os.path.isdir(db_path):\n",
    "            shutil.rmtree(db_path)\n",
    "        else:\n",
    "            os.remove(db_path)\n",
    "        print(\" Old database removed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not remove old db: {e}\")\n",
    "\n",
    "# 1. RELOAD YOUR SAVED DATA\n",
    "# ------------------------------------------------------------------\n",
    "print(\" Reloading rescued data...\")\n",
    "df = pd.read_parquet(\"climate_news_data.parquet\")\n",
    "final_features = np.load(\"climate_vectors.npy\")\n",
    "print(f\" Data Loaded. Articles: {len(df)} | Vector Dim: {final_features.shape[1]}\")\n",
    "\n",
    "# 2. SETUP MILVUS LITE (Fresh instance)\n",
    "# ------------------------------------------------------------------\n",
    "# Create a fresh client pointing to a clean database file\n",
    "client = MilvusClient(uri=db_path)\n",
    "COLLECTION_NAME = \"climate_articles\"\n",
    "\n",
    "# 3. DEFINE THE SCHEMA\n",
    "# ------------------------------------------------------------------\n",
    "if client.has_collection(COLLECTION_NAME):\n",
    "    client.drop_collection(COLLECTION_NAME)\n",
    "    print(f\"Dropped existing collection '{COLLECTION_NAME}'\")\n",
    "\n",
    "schema = client.create_schema(auto_id=True, enable_dynamic_field=True)\n",
    "\n",
    "# Add Fields\n",
    "schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "schema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=2560) # Matches your Qwen size\n",
    "schema.add_field(field_name=\"category\", datatype=DataType.VARCHAR, max_length=512)\n",
    "schema.add_field(field_name=\"date\", datatype=DataType.VARCHAR, max_length=50)\n",
    "schema.add_field(field_name=\"text\", datatype=DataType.VARCHAR, max_length=65535)\n",
    "\n",
    "# 4. DEFINE INDEX\n",
    "# ------------------------------------------------------------------\n",
    "index_params = client.prepare_index_params()\n",
    "index_params.add_index(\n",
    "    field_name=\"vector\", \n",
    "    index_type=\"AUTOINDEX\", \n",
    "    metric_type=\"COSINE\"\n",
    ")\n",
    "\n",
    "# 5. CREATE COLLECTION\n",
    "# ------------------------------------------------------------------\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    schema=schema,\n",
    "    index_params=index_params\n",
    ")\n",
    "print(f\" Collection '{COLLECTION_NAME}' created.\")\n",
    "\n",
    "# 6. INSERT DATA\n",
    "# ------------------------------------------------------------------\n",
    "data_to_insert = []\n",
    "print(\"Preparing data for insertion...\")\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    vector_list = final_features[idx].tolist()\n",
    "    date_str = row['date'].strftime('%Y-%m-%d') if pd.notnull(row['date']) else \"\"\n",
    "    \n",
    "    entry = {\n",
    "        \"vector\": vector_list,\n",
    "        \"text\": str(row['body']),\n",
    "        \"title\": str(row['title']),\n",
    "        \"category\": str(row['category']),\n",
    "        \"date\": date_str,\n",
    "        \"tags\": str(row['tags'])\n",
    "    }\n",
    "    data_to_insert.append(entry)\n",
    "\n",
    "# Insert in batches\n",
    "batch_size = 100\n",
    "total_inserted = 0\n",
    "\n",
    "for i in range(0, len(data_to_insert), batch_size):\n",
    "    batch = data_to_insert[i:i+batch_size]\n",
    "    res = client.insert(collection_name=COLLECTION_NAME, data=batch)\n",
    "    total_inserted += res['insert_count']\n",
    "    print(f\"Inserted batch {i} to {i+len(batch)}...\")\n",
    "\n",
    "print(f\" SUCCESS! Stored {total_inserted} articles in '{db_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d50db5",
   "metadata": {},
   "source": [
    "# RAG Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f70bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "client = MilvusClient(uri=\"./climate_news.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbde41ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RAG Chain initialized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. SETUP MODELS\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    base_url=\"http://127.0.0.1:1234/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    model=\"Qwen3-Embedding-4B-GGUF\",\n",
    "    check_embedding_ctx_length=False\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://127.0.0.1:1234/v1\", # Verify this is your Ministral server port\n",
    "    api_key=\"local-key\",\n",
    "    model=\"nvidia/nemotron-3-nan\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# 2. CONNECT TO VECTOR STORE\n",
    "# Note: Ensure climate_news.db is NOT open in any other software\n",
    "vector_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"uri\": \"./climate_news.db\"},\n",
    "    collection_name=\"climate_articles\",\n",
    "    text_field=\"text\",\n",
    "    auto_id=True\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# 3. DEFINE RAG LOGIC\n",
    "template = \"\"\"You are a specialized Climate News Assistant.\n",
    "Use the context below to answer. If unsure, say you can't find it.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(\" RAG Chain initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ac5b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the system\n",
    "query = \"Tell me any news about tiny snails on Atlantic Island\"\n",
    "response = rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5980d990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Question: Tell me any news about tiny snails on Atlantic Island\n",
      "\n",
      " Agent Answer:\n",
      "\n",
      "**News about tiny snails on the Atlantic Island (Desertas Islands)**  \n",
      "\n",
      "- **Population rescued from the brink:** More than 1,300 tiny, criticallyendangered land snailsmembers of two species that had not been seen for over a centurywere released on a small island in the Madeira archipelago (off the Atlantic coast of Morocco).  \n",
      "\n",
      "- **Breeding programme success:** The snails were taken into captivity at Chester Zoo (and other European zoos), where keepers built miniature habitat tanks and, after a few months, cracked the breeding code. Multiple generations were produced, allowing a network of zoos to raise enough individuals for reintroduction.  \n",
      "\n",
      "- **Wild release site:** The snails were set free on Bugio, a neighbouring island that has been a protected, humanfree refuge since 1990. Invasive predators (rats, mice, goats) have been eradicated there, giving the snails a safe environment to establish a wild population.  \n",
      "\n",
      "- **Individual marking & monitoring:** Each released snail has been uniquely marked so that conservationists can track survival, growth, and breeding success.  \n",
      "\n",
      "- **Ecological significance:** The Desertas Islands are the only place on Earth where these snail species exist. The reintroduction marks the first step in preventing their extinction and restoring a unique component of the Atlantic islands biodiversity.  \n",
      "\n",
      "In short, recent news highlights a collaborative conservation effort that rescued two obscure, peasized snail species from extinction, bred them in zoos, and released over a thousand individuals onto a protected Atlantic island to kickstart a wild population.\n"
     ]
    }
   ],
   "source": [
    "print(f\" Question: {query}\\n\")\n",
    "print(\" Agent Answer:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38babca8",
   "metadata": {},
   "source": [
    "# Embedding PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e657d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dropping old collection 'research_papers'\n",
      " Clean slate ready.\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "DB_PATH = \"./climate_news.db\"\n",
    "COLLECTION_NAME = \"research_papers\"\n",
    "\n",
    "client = MilvusClient(uri=DB_PATH)\n",
    "\n",
    "if client.has_collection(COLLECTION_NAME):\n",
    "    print(f\" Dropping old collection '{COLLECTION_NAME}'\")\n",
    "    client.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "print(\" Clean slate ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92d9c173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Embedded 20 / 411\n",
      " Embedded 40 / 411\n",
      " Embedded 40 / 411\n",
      " Embedded 60 / 411\n",
      " Embedded 60 / 411\n",
      " Embedded 80 / 411\n",
      " Embedded 80 / 411\n",
      " Embedded 100 / 411\n",
      " Embedded 100 / 411\n",
      " Embedded 120 / 411\n",
      " Embedded 120 / 411\n",
      " Embedded 140 / 411\n",
      " Embedded 140 / 411\n",
      " Embedded 160 / 411\n",
      " Embedded 160 / 411\n",
      " Embedded 180 / 411\n",
      " Embedded 180 / 411\n",
      " Embedded 200 / 411\n",
      " Embedded 200 / 411\n",
      " Embedded 220 / 411\n",
      " Embedded 220 / 411\n",
      " Embedded 240 / 411\n",
      " Embedded 240 / 411\n",
      " Embedded 260 / 411\n",
      " Embedded 260 / 411\n",
      " Embedded 280 / 411\n",
      " Embedded 280 / 411\n",
      " Embedded 300 / 411\n",
      " Embedded 300 / 411\n",
      " Embedded 320 / 411\n",
      " Embedded 320 / 411\n",
      " Embedded 340 / 411\n",
      " Embedded 340 / 411\n",
      " Embedded 360 / 411\n",
      " Embedded 360 / 411\n",
      " Embedded 380 / 411\n",
      " Embedded 380 / 411\n",
      " Embedded 400 / 411\n",
      " Embedded 400 / 411\n",
      " Embedded 411 / 411\n",
      " Embedded 411 / 411\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "PAPER_FOLDER = \"./research_paper\"\n",
    "EMBED_URL = \"http://127.0.0.1:1234/v1/embeddings\"\n",
    "MODEL_NAME = \"text-embedding-qwen3-embedding-4b\"\n",
    "\n",
    "loader = PyPDFDirectoryLoader(PAPER_FOLDER)\n",
    "raw_docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=80)\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "\n",
    "def embed_batch(texts):\n",
    "    resp = requests.post(\n",
    "        EMBED_URL,\n",
    "        json={\"model\": MODEL_NAME, \"input\": texts},\n",
    "        timeout=60\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    return [x[\"embedding\"] for x in resp.json()[\"data\"]]\n",
    "\n",
    "vectors = []\n",
    "texts = []\n",
    "metas = []\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "for i in range(0, len(docs), batch_size):\n",
    "    batch = docs[i:i+batch_size]\n",
    "    batch_texts = [d.page_content.strip() for d in batch]\n",
    "\n",
    "    emb = embed_batch(batch_texts)\n",
    "\n",
    "    vectors.extend(emb)\n",
    "    texts.extend(batch_texts)\n",
    "\n",
    "    for d in batch:\n",
    "        metas.append({\n",
    "            \"source\": d.metadata.get(\"source\", \"\"),\n",
    "            \"page\": int(d.metadata.get(\"page\", -1))\n",
    "        })\n",
    "\n",
    "    print(f\" Embedded {i+len(batch)} / {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b6aa7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Collection created.\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient, DataType\n",
    "\n",
    "DIM = len(vectors[0])\n",
    "client = MilvusClient(uri=DB_PATH)\n",
    "\n",
    "schema = client.create_schema(auto_id=True, enable_dynamic_field=False)\n",
    "\n",
    "schema.add_field(\"id\", DataType.INT64, is_primary=True)\n",
    "schema.add_field(\"vector\", DataType.FLOAT_VECTOR, dim=DIM)\n",
    "schema.add_field(\"text\", DataType.VARCHAR, max_length=65535)\n",
    "schema.add_field(\"source\", DataType.VARCHAR, max_length=1024)\n",
    "schema.add_field(\"page\", DataType.INT64)\n",
    "\n",
    "index_params = client.prepare_index_params()\n",
    "index_params.add_index(\n",
    "    field_name=\"vector\",\n",
    "    index_type=\"AUTOINDEX\",\n",
    "    metric_type=\"COSINE\"\n",
    ")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"research_papers\",\n",
    "    schema=schema,\n",
    "    index_params=index_params\n",
    ")\n",
    "\n",
    "print(\" Collection created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c8b189e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 100 / 411\n",
      "Inserted 200 / 411\n",
      "Inserted 300 / 411\n",
      "Inserted 400 / 411\n",
      "Inserted 500 / 411\n",
      " SUCCESS: research_papers indexed.\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for i in range(len(vectors)):\n",
    "    data.append({\n",
    "        \"vector\": vectors[i],\n",
    "        \"text\": texts[i],\n",
    "        \"source\": metas[i][\"source\"],\n",
    "        \"page\": metas[i][\"page\"]\n",
    "    })\n",
    "\n",
    "batch_size = 100\n",
    "for i in range(0, len(data), batch_size):\n",
    "    client.insert(\n",
    "        collection_name=\"research_papers\",\n",
    "        data=data[i:i+batch_size]\n",
    "    )\n",
    "    print(f\"Inserted {i+batch_size} / {len(data)}\")\n",
    "\n",
    "print(\" SUCCESS: research_papers indexed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "764d25ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "Score: 0.5711402297019958\n",
      "Source: research_paper/2401.09646v1.pdf\n",
      "Page: 46\n",
      "tions on climate change.\\n Cite the documents provided in\n",
      "the context.\n",
      "StackExchange Youre an AI assistant generating answers to questions on\n",
      "the website stackexchange on the topic {source}.\n",
      "AppTek General Youre a helpful and harmless AI assistant.\n",
      "OASST-1 Youre Open Assistant, an AI language model, developed\n",
      "by Laion AI together with an open source community and\n",
      "trained using crowdsourced data.\n",
      "Dolly Youre an AI language model trained on data generated by\n",
      "employees of databricks.\n",
      "Llama-2 Sa\n",
      "\n",
      "---\n",
      "Score: 0.5260053277015686\n",
      "Source: research_paper/2505.18653v1.pdf\n",
      "Page: 7\n",
      "Task 0-Shot 5-Shot Diff.\n",
      "CDP-QA-Cities .55 (.10) .63 (.06) .07\n",
      "CDP-QA-Corp. .53 (.11) .62 (.05) .09\n",
      "CDP-QA-States .56 (.11) .64 (.06) .07\n",
      "CDP-Topic-Cities .32 (.02) .35 (.01) .03\n",
      "Climate Commit. .61 (.09) .67 (.04) .06\n",
      "Climate Detection .57 (.15) .71 (.06) .14\n",
      "Climate Eng .50 (.08) .54 (.05) .04\n",
      "Climate NER .13 (.06) .20 (.05) .08\n",
      "Climate Sentiment .60 (.11) .70 (.04) .09\n",
      "Climate Specificity .58 (.13) .69 (.07) .10\n",
      "Climate Stance .17 (.07) .51 (.09) .34\n",
      "Climate-Fever .43 (.11) .46 (.08) .03\n",
      "Clim\n",
      "\n",
      "---\n",
      "Score: 0.517491340637207\n",
      "Source: research_paper/2506.13796v1.pdf\n",
      "Page: 11\n",
      "assumptions about future socio-economic conditions and related mitigation measures.45 These are quantitative projections and are neither predictions nor forecasts. \n",
      "Around half of all modelled global emission scenarios assume cost-effective approaches that rely on least-cost mission abatement options globally. The other half look at \n",
      "existing policies and regionally and sectorally differentiated actions. Most do not make explicit assumptions about global equity, environmental justice\n",
      "Chunk 2 (fr\n"
     ]
    }
   ],
   "source": [
    "query = \"What does the paper say about climate mitigation strategies?\"\n",
    "\n",
    "query_emb = embed_batch([query])[0]\n",
    "\n",
    "res = client.search(\n",
    "    collection_name=\"research_papers\",\n",
    "    data=[query_emb],\n",
    "    anns_field=\"vector\",\n",
    "    limit=3,\n",
    "    output_fields=[\"text\", \"source\", \"page\"]\n",
    ")\n",
    "\n",
    "for hit in res[0]:\n",
    "    print(\"\\n---\")\n",
    "    print(\"Score:\", hit[\"distance\"])\n",
    "    print(\"Source:\", hit.get(\"source\", \"N/A\"))\n",
    "    print(\"Page:\", hit.get(\"page\", \"N/A\"))\n",
    "    print(hit[\"text\"][:500])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66676566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Research Paper RAG Chain initialized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. FAKE EMBEDDINGS (IMPORTANT)\n",
    "# We already embedded research papers manually.\n",
    "# LangChain must NOT try to re-embed stored documents.\n",
    "# ------------------------------------------------------------------\n",
    "class FakeEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts): \n",
    "        return []\n",
    "\n",
    "    def embed_query(self, text): \n",
    "        return []\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. REAL QUERY EMBEDDINGS (for search only)\n",
    "# ------------------------------------------------------------------\n",
    "query_embeddings = OpenAIEmbeddings(\n",
    "    base_url=\"http://127.0.0.1:1234/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    model=\"text-embedding-qwen3-embedding-4b\",\n",
    "    check_embedding_ctx_length=False\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. LLM\n",
    "# ------------------------------------------------------------------\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://127.0.0.1:1234/v1\",  # Ministral server\n",
    "    api_key=\"local-key\",\n",
    "    model=\"nvidia/nemotron-3-nano\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. CONNECT TO RESEARCH PAPERS VECTOR STORE\n",
    "# ------------------------------------------------------------------\n",
    "vector_store = Milvus(\n",
    "    embedding_function=FakeEmbeddings(),   #  CRITICAL\n",
    "    connection_args={\"uri\": \"./climate_news.db\"},\n",
    "    collection_name=\"research_papers\",\n",
    "    vector_field=\"vector\",                 #  matches schema\n",
    "    text_field=\"text\",\n",
    "    auto_id=True\n",
    ")\n",
    "\n",
    "def retrieve_docs(query: str, k: int = 3):\n",
    "    # 1. Embed query explicitly (DENSE)\n",
    "    query_vector = query_embeddings.embed_query(query)\n",
    "\n",
    "    # 2. Search using vector directly\n",
    "    docs = vector_store.similarity_search_by_vector(\n",
    "        embedding=query_vector,\n",
    "        k=k\n",
    "    )\n",
    "    return docs\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5. PROMPT (paper-aware, citation-friendly)\n",
    "# ------------------------------------------------------------------\n",
    "template = \"\"\"You are an academic research assistant.\n",
    "\n",
    "You are given excerpts from research papers.\n",
    "Use ONLY the provided context to answer when possible.\n",
    "If the answer is not found, say you could not find this\n",
    "information in the provided research papers.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER (cite sources if relevant):\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    blocks = []\n",
    "    for d in docs:\n",
    "        source = d.metadata.get(\"source\", \"unknown\")\n",
    "        page = d.metadata.get(\"page\", \"N/A\")\n",
    "        blocks.append(\n",
    "            f\"[Source: {source}, Page: {page}]\\n{d.page_content}\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(blocks)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6. RAG CHAIN\n",
    "# ------------------------------------------------------------------\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retrieve_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\" Research Paper RAG Chain initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed5736e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How is ClimateGPT model trained, and what is the model architecture like?\"\n",
    "response = rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d23f25c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Question: How is ClimateGPT model trained, and what is the model architecture like?\n",
      "\n",
      " Agent Answer:\n",
      "\n",
      "**Training**  \n",
      "- ClimateGPT was pretrained on a compute cluster using a fork of the **MegatronLLM** repository[Document1, page40].  \n",
      "- After the pretraining phase, the model underwent **instruction finetuning (IFT)** to align its outputs with the expected format[Document1, page40].\n",
      "\n",
      "**Model Architecture**  \n",
      "- ClimateGPT is built as an **autoregressive language model** that employs an **optimized transformer architecture**[Document1, page40].  \n",
      "\n",
      "These details describe both the training pipeline and the structural design of the ClimateGPT models.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\" Question: {query}\\n\")\n",
    "print(\" Agent Answer:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aa3400",
   "metadata": {},
   "source": [
    "# New PDF Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ae8bb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dropping old collection 'research_papers'\n",
      " Clean slate ready.\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "DB_PATH = \"./climate_news.db\"\n",
    "COLLECTION_NAME = \"research_papers\"\n",
    "\n",
    "client = MilvusClient(uri=DB_PATH)\n",
    "\n",
    "if client.has_collection(COLLECTION_NAME):\n",
    "    print(f\" Dropping old collection '{COLLECTION_NAME}'\")\n",
    "    client.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "print(\" Clean slate ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f3d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from langchain_docling import DoclingLoader\n",
    "from langchain_docling.loader import ExportType\n",
    "from docling.chunking import HybridChunker\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# 1. SETUP\n",
    "PAPER_FOLDER = Path(\"./research_paper\")\n",
    "EMBED_URL = \"http://127.0.0.1:1234/v1/embeddings\"\n",
    "MODEL_NAME = \"text-embedding-qwen3-embedding-4b\"\n",
    "EMBED_MAX_LENGTH = 512\n",
    "\n",
    "\n",
    "pdf_files = list(PAPER_FOLDER.glob(\"*.pdf\"))\n",
    "assert len(pdf_files) > 0, \"No PDFs found in research_paper\"\n",
    "\n",
    "loader = DoclingLoader(\n",
    "    file_path=[str(p) for p in pdf_files],\n",
    "    export_type=ExportType.DOC_CHUNKS,\n",
    "    chunker=HybridChunker(max_tokens=EMBED_MAX_LENGTH)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "026ce8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 00:16:18,426 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-02 00:16:18,468 - INFO - Going to convert document batch...\n",
      "2026-01-02 00:16:18,469 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2026-01-02 00:16:18,468 - INFO - Going to convert document batch...\n",
      "2026-01-02 00:16:18,469 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2026-01-02 00:16:18,488 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2026-01-02 00:16:18,488 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-02 00:16:18,490 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2026-01-02 00:16:18,488 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2026-01-02 00:16:18,488 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-02 00:16:18,490 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2026-01-02 00:16:18,511 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2026-01-02 00:16:18,511 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-02 00:16:18,520 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2026-01-02 00:16:18,511 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2026-01-02 00:16:18,511 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-02 00:16:18,520 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2026-01-02 00:16:19,384 - INFO - Auto OCR model selected ocrmac.\n",
      "2026-01-02 00:16:19,384 - INFO - Auto OCR model selected ocrmac.\n",
      "2026-01-02 00:16:19,403 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2026-01-02 00:16:19,403 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-02 00:16:19,406 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2026-01-02 00:16:19,411 - INFO - Accelerator device: 'mps'\n",
      "2026-01-02 00:16:19,403 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2026-01-02 00:16:19,403 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-02 00:16:19,406 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2026-01-02 00:16:19,411 - INFO - Accelerator device: 'mps'\n",
      "2026-01-02 00:16:23,375 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2026-01-02 00:16:23,376 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-02 00:16:23,377 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2026-01-02 00:16:23,375 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2026-01-02 00:16:23,376 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-02 00:16:23,377 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2026-01-02 00:16:23,691 - INFO - Accelerator device: 'mps'\n",
      "2026-01-02 00:16:23,691 - INFO - Accelerator device: 'mps'\n",
      "2026-01-02 00:16:24,063 - INFO - Processing document 2401.09646v1.pdf\n",
      "2026-01-02 00:16:24,063 - INFO - Processing document 2401.09646v1.pdf\n",
      "2026-01-02 00:16:47,736 - INFO - Finished converting document 2401.09646v1.pdf in 29.31 sec.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (980 > 512). Running this sequence through the model will result in indexing errors\n",
      "2026-01-02 00:16:47,736 - INFO - Finished converting document 2401.09646v1.pdf in 29.31 sec.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (980 > 512). Running this sequence through the model will result in indexing errors\n",
      "2026-01-02 00:16:48,578 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-02 00:16:48,581 - INFO - Going to convert document batch...\n",
      "2026-01-02 00:16:48,581 - INFO - Processing document 2505.18653v1.pdf\n",
      "2026-01-02 00:16:48,578 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-02 00:16:48,581 - INFO - Going to convert document batch...\n",
      "2026-01-02 00:16:48,581 - INFO - Processing document 2505.18653v1.pdf\n",
      "2026-01-02 00:17:04,019 - INFO - Finished converting document 2505.18653v1.pdf in 15.44 sec.\n",
      "2026-01-02 00:17:04,019 - INFO - Finished converting document 2505.18653v1.pdf in 15.44 sec.\n",
      "2026-01-02 00:17:04,573 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-02 00:17:04,576 - INFO - Going to convert document batch...\n",
      "2026-01-02 00:17:04,576 - INFO - Processing document 2506.13796v1.pdf\n",
      "2026-01-02 00:17:04,573 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-02 00:17:04,576 - INFO - Going to convert document batch...\n",
      "2026-01-02 00:17:04,576 - INFO - Processing document 2506.13796v1.pdf\n",
      "2026-01-02 00:17:08,955 - INFO - Finished converting document 2506.13796v1.pdf in 4.38 sec.\n",
      "2026-01-02 00:17:08,955 - INFO - Finished converting document 2506.13796v1.pdf in 4.38 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 232 structured Docling chunks.\n",
      " Token-safe chunks after splitting: 983\n"
     ]
    }
   ],
   "source": [
    "token_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=60,\n",
    "    separators=[\n",
    "        \"\\n\\n\",        # paragraphs\n",
    "        \"\\n\",          # line breaks\n",
    "        \". \",          # sentences\n",
    "        \" \",           # fallback\n",
    "        \"\"\n",
    "    ]\n",
    ")\n",
    "raw_docs = loader.load()\n",
    "print(f\" Loaded {len(raw_docs)} structured Docling chunks.\")\n",
    "\n",
    "docs = []\n",
    "global_chunk_id = 0\n",
    "\n",
    "for d in raw_docs:\n",
    "    splits = token_splitter.split_text(d.page_content)\n",
    "\n",
    "    for i, split in enumerate(splits):\n",
    "        meta = d.metadata.copy()\n",
    "\n",
    "        # Preserve structure + make graph-friendly\n",
    "        meta.update({\n",
    "            \"parent_chunk_id\": meta.get(\"chunk_id\", None),\n",
    "            \"sub_chunk_id\": i,\n",
    "            \"global_chunk_id\": global_chunk_id,\n",
    "            \"source_file\": meta.get(\"source\", None),\n",
    "            \"section\": meta.get(\"heading\", None),\n",
    "            \"page\": meta.get(\"page\", None),\n",
    "        })\n",
    "\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=split,\n",
    "                metadata=meta\n",
    "            )\n",
    "        )\n",
    "        global_chunk_id += 1\n",
    "\n",
    "print(f\" Token-safe chunks after splitting: {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e821ddde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Chunk Sanity Check ---\n",
      "Total Chunks:      983\n",
      "Max Token Length:  332\n",
      "Average Tokens:    90.5\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Qwen2 and Qwen3 share the same tokenizer logic; \n",
    "# this public ID works without needing a private token.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Alibaba-NLP/gte-Qwen2-7B-instruct\", trust_remote_code=True)\n",
    "\n",
    "# Count tokens for each final chunk\n",
    "token_lengths = [len(tokenizer.encode(d.page_content)) for d in docs]\n",
    "\n",
    "print(f\"--- Final Chunk Sanity Check ---\")\n",
    "print(f\"Total Chunks:      {len(docs)}\")\n",
    "print(f\"Max Token Length:  {max(token_lengths)}\")\n",
    "print(f\"Average Tokens:    {sum(token_lengths)/len(token_lengths):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ef6f386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Indexed 20 / 983\n",
      " Indexed 40 / 983\n",
      " Indexed 40 / 983\n",
      " Indexed 60 / 983\n",
      " Indexed 60 / 983\n",
      " Indexed 80 / 983\n",
      " Indexed 80 / 983\n",
      " Indexed 100 / 983\n",
      " Indexed 100 / 983\n",
      " Indexed 120 / 983\n",
      " Indexed 120 / 983\n",
      " Indexed 140 / 983\n",
      " Indexed 140 / 983\n",
      " Indexed 160 / 983\n",
      " Indexed 160 / 983\n",
      " Indexed 180 / 983\n",
      " Indexed 180 / 983\n",
      " Indexed 200 / 983\n",
      " Indexed 200 / 983\n",
      " Indexed 220 / 983\n",
      " Indexed 220 / 983\n",
      " Indexed 240 / 983\n",
      " Indexed 240 / 983\n",
      " Indexed 260 / 983\n",
      " Indexed 260 / 983\n",
      " Indexed 280 / 983\n",
      " Indexed 280 / 983\n",
      " Indexed 300 / 983\n",
      " Indexed 300 / 983\n",
      " Indexed 320 / 983\n",
      " Indexed 320 / 983\n",
      " Indexed 340 / 983\n",
      " Indexed 340 / 983\n",
      " Indexed 360 / 983\n",
      " Indexed 360 / 983\n",
      " Indexed 380 / 983\n",
      " Indexed 380 / 983\n",
      " Indexed 400 / 983\n",
      " Indexed 400 / 983\n",
      " Indexed 420 / 983\n",
      " Indexed 420 / 983\n",
      " Indexed 440 / 983\n",
      " Indexed 440 / 983\n",
      " Indexed 460 / 983\n",
      " Indexed 460 / 983\n",
      " Indexed 480 / 983\n",
      " Indexed 480 / 983\n",
      " Indexed 500 / 983\n",
      " Indexed 500 / 983\n",
      " Indexed 520 / 983\n",
      " Indexed 520 / 983\n",
      " Indexed 540 / 983\n",
      " Indexed 540 / 983\n",
      " Indexed 560 / 983\n",
      " Indexed 560 / 983\n",
      " Indexed 580 / 983\n",
      " Indexed 580 / 983\n",
      " Indexed 600 / 983\n",
      " Indexed 600 / 983\n",
      " Indexed 620 / 983\n",
      " Indexed 620 / 983\n",
      " Indexed 640 / 983\n",
      " Indexed 640 / 983\n",
      " Indexed 660 / 983\n",
      " Indexed 660 / 983\n",
      " Indexed 680 / 983\n",
      " Indexed 680 / 983\n",
      " Indexed 700 / 983\n",
      " Indexed 700 / 983\n",
      " Indexed 720 / 983\n",
      " Indexed 720 / 983\n",
      " Indexed 740 / 983\n",
      " Indexed 740 / 983\n",
      " Indexed 760 / 983\n",
      " Indexed 760 / 983\n",
      " Indexed 780 / 983\n",
      " Indexed 780 / 983\n",
      " Indexed 800 / 983\n",
      " Indexed 800 / 983\n",
      " Indexed 820 / 983\n",
      " Indexed 820 / 983\n",
      " Indexed 840 / 983\n",
      " Indexed 840 / 983\n",
      " Indexed 860 / 983\n",
      " Indexed 860 / 983\n",
      " Indexed 880 / 983\n",
      " Indexed 880 / 983\n",
      " Indexed 900 / 983\n",
      " Indexed 900 / 983\n",
      " Indexed 920 / 983\n",
      " Indexed 920 / 983\n",
      " Indexed 940 / 983\n",
      " Indexed 940 / 983\n",
      " Indexed 960 / 983\n",
      " Indexed 960 / 983\n",
      " Indexed 980 / 983\n",
      " Indexed 980 / 983\n",
      " Indexed 983 / 983\n",
      " Indexed 983 / 983\n"
     ]
    }
   ],
   "source": [
    "# 3. MANUAL EMBEDDING (Ensures LM Studio compatibility)\n",
    "def get_embeddings_manually(texts):\n",
    "    payload = {\"model\": MODEL_NAME, \"input\": texts}\n",
    "    try:\n",
    "        # Note: Ensure LM Studio has this exact model loaded or it may error\n",
    "        response = requests.post(EMBED_URL, json=payload, timeout=120)\n",
    "        response.raise_for_status()\n",
    "        return [item[\"embedding\"] for item in response.json()[\"data\"]]\n",
    "    except Exception as e:\n",
    "        print(f\" Embedding Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# 4. PROCESS IN BATCHES\n",
    "all_vectors = []\n",
    "valid_docs = []\n",
    "batch_size = 20\n",
    "\n",
    "for i in range(0, len(docs), batch_size):\n",
    "    batch = docs[i : i + batch_size]\n",
    "    # Docling metadata includes 'heading', 'page', and often 'references'\n",
    "    batch_texts = [d.page_content.strip() for d in batch]\n",
    "    vectors = get_embeddings_manually(batch_texts)\n",
    "    \n",
    "    if vectors:\n",
    "        all_vectors.extend(vectors)\n",
    "        valid_docs.extend(batch)\n",
    "        print(f\" Indexed {i + len(batch)} / {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4aab3da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SUCCESS: 983 structured chunks stored.\n"
     ]
    }
   ],
   "source": [
    "class FakeEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts): return []\n",
    "    def embed_query(self, text): return []\n",
    "\n",
    "def sanitize_metadata(meta: dict) -> dict:\n",
    "    clean = {}\n",
    "    for k, v in meta.items():\n",
    "        k = k.replace(\".\", \"_\")\n",
    "        if v is None:\n",
    "            continue\n",
    "        clean[k] = str(v)\n",
    "    return clean\n",
    "\n",
    "if all_vectors:\n",
    "    vector_store = Milvus(\n",
    "        embedding_function=FakeEmbeddings(),\n",
    "        connection_args={\"uri\": \"./climate_news.db\"},\n",
    "        collection_name=\"research_papers\",\n",
    "        drop_old=True,\n",
    "        auto_id=True\n",
    "    )\n",
    "\n",
    "    texts = [d.page_content for d in valid_docs]\n",
    "    metadatas = [sanitize_metadata(d.metadata) for d in valid_docs]\n",
    "\n",
    "    vector_store.add_embeddings(\n",
    "        texts=texts,\n",
    "        embeddings=all_vectors,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "\n",
    "    print(f\" SUCCESS: {len(all_vectors)} structured chunks stored.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccdc1ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# 1. QUERY EMBEDDING CLIENT (Points to LM Studio)\n",
    "# check_embedding_ctx_length=False prevents pydantic warnings with local models\n",
    "query_embeddings = OpenAIEmbeddings(\n",
    "    base_url=\"http://127.0.0.1:1234/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    model=\"text-embedding-qwen3-embedding-4b\",\n",
    "    check_embedding_ctx_length=False \n",
    ")\n",
    "\n",
    "# 2. CHAT MODEL (Ministral/Nemotron via LM Studio)\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://127.0.0.1:1234/v1\",\n",
    "    api_key=\"local-key\",\n",
    "    model=\"nvidia/nemotron-3-nano\", # Ensure this matches your loaded LLM\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# 3. CONNECT TO STORE\n",
    "vector_store = Milvus(\n",
    "    embedding_function=FakeEmbeddings(), # Use the dummy class to skip LangChain's internal embedding\n",
    "    connection_args={\"uri\": \"./climate_news.db\"},\n",
    "    collection_name=\"research_papers\",\n",
    "    vector_field=\"vector\", \n",
    "    text_field=\"text\",\n",
    ")\n",
    "\n",
    "# 4. RETRIEVAL WRAPPER\n",
    "def retrieve_docs(query: str):\n",
    "    # This specifically uses your Qwen3 model to generate the query vector\n",
    "    query_vector = query_embeddings.embed_query(query)\n",
    "    # Search Milvus using the generated vector\n",
    "    return vector_store.similarity_search_by_vector(embedding=query_vector, k=3)\n",
    "\n",
    "# 5. FORMATTING & PROMPT\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([\n",
    "        f\"[Source: {d.metadata.get('source_file', 'unknown')}, Page: {d.metadata.get('page', 'N/A')}]\\n{d.page_content}\"\n",
    "        for d in docs\n",
    "    ])\n",
    "\n",
    "template = \"\"\"You are an academic research assistant.\n",
    "Use ONLY the provided context to answer. If not found, say so.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 6. THE CHAIN\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": RunnableLambda(retrieve_docs) | format_docs, \n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c935f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 01:37:47,489 - INFO - HTTP Request: POST http://127.0.0.1:1234/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-02 01:37:58,998 - INFO - HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-02 01:37:58,998 - INFO - HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "query = \"Give me details of how Climate GPT was trained\"\n",
    "response = rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "312a5799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Training Overview of ClimateGPT**\n",
      "\n",
      "| Aspect | Details from the source |\n",
      "|--------|--------------------------|\n",
      "| **Training phases** | 1. **Pretraining** 2. **Instruction finetuning (IFT)** |\n",
      "| **Pretraining data** |  A corpus of **4.2billion climatespecific tokens**.<br> The underlying fromscratch models were also trained on a much larger generic corpus of **300billion tokens**.<br> The pretraining data includes documents dated up to **October2023**. |\n",
      "| **Training timeline** | The model was **trained between September2023 and November2023**. |\n",
      "| **Instruction finetuning (IFT)** |  After the base pretraining, the model underwent **instruction finetuning** to align its outputs with the expected format.<br> The IFT dataset combined:<br>- Public instructionfinetuned datasets.<br>- Additional IFT data that was **collected in cooperation with climate experts**. |\n",
      "| **Model variants** | The paper reports performance on several variants (e.g., ClimateGPT70B, ClimateGPTFSC7B) evaluated on tasks such as MCQ, CDPQA, ClimaText, ClimateEng, and ClimateStance. |\n",
      "| **Licensing** | The model is released under the **ClimateGPT Community License**. |\n",
      "\n",
      "**Summary**  \n",
      "ClimateGPT was first pretrained on a large, climatefocused token corpus (4.2B tokens) and, for the fromscratch versions, on an even larger generic corpus (300B tokens). The pretraining data is static, covering documents up to October2023, and the training period spanned September to November2023. To make the model instructionaligned, it was then finetuned using a mixture of publicly available instructionfinetuned datasets and a proprietary set of instruction data that was curated with input from climate experts. This twostage process (pretraining  instruction finetuning) yields the final ClimateGPT model.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe8e730",
   "metadata": {},
   "source": [
    "# Model with combined retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d167691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. LOAD COLLECTION REGISTRY (Step 1)\n",
    "# ------------------------------------------------------------------\n",
    "with open(\"db_description.json\", \"r\") as f:\n",
    "    COLLECTION_REGISTRY = json.load(f)\n",
    "\n",
    "def format_registry():\n",
    "    \"\"\"Convert collection registry JSON into readable text for the LLM.\"\"\"\n",
    "    blocks = []\n",
    "    for col in COLLECTION_REGISTRY[\"collections\"]:\n",
    "        blocks.append(\n",
    "            f\"\"\"\n",
    "Collection: {col['name']}\n",
    "Type: {col['type']}\n",
    "Domain: {col['domain']}\n",
    "Granularity: {col['granularity']}\n",
    "Strengths: {\", \".join(col['strengths'])}\n",
    "Weaknesses: {\", \".join(col['weaknesses'])}\n",
    "\"\"\"\n",
    "        )\n",
    "    return \"\\n\".join(blocks)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. MULTI-COLLECTION RETRIEVER (Step 2)\n",
    "# ------------------------------------------------------------------\n",
    "# One Milvus vector store per collection\n",
    "research_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"uri\": \"./climate_news.db\"},\n",
    "    collection_name=\"research_papers\",\n",
    "    text_field=\"text\",\n",
    ")\n",
    "\n",
    "news_store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"uri\": \"./climate_news.db\"},\n",
    "    collection_name=\"climate_articles\",\n",
    "    text_field=\"text\",\n",
    ")\n",
    "\n",
    "research_retriever = research_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "news_retriever = news_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "\n",
    "def retrieve_docs(query: str):\n",
    "    \"\"\"\n",
    "    Retrieve from BOTH collections.\n",
    "    Later well make this LLM-controlled.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    docs.extend(research_retriever.invoke(query))\n",
    "    docs.extend(news_retriever.invoke(query))\n",
    "    return docs\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    blocks = []\n",
    "    for d in docs:\n",
    "        source = d.metadata.get(\"source\", \"unknown\")\n",
    "        blocks.append(f\"[SOURCE: {source}]\\n{d.page_content}\")\n",
    "    return \"\\n\\n\".join(blocks)\n",
    "\n",
    "\n",
    "# Wrap functions as Runnables \n",
    "retrieve_docs_runnable = RunnableLambda(retrieve_docs)\n",
    "format_docs_runnable = RunnableLambda(format_docs)\n",
    "registry_runnable = RunnableLambda(lambda _: format_registry())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50f4fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# COLLECTION ROUTER PROMPT\n",
    "# ------------------------------------------------------------------\n",
    "router_template = \"\"\"You are a routing assistant.\n",
    "\n",
    "You are given a list of available data collections.\n",
    "Your job is to decide which collections (if any) should be queried\n",
    "to answer the user question.\n",
    "\n",
    "Rules:\n",
    "- Only select collections that are clearly relevant.\n",
    "- If none are relevant, return \"none\".\n",
    "- Return ONLY valid JSON.\n",
    "- Do NOT explain your reasoning.\n",
    "\n",
    "Available collections:\n",
    "{registry}\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\n",
    "Respond in this exact JSON format:\n",
    "{{\n",
    "  \"collections\": [\"collection_name_1\", \"collection_name_2\"]\n",
    "}}\n",
    "\n",
    "OR\n",
    "\n",
    "{{\n",
    "  \"collections\": \"none\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "router_prompt = PromptTemplate.from_template(router_template)\n",
    "\n",
    "router_chain = (\n",
    "    {\n",
    "        \"registry\": registry_runnable,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | router_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "def select_collections(query: str):\n",
    "    \"\"\"Stage 1: Decide which collections to use.\"\"\"\n",
    "    raw = router_chain.invoke(query)\n",
    "    try:\n",
    "        return json.loads(raw)\n",
    "    except json.JSONDecodeError:\n",
    "        # Hard fail-safe\n",
    "        return {\"collections\": \"none\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ee0fc7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_VECTORSTORES = {\n",
    "    \"research_papers\": research_store,\n",
    "    \"climate_articles\": news_store,\n",
    "}\n",
    "\n",
    "def retrieve_from_collections(query: str, collections):\n",
    "    \"\"\"Retrieve documents from selected collections using direct vector search.\"\"\"\n",
    "    docs = []\n",
    "\n",
    "    # Embed the query once using the real embedding model\n",
    "    query_vector = query_embeddings.embed_query(query)\n",
    "\n",
    "    for name in collections:\n",
    "        store = COLLECTION_VECTORSTORES.get(name)\n",
    "        if store:\n",
    "            # Use similarity_search_by_vector instead of similarity_search\n",
    "            # to avoid re-embedding with FakeEmbeddings\n",
    "            docs.extend(store.similarity_search_by_vector(embedding=query_vector, k=3))\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf7a7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_template = \"\"\"You are a careful climate research assistant.\n",
    "\n",
    "Use ONLY the provided context to answer the question.\n",
    "If the context does not contain the answer, say so clearly.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "final_prompt = PromptTemplate.from_template(final_template)\n",
    "\n",
    "final_chain = (\n",
    "    {\n",
    "        \"context\": RunnablePassthrough(),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | final_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8eb2464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query: str):\n",
    "    \"\"\"\n",
    "    Full pipeline:\n",
    "    1. Decide collections\n",
    "    2. Retrieve context if needed\n",
    "    3. Answer with or without RAG\n",
    "    \"\"\"\n",
    "\n",
    "    routing = select_collections(query)\n",
    "    collections = routing.get(\"collections\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # CASE 1: No retrieval needed\n",
    "    # --------------------------------------------------\n",
    "    if collections == \"none\":\n",
    "        return llm.invoke(\n",
    "            f\"The following question is not answered using the provided datasets. \"\n",
    "            f\"Answer from general knowledge and say so explicitly if question is technical.\\n\\nQuestion: {query}\"\n",
    "        )\n",
    "    \n",
    "    if isinstance(collections, str):\n",
    "        collections = [collections]\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # CASE 2: Targeted RAG\n",
    "    # --------------------------------------------------\n",
    "    docs = retrieve_from_collections(query, collections)\n",
    "    context = format_docs(docs)\n",
    "\n",
    "    if not context.strip():\n",
    "        return \"The selected datasets do not contain relevant information.\"\n",
    "\n",
    "    return final_chain.invoke(\n",
    "        {\n",
    "            \"context\": context,\n",
    "            \"question\": query\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52aa273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Training**\n",
      "\n",
      "- ClimateGPT was pretrained on a large corpus using a cluster provided by **MLFoundry**.  \n",
      "- The pretraining was carried out with a **fork of the MegatronLLM repository** from the EPFL LLM team.  \n",
      "- After the base pretraining, the models underwent **instruction finetuning (IFT)** so that they could produce the expected output format for questionanswering tasks.  \n",
      "- The training period spanned **September2023November2023**.  \n",
      "- The model is released under the **ClimateGPT Community License**.\n",
      "\n",
      "**Model Architecture**\n",
      "\n",
      "- ClimateGPT is an **autoregressive language model**.  \n",
      "- It is built on an **optimized transformer architecture**.  \n",
      "- The architecture supports input that is **textonly** and generates **textonly** outputs.  \n",
      "- The released versions come in several parameter sizes: **7B, 13B, and 70B**, with two distinct 7B variants trained from scratch.  \n",
      "\n",
      "These details are drawn directly from the provided research paper excerpts.\n"
     ]
    }
   ],
   "source": [
    "print(answer_query(\"How is ClimateGPT model trained, and what is the model architecture like?\"))\n",
    "print(\"\\n\\n\\n\")\n",
    "print(answer_query(\"Tell me any news about tiny snails on Atlantic Island\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d445c4",
   "metadata": {},
   "source": [
    "# Graph Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fb6688bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI = \"neo4j://127.0.0.1:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"manav@1234\"\n",
    "\n",
    "EMBED_URL = \"http://127.0.0.1:1234/v1/embeddings\"\n",
    "MODEL_NAME = \"text-embedding-qwen3-embedding-4b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "74b375e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All Constraints (Hash, Path, Chunk) initialized.\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "class Neo4jManager:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def check_connection(self):\n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                result = session.run(\"RETURN 'Connection Successful' AS msg\")\n",
    "                return result.single()[\"msg\"]\n",
    "        except Exception as e:\n",
    "            return f\"Connection Failed: {e}\"\n",
    "\n",
    "    def setup_constraints(self):\n",
    "        \"\"\"Initializes all unique constraints for the Graph.\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Content Hash: The ultimate ID for a document's content\n",
    "            session.run(\"CREATE CONSTRAINT doc_hash IF NOT EXISTS FOR (d:Document) REQUIRE d.hash IS UNIQUE\")\n",
    "            # File Path: Prevents the same path being indexed twice\n",
    "            session.run(\"CREATE CONSTRAINT doc_path IF NOT EXISTS FOR (d:Document) REQUIRE d.path IS UNIQUE\")\n",
    "            # Chunk ID: Unique identifier for text segments\n",
    "            session.run(\"CREATE CONSTRAINT chunk_id IF NOT EXISTS FOR (c:Chunk) REQUIRE c.id IS UNIQUE\")\n",
    "            print(\" All Constraints (Hash, Path, Chunk) initialized.\")\n",
    "        \n",
    "    def get_processed_hashes(self):\n",
    "        \"\"\"Returns a set of all content hashes already in the DB.\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\"MATCH (d:Document) RETURN d.hash AS hash\")\n",
    "            return {record[\"hash\"] for record in result}\n",
    "        \n",
    "    def clear_data(self):\n",
    "        \"\"\"\n",
    "        Deletes all nodes and relationships. \n",
    "        Note: This keeps your constraints and indexes intact.\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # DETACH DELETE removes the node AND any relationships connected to it\n",
    "            result = session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "            summary = result.consume()\n",
    "            print(f\" Data Cleared: Deleted {summary.counters.nodes_deleted} nodes \"\n",
    "                  f\"and {summary.counters.relationships_deleted} relationships.\")\n",
    "\n",
    "    def clear_schema(self):\n",
    "        \"\"\"\n",
    "        Removes all constraints and indexes from the database.\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # 1. Get all constraints\n",
    "            constraints = session.run(\"SHOW CONSTRAINTS\")\n",
    "            for record in constraints:\n",
    "                session.run(f\"DROP CONSTRAINT {record['name']}\")\n",
    "            \n",
    "            # 2. Get all indexes\n",
    "            indexes = session.run(\"SHOW INDEXES\")\n",
    "            for record in indexes:\n",
    "                # We skip lookup indexes which are system-managed\n",
    "                if record['type'] != 'LOOKUP':\n",
    "                    session.run(f\"DROP INDEX {record['name']}\")\n",
    "                    \n",
    "            print(\" Schema Cleared: All constraints and indexes removed.\")\n",
    "\n",
    "    def hard_reset(self):\n",
    "        \"\"\"Wipes both data and schema for a completely fresh start.\"\"\"\n",
    "        self.clear_data()\n",
    "        self.clear_schema()\n",
    "        print(\"  Hard Reset Complete: The database is now empty and has no rules.\")\n",
    "\n",
    "\n",
    "db_manager = Neo4jManager(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "db_manager.setup_constraints()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c8194536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "class SmartIngestor:\n",
    "    def __init__(self, db_manager, base_dir=\"./data\"):\n",
    "        self.db_manager = db_manager\n",
    "        self.base_dir = Path(base_dir)\n",
    "\n",
    "    def get_file_hash(self, filepath):\n",
    "        \"\"\"Standard SHA-256 hashing for deduplication.\"\"\"\n",
    "        sha256_hash = hashlib.sha256()\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "                sha256_hash.update(byte_block)\n",
    "        return sha256_hash.hexdigest()\n",
    "\n",
    "    def get_document_type(self, filepath):\n",
    "        \"\"\"\n",
    "        Determines the label based on folder depth.\n",
    "        - Root files: 'Generic'\n",
    "        - Subfolders: Joins folder names with '_' (e.g., ResearchPapers_2024)\n",
    "        \"\"\"\n",
    "        relative_path = Path(filepath).relative_to(self.base_dir)\n",
    "        # Get all parent folders between base_dir and the file\n",
    "        parts = relative_path.parts[:-1] \n",
    "        \n",
    "        if not parts:\n",
    "            return \"Generic\"\n",
    "        \n",
    "        # Clean folder names: replace spaces with CamelCase or underscores\n",
    "        clean_parts = [p.replace(\" \", \"\") for p in parts]\n",
    "        return \"_\".join(clean_parts)\n",
    "\n",
    "    def scan_and_plan(self):\n",
    "        \"\"\"Scans directory and compares hashes against Neo4j.\"\"\"\n",
    "        existing_hashes = self.db_manager.get_processed_hashes()\n",
    "        to_process = []\n",
    "        \n",
    "        print(f\"---   Scanning Directory: {self.base_dir} ---\")\n",
    "        \n",
    "        # walk() through all subdirectories\n",
    "        for root, dirs, files in os.walk(self.base_dir):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(\".pdf\"):\n",
    "                    full_path = os.path.join(root, file)\n",
    "                    file_hash = self.get_file_hash(full_path)\n",
    "                    \n",
    "                    if file_hash in existing_hashes:\n",
    "                        print(f\" SKIP: '{file}' (Already Indexed)\")\n",
    "                        continue\n",
    "                    \n",
    "                    doc_type = self.get_document_type(full_path)\n",
    "                    print(f\" PENDING: '{file}' | Type: {doc_type}\")\n",
    "                    \n",
    "                    to_process.append({\n",
    "                        \"path\": full_path,\n",
    "                        \"filename\": file,\n",
    "                        \"hash\": file_hash,\n",
    "                        \"category\": doc_type\n",
    "                    })\n",
    "                    \n",
    "        print(f\"\\n Summary: {len(to_process)} new files ready for embedding.\")\n",
    "        return to_process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92802d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data Cleared: Deleted 280 nodes and 279 relationships.\n",
      " Schema Cleared: All constraints and indexes removed.\n",
      "  Hard Reset Complete: The database is now empty and has no rules.\n"
     ]
    }
   ],
   "source": [
    "# db_manager.hard_reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e26534ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---   Scanning Directory: data ---\n",
      " PENDING: 'Annual-Statement-2023.pdf' | Type: climate_weather_reports\n",
      " PENDING: 'Annual-Statement-2022.pdf' | Type: climate_weather_reports\n",
      " PENDING: 'Financial-Year-summary-2023-24.pdf' | Type: climate_weather_reports\n",
      " PENDING: 'Annual-Statement-2024.pdf' | Type: climate_weather_reports\n",
      " PENDING: 'scs72.pdf' | Type: climate_weather_reports\n",
      " PENDING: '24-00239_REPORT_StateoftheClimate2024_241022.pdf' | Type: climate_weather_reports\n",
      " PENDING: 'scs73.pdf' | Type: climate_weather_reports\n",
      " PENDING: 'EP2025-1632.pdf' | Type: climate_weather_reports\n",
      " PENDING: 'CCIA_2015_NRM_TechnicalReport_WEB.pdf' | Type: climate_weather_reports\n",
      " PENDING: '2401.09646v1.pdf' | Type: research_papers\n",
      " PENDING: 'es24007.pdf' | Type: research_papers\n",
      " PENDING: 'hess-28-1383-2024.pdf' | Type: research_papers\n",
      " PENDING: 'hess-28-1251-2024.pdf' | Type: research_papers\n",
      " PENDING: 'Attribution-of-extreme-events_Lane-etal_2023.pdf' | Type: research_papers\n",
      " PENDING: 's44168-025-00273-y.pdf' | Type: research_papers\n",
      " PENDING: 'nhess-21-941-2021.pdf' | Type: research_papers\n",
      " PENDING: '2505.18653v1.pdf' | Type: research_papers\n",
      " PENDING: 's43247-025-02179-3.pdf' | Type: research_papers\n",
      " PENDING: 's41467-018-05938-3.pdf' | Type: research_papers\n",
      " PENDING: '2506.13796v1.pdf' | Type: research_papers\n",
      "\n",
      " Summary: 20 new files ready for embedding.\n"
     ]
    }
   ],
   "source": [
    "# Run Scanner\n",
    "ingestor = SmartIngestor(db_manager, base_dir=\"./data\")\n",
    "pending_files = ingestor.scan_and_plan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "93b33109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting Sanity Check for: Annual-Statement-2023.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 23:11:49,339 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-02 23:11:49,349 - INFO - Going to convert document batch...\n",
      "2026-01-02 23:11:49,349 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2026-01-02 23:11:49,349 - INFO - Auto OCR model selected ocrmac.\n",
      "2026-01-02 23:11:49,350 - INFO - Accelerator device: 'mps'\n",
      "2026-01-02 23:11:49,349 - INFO - Going to convert document batch...\n",
      "2026-01-02 23:11:49,349 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2026-01-02 23:11:49,349 - INFO - Auto OCR model selected ocrmac.\n",
      "2026-01-02 23:11:49,350 - INFO - Accelerator device: 'mps'\n",
      "2026-01-02 23:11:50,141 - INFO - Accelerator device: 'mps'\n",
      "2026-01-02 23:11:50,141 - INFO - Accelerator device: 'mps'\n",
      "2026-01-02 23:11:50,367 - INFO - Processing document Annual-Statement-2023.pdf\n",
      "2026-01-02 23:11:50,367 - INFO - Processing document Annual-Statement-2023.pdf\n",
      "2026-01-02 23:12:09,424 - INFO - Finished converting document Annual-Statement-2023.pdf in 20.09 sec.\n",
      "2026-01-02 23:12:09,424 - INFO - Finished converting document Annual-Statement-2023.pdf in 20.09 sec.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1043 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1043 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generating 279 embeddings via LM Studio...\n",
      " Successfully stored 'Annual-Statement-2023.pdf' in Neo4j.\n",
      "\n",
      " Sanity Check Complete. One file processed.\n",
      "Check Neo4j Browser with: MATCH (n:Document)-[:HAS_CHUNK]->(c) RETURN n,c LIMIT 25\n",
      " Successfully stored 'Annual-Statement-2023.pdf' in Neo4j.\n",
      "\n",
      " Sanity Check Complete. One file processed.\n",
      "Check Neo4j Browser with: MATCH (n:Document)-[:HAS_CHUNK]->(c) RETURN n,c LIMIT 25\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from langchain_docling import DoclingLoader\n",
    "from langchain_docling.loader import ExportType\n",
    "from docling.chunking import HybridChunker\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# --- 2. THE BATCH TRANSACTION (Neo4j Logic) ---\n",
    "def store_in_neo4j(db_manager, file_info, chunks, vectors):\n",
    "    \"\"\"\n",
    "    Writes a Document and all its Chunks to Neo4j in one transaction.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MERGE (d:Document {hash: $file_hash})\n",
    "    ON CREATE SET \n",
    "        d.path = $path, \n",
    "        d.name = $filename, \n",
    "        d.category = $category,\n",
    "        d.label = $label\n",
    "    \n",
    "    WITH d\n",
    "    UNWIND $chunk_data AS chunk\n",
    "    CREATE (c:Chunk {id: chunk.id})\n",
    "    SET \n",
    "        c.text = chunk.text,\n",
    "        c.embedding = chunk.vector,\n",
    "        c.page = chunk.page,\n",
    "        c.section = chunk.section,\n",
    "        c.index = chunk.index\n",
    "    CREATE (d)-[:HAS_CHUNK]->(c)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare batch data\n",
    "    chunk_data = []\n",
    "    for i, (doc, vector) in enumerate(zip(chunks, vectors)):\n",
    "        chunk_data.append({\n",
    "            \"id\": str(uuid.uuid4()), # Unique ID for constraint\n",
    "            \"text\": doc.page_content,\n",
    "            \"vector\": vector,\n",
    "            \"page\": doc.metadata.get(\"page\", \"N/A\"),\n",
    "            \"section\": doc.metadata.get(\"section\", \"N/A\"),\n",
    "            \"index\": i\n",
    "        })\n",
    "\n",
    "    with db_manager.driver.session() as session:\n",
    "        session.run(query, \n",
    "            file_hash=file_info['hash'],\n",
    "            path=file_info['path'],\n",
    "            filename=file_info['filename'],\n",
    "            category=file_info['category'],\n",
    "            label=file_info['category'], # Adding as a secondary label\n",
    "            chunk_data=chunk_data\n",
    "        )\n",
    "    print(f\" Successfully stored '{file_info['filename']}' in Neo4j.\")\n",
    "\n",
    "# --- 3. THE MAIN LOOP (Sanity Check Version) ---\n",
    "\n",
    "# Get pending files from previous step\n",
    "# (Assuming 'pending_files' is the list from ingestor.scan_and_plan())\n",
    "\n",
    "if not pending_files:\n",
    "    print(\"No pending files to process.\")\n",
    "else:\n",
    "    # --- SANITY CHECK: LIMIT TO 1 FILE ---\n",
    "    target_file = pending_files[0] \n",
    "    print(f\" Starting Sanity Check for: {target_file['filename']}\")\n",
    "\n",
    "    # A. Load with Docling\n",
    "    loader = DoclingLoader(\n",
    "        file_path=[target_file['path']],\n",
    "        export_type=ExportType.DOC_CHUNKS,\n",
    "        chunker=HybridChunker(max_tokens=512)\n",
    "    )\n",
    "    \n",
    "    token_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=512, chunk_overlap=60, \n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    raw_docs = loader.load()\n",
    "    processed_chunks = []\n",
    "    \n",
    "    # B. Split & Metadata Wrap\n",
    "    for d in raw_docs:\n",
    "        splits = token_splitter.split_text(d.page_content)\n",
    "        for i, split in enumerate(splits):\n",
    "            meta = d.metadata.copy()\n",
    "            meta.update({\n",
    "                \"section\": meta.get(\"heading\", \"N/A\"),\n",
    "                \"page\": meta.get(\"page\", \"N/A\")\n",
    "            })\n",
    "            processed_chunks.append(Document(page_content=split, metadata=meta))\n",
    "\n",
    "    # C. Embed (Batch call)\n",
    "    batch_texts = [c.page_content.strip() for c in processed_chunks]\n",
    "    payload = {\"model\": MODEL_NAME, \"input\": batch_texts}\n",
    "    \n",
    "    print(f\" Generating {len(batch_texts)} embeddings via LM Studio...\")\n",
    "    response = requests.post(EMBED_URL, json=payload, timeout=120)\n",
    "    response.raise_for_status()\n",
    "    vectors = [item[\"embedding\"] for item in response.json()[\"data\"]]\n",
    "\n",
    "    # D. Store\n",
    "    store_in_neo4j(db_manager, target_file, processed_chunks, vectors)\n",
    "\n",
    "    print(\"\\n Sanity Check Complete. One file processed.\")\n",
    "    print(\"Check Neo4j Browser with: MATCH (n:Document)-[:HAS_CHUNK]->(c) RETURN n,c LIMIT 25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "31be9430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 24035195-4b60-4a5b-92e6-c01fe28678e4)')' thrown while requesting HEAD https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct/resolve/main/tokenizer_config.json\n",
      "2026-01-03 00:04:29,541 - WARNING - '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 24035195-4b60-4a5b-92e6-c01fe28678e4)')' thrown while requesting HEAD https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "2026-01-03 00:04:29,543 - WARNING - Retrying in 1s [Retry 1/5].\n",
      "2026-01-03 00:04:29,541 - WARNING - '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 24035195-4b60-4a5b-92e6-c01fe28678e4)')' thrown while requesting HEAD https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "2026-01-03 00:04:29,543 - WARNING - Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---  TOKEN SANITY CHECK ---\n",
      "Total Chunks:        279\n",
      "Max Tokens in Chunk: 194\n",
      "Avg Tokens / Chunk:  79.5\n",
      " SUCCESS: All chunks are within the 512 token limit.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Load the Tokenizer\n",
    "# Qwen2 and Qwen3 share the same tokenizer logic. \n",
    "# This ID allows you to download just the tokenizer config without the 4B model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Alibaba-NLP/gte-Qwen2-7B-instruct\", trust_remote_code=True)\n",
    "\n",
    "def run_token_sanity_check(chunks):\n",
    "    \"\"\"\n",
    "    Analyzes the token density of the generated chunks.\n",
    "    \"\"\"\n",
    "    # Extract text from LangChain Document objects\n",
    "    texts = [c.page_content for c in chunks]\n",
    "    \n",
    "    # Encode and count tokens\n",
    "    token_counts = [len(tokenizer.encode(t)) for t in texts]\n",
    "    \n",
    "    total_chunks = len(token_counts)\n",
    "    max_tokens = max(token_counts)\n",
    "    avg_tokens = sum(token_counts) / total_chunks\n",
    "    \n",
    "    print(\"\\n---  TOKEN SANITY CHECK ---\")\n",
    "    print(f\"Total Chunks:        {total_chunks}\")\n",
    "    print(f\"Max Tokens in Chunk: {max_tokens}\")\n",
    "    print(f\"Avg Tokens / Chunk:  {avg_tokens:.1f}\")\n",
    "    \n",
    "    # Check against model limit (assuming 512 for Qwen3-4B-Embed)\n",
    "    MODEL_LIMIT = 512 \n",
    "    if max_tokens > MODEL_LIMIT:\n",
    "        print(f\"  WARNING: {sum(1 for x in token_counts if x > MODEL_LIMIT)} chunks exceed the {MODEL_LIMIT} token limit!\")\n",
    "    else:\n",
    "        print(f\" SUCCESS: All chunks are within the {MODEL_LIMIT} token limit.\")\n",
    "    \n",
    "    return token_counts\n",
    "\n",
    "# Execute check\n",
    "token_stats = run_token_sanity_check(processed_chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
